{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuZero-TSP-Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2-9WODNu4bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import os\n",
        "import gym\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "from .abstract_game import AbstractGame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os38wzt_wv8N",
        "colab_type": "text"
      },
      "source": [
        "#MuZero Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE_Dy-lLvvJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MuZeroConfig:\n",
        "    def __init__(self):\n",
        "        self.seed = 0  # Seed for numpy, torch and the game\n",
        "        ### Game\n",
        "\n",
        "        #What are the observations of our TSP?\n",
        "        self.observation_shape = (3, 3, 3)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n",
        "        #How many possible actions do we have\n",
        "        self.action_space = [i for i in range(9)]  # Fixed list of all possible actions. You should only edit the length\n",
        "        #Fixed to single player\n",
        "        self.players = [i for i in range(1)]  # List of players. You should only edit the length\n",
        "        self.stacked_observations = 0  # Number of previous observation and previous actions to add to the current observation\n",
        "\n",
        "\n",
        "\n",
        "        ### Self-Play\n",
        "        self.num_actors = 1  # Number of simultaneous threads self-playing to feed the replay buffer\n",
        "        self.max_moves = 9  # Maximum number of moves if game is not finished before\n",
        "        #To be changed later\n",
        "        self.num_simulations = 25  # Number of future moves self-simulated\n",
        "        self.discount = 1  # Chronological discount of the reward\n",
        "        #what?\n",
        "        self.temperature_threshold = 6  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n",
        "\n",
        "        # Root prior exploration noise\n",
        "        self.root_dirichlet_alpha = 0.1\n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB formula\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "\n",
        "\n",
        "        ### Network\n",
        "        self.network = \"resnet\"  # \"resnet\" / \"fullyconnected\"\n",
        "        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n",
        "\n",
        "        # Residual Network\n",
        "        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n",
        "        self.blocks = 1  # Number of blocks in the ResNet\n",
        "        self.channels = 16  # Number of channels in the ResNet\n",
        "        self.reduced_channels = 16  # Number of channels before heads of dynamic and prediction networks\n",
        "        self.resnet_fc_reward_layers = [8]  # Define the hidden layers in the reward head of the dynamic network\n",
        "        self.resnet_fc_value_layers = [8]  # Define the hidden layers in the value head of the prediction network\n",
        "        self.resnet_fc_policy_layers = [8]  # Define the hidden layers in the policy head of the prediction network\n",
        "\n",
        "        # Fully Connected Network\n",
        "        self.encoding_size = 32\n",
        "        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n",
        "        self.fc_value_layers = []  # Define the hidden layers in the value network\n",
        "        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n",
        "        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n",
        "        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n",
        "\n",
        "\n",
        "\n",
        "        ### Training\n",
        "        self.results_path = os.path.join(os.path.dirname(__file__), \"../results\", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\"))  # Path to store the model weights and TensorBoard logs\n",
        "        self.training_steps = 100000  # Total number of training steps (ie weights update according to a batch)\n",
        "        self.batch_size = 64  # Number of parts of games to train on at each training step\n",
        "        self.checkpoint_interval = 10  # Number of training steps before using the model for sef-playing\n",
        "        self.value_loss_weight = 0.25  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n",
        "        self.training_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Train on GPU if available\n",
        "\n",
        "        self.optimizer = \"Adam\"  # \"Adam\" or \"SGD\". Paper uses SGD\n",
        "        self.weight_decay = 1e-4  # L2 weights regularization\n",
        "        self.momentum = 0.9  # Used only if optimizer is SGD\n",
        "\n",
        "        # Exponential learning rate schedule\n",
        "        self.lr_init = 0.01  # Initial learning rate\n",
        "        self.lr_decay_rate = 1  # Set it to 1 to use a constant learning rate\n",
        "        self.lr_decay_steps = 10000\n",
        "\n",
        "\n",
        "        ### Replay Buffer\n",
        "        self.window_size = 3000  # Number of self-play games to keep in the replay buffer\n",
        "        self.num_unroll_steps = 20  # Number of game moves to keep for every batch element\n",
        "        self.td_steps = 20  # Number of steps in the future to take into account for calculating the target value\n",
        "        self.use_last_model_value = False  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n",
        "\n",
        "        # Prioritized Replay (See paper appendix Training)\n",
        "        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n",
        "        self.use_max_priority = True  # Use the n-step TD error as initial priority. Better for large replay buffer\n",
        "        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n",
        "        self.PER_beta = 1.0\n",
        "\n",
        "\n",
        "\n",
        "        ### Adjust the self play / training ratio to avoid over/underfitting\n",
        "        self.self_play_delay = 0  # Number of seconds to wait after each played game\n",
        "        self.training_delay = 0  # Number of seconds to wait after each training step\n",
        "        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8DiEq4Vvvzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def visit_softmax_temperature_fn(self, trained_steps):\n",
        "    \"\"\"\n",
        "    Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n",
        "    The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n",
        "\n",
        "    Returns:\n",
        "        Positive float.\n",
        "    \"\"\"\n",
        "    return 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I34AgKsGv5CF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2390d373-37ae-42b2-fe49-0c62c2553f25"
      },
      "source": [
        "from itertools import permutations \n",
        "\n",
        "class MuZeroTSP():\n",
        "    \"\"\"\n",
        "    Game wrapper.\n",
        "    \"\"\"\n",
        "\n",
        "    #I added this from our exercise\n",
        "    def __init__(self, args):\n",
        "        self.env = MuZeroTSP()\n",
        "        self.num_node = args.num_node # number of nodes in the graph\n",
        "        self.graph = np.random.rand(self.num_node, 2) # each index representing xy coordinates\n",
        "\n",
        "    #I added this from our exercsie\n",
        "    def getInitState(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            first_state: a representation of the graph\n",
        "            left column representing visited nodes\n",
        "            right column will always have a single 1 and the rest are 0's. index with the 1 in the right column is current node\n",
        "        \"\"\"\n",
        "        \n",
        "        # Always start with first node as current node \n",
        "        first_state = np.zeros([self.num_node, 2])\n",
        "        first_state[0,0] = 1\n",
        "        first_state[0,1] = 1\n",
        "        return first_state\n",
        "\n",
        "    def getNextState(self, state, action):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "            action: action taken by current player\n",
        "        Returns:\n",
        "            next_s: graph after applying action\n",
        "            reward: reward from action\n",
        "        \"\"\"\n",
        "        \n",
        "        next_s = state.copy()\n",
        "        # zero out current node\n",
        "        next_s[:, 1] = 0\n",
        "        # 1 in left column for visited, 1 in right column for current node\n",
        "        next_s[action, :] = 1\n",
        "        prev_a = np.where(state[:, 1] == 1)[0][0]\n",
        "        # get xy coordinates for prev_node and current_node from the graph\n",
        "        prev_node = self.graph[prev_a]\n",
        "        current_node = self.graph[action]\n",
        "        reward = 1 - np.linalg.norm(current_node - prev_node)\n",
        "        if self.num_node == np.sum(next_s[:, 0]): #end of game\n",
        "            reward += 1 - np.linalg.norm(current_node - self.graph[0])\n",
        "            \n",
        "        return next_s, reward\n",
        "\n",
        "    #I added this from our exercsie\n",
        "    def getValidMoves(self, state):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "        Returns:\n",
        "            list of valid moves, 1 for valid, 0 for invalid \n",
        "        \"\"\"\n",
        "        return 1 - state[:, 0]\n",
        "\n",
        "    #I added this from our exercsie\n",
        "\n",
        "    def getGameEnded(self, state):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "        Returns:\n",
        "            r: 0 if game has not ended. 1 if it has\n",
        "               \n",
        "        \"\"\"\n",
        "        end = 0\n",
        "        if self.num_node == np.sum(state[:, 0]):\n",
        "            end = 1\n",
        "        return end\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Apply action to the game.\n",
        "        \n",
        "        Args:\n",
        "            action : action of the action_space to take.\n",
        "\n",
        "        Returns:\n",
        "            The new observation, the reward and a boolean if the game has ended.\n",
        "        \"\"\"\n",
        "        observation, reward, done = self.env.step(action)\n",
        "        return observation, reward*20, done\n",
        "\n",
        "    def to_play(self):\n",
        "        \"\"\"\n",
        "        Return the current player.\n",
        "\n",
        "        Returns:\n",
        "            The current player, it should be an element of the players list in the config. \n",
        "        \"\"\"\n",
        "        return self.env.to_play()\n",
        "\n",
        "    def legal_actions(self):\n",
        "        \"\"\"\n",
        "        Should return the legal actions at each turn, if it is not available, it can return\n",
        "        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n",
        "        \n",
        "        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n",
        "        equal to the action space but to return a negative reward if the action is illegal.\n",
        "    \n",
        "        Returns:\n",
        "            An array of integers, subset of the action space.\n",
        "        \"\"\"\n",
        "        return self.env.legal_actions()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the game for a new game.\n",
        "        \n",
        "        Returns:\n",
        "            Initial observation of the game.\n",
        "        \"\"\"\n",
        "        return self.env.reset()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Properly close the game.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Display the game observation.\n",
        "        \"\"\"\n",
        "        self.env.render()\n",
        "        input(\"Press enter to take a step \")\n",
        "\n",
        "    def encode_board(self):\n",
        "        return self.env.encode_board()\n",
        "\n",
        "    def human_to_action(self):\n",
        "        \"\"\"\n",
        "        For multiplayer games, ask the user for a legal action\n",
        "        and return the corresponding action number.\n",
        "\n",
        "        Returns:\n",
        "            An integer from the action space.\n",
        "        \"\"\"\n",
        "        choice = input(\n",
        "            \"Enter the column to play for the player {}: \".format(self.to_play())\n",
        "        )\n",
        "        while choice not in [str(action) for action in self.legal_actions()]:\n",
        "            choice = input(\"Enter another column : \")\n",
        "        return int(choice)\n",
        "\n",
        "    def action_to_string(self, action_number):\n",
        "        \"\"\"\n",
        "        Convert an action number to a string representing the action.\n",
        "        \n",
        "        Args:\n",
        "            action_number: an integer from the action space.\n",
        "\n",
        "        Returns:\n",
        "            String representing the action.\n",
        "        \"\"\"\n",
        "        return \"Play column {}\".format(action_number)\n",
        "\n",
        "    def optimal(self):\n",
        "      current_reward = 0\n",
        "      val = 100000\n",
        "      path = []\n",
        "      graph = self.graph\n",
        "      for perm in list(permutations(np.arange(self.num_node)[1:])):\n",
        "        current_reward = 0\n",
        "        \n",
        "        current_reward += np.linalg.norm(graph[0] - graph[perm[0]])\n",
        "        for i in range(len(perm) - 1):\n",
        "            j = perm[i]\n",
        "            k = perm[i+1]\n",
        "            current_reward += np.linalg.norm(graph[k] - graph[j])\n",
        "        current_reward += np.linalg.norm(graph[perm[-1]] - graph[0])\n",
        "        \n",
        "        if val > current_reward:\n",
        "            val = current_reward\n",
        "            path = perm\n",
        "            \n",
        "      return val, path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP5DK92Iwqlt",
        "colab_type": "text"
      },
      "source": [
        "#TicTacToe.py class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPGc-1hdv_9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = numpy.zeros((3, 3)).astype(int)\n",
        "        self.player = 1\n",
        "\n",
        "    def to_play(self):\n",
        "        return 0 if self.player == 1 else 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = numpy.zeros((3, 3)).astype(int)\n",
        "        self.player = 1\n",
        "        return self.get_observation()\n",
        "\n",
        "    def step(self, action):\n",
        "        row = action // 3\n",
        "        col = action % 3\n",
        "        self.board[row, col] = self.player\n",
        "\n",
        "        done = self.is_finished()\n",
        "\n",
        "        reward = 1 if done and 0 < len(self.legal_actions()) else 0\n",
        "\n",
        "        self.player *= -1\n",
        "\n",
        "        return self.get_observation(), reward, done\n",
        "\n",
        "    def get_observation(self):\n",
        "        board_player1 = numpy.where(self.board == 1, 1.0, 0.0)\n",
        "        board_player2 = numpy.where(self.board == -1, 1.0, 0.0)\n",
        "        board_to_play = numpy.full((3, 3), self.player).astype(float)\n",
        "        return numpy.array([board_player1, board_player2, board_to_play])\n",
        "\n",
        "    def legal_actions(self):\n",
        "        legal = []\n",
        "        for i in range(9):\n",
        "            row = i // 3\n",
        "            col = i % 3\n",
        "            if self.board[row, col] == 0:\n",
        "                legal.append(i)\n",
        "        return legal\n",
        "\n",
        "    def is_finished(self):\n",
        "        # Horizontal and vertical checks\n",
        "        for i in range(3):\n",
        "            if (self.board[i, :] == self.player * numpy.ones(3).astype(int)).all():\n",
        "                return True\n",
        "            if (self.board[:, i] == self.player * numpy.ones(3).astype(int)).all():\n",
        "                return True\n",
        "\n",
        "        # Diagonal checks\n",
        "        if (\n",
        "            self.board[0, 0] == self.player\n",
        "            and self.board[1, 1] == self.player\n",
        "            and self.board[2, 2] == self.player\n",
        "        ):\n",
        "            return True\n",
        "        if (\n",
        "            self.board[2, 0] == self.player\n",
        "            and self.board[1, 1] == self.player\n",
        "            and self.board[0, 2] == self.player\n",
        "        ):\n",
        "            return True\n",
        "\n",
        "        # No legal actions means a draw\n",
        "        if len(self.legal_actions()) == 0:\n",
        "            return True\n",
        "\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL8WKY-PwEVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render(self):\n",
        "    print(self.board[::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBLyOIYuwlRf",
        "colab_type": "text"
      },
      "source": [
        "#MuZero.Py Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY_-2CqqwgmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import importlib\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy\n",
        "import ray\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import models\n",
        "import replay_buffer\n",
        "import self_play\n",
        "import shared_storage\n",
        "import trainer\n",
        "\n",
        "\n",
        "class MuZero:\n",
        "    \"\"\"\n",
        "    Main class to manage MuZero.\n",
        "\n",
        "    Args:\n",
        "        game_name (str): Name of the game module, it should match the name of a .py file\n",
        "        in the \"./games\" directory.\n",
        "\n",
        "    Example:\n",
        "        >>> muzero = MuZero(\"cartpole\")\n",
        "        >>> muzero.train()\n",
        "        >>> muzero.test()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game_name):\n",
        "        self.game_name = game_name\n",
        "\n",
        "        # Load the game and the config from the module with the game name\n",
        "        try:\n",
        "            game_module = importlib.import_module(\"games.\" + self.game_name)\n",
        "            self.config = game_module.MuZeroConfig()\n",
        "            self.Game = game_module.Game\n",
        "        except Exception as err:\n",
        "            print(\n",
        "                '{} is not a supported game name, try \"cartpole\" or refer to the documentation for adding a new game.'.format(\n",
        "                    self.game_name\n",
        "                )\n",
        "            )\n",
        "            raise err\n",
        "\n",
        "        # Fix random generator seed\n",
        "        numpy.random.seed(self.config.seed)\n",
        "        torch.manual_seed(self.config.seed)\n",
        "\n",
        "        # Weights used to initialize components\n",
        "        self.muzero_weights = models.MuZeroNetwork(self.config).get_weights()\n",
        "\n",
        "    def train(self):\n",
        "        ray.init()\n",
        "        os.makedirs(self.config.results_path, exist_ok=True)\n",
        "        writer = SummaryWriter(self.config.results_path)\n",
        "\n",
        "        # Initialize workers\n",
        "        training_worker = trainer.Trainer.options(\n",
        "            num_gpus=1 if \"cuda\" in self.config.training_device else 0\n",
        "        ).remote(copy.deepcopy(self.muzero_weights), self.config)\n",
        "        shared_storage_worker = shared_storage.SharedStorage.remote(\n",
        "            copy.deepcopy(self.muzero_weights), self.game_name, self.config,\n",
        "        )\n",
        "        replay_buffer_worker = replay_buffer.ReplayBuffer.remote(self.config)\n",
        "        self_play_workers = [\n",
        "            self_play.SelfPlay.remote(\n",
        "                copy.deepcopy(self.muzero_weights),\n",
        "                self.Game(self.config.seed + seed),\n",
        "                self.config,\n",
        "            )\n",
        "            for seed in range(self.config.num_actors)\n",
        "        ]\n",
        "        test_worker = self_play.SelfPlay.remote(\n",
        "            copy.deepcopy(self.muzero_weights),\n",
        "            self.Game(self.config.seed + self.config.num_actors),\n",
        "            self.config,\n",
        "        )\n",
        "\n",
        "        # Launch workers\n",
        "        [\n",
        "            self_play_worker.continuous_self_play.remote(\n",
        "                shared_storage_worker, replay_buffer_worker\n",
        "            )\n",
        "            for self_play_worker in self_play_workers\n",
        "        ]\n",
        "        test_worker.continuous_self_play.remote(shared_storage_worker, None, True)\n",
        "        training_worker.continuous_update_weights.remote(\n",
        "            replay_buffer_worker, shared_storage_worker\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            \"\\nTraining...\\nRun tensorboard --logdir ./results and go to http://localhost:6006/ to see in real time the training performance.\\n\"\n",
        "        )\n",
        "        # Save hyperparameters to TensorBoard\n",
        "        hp_table = [\n",
        "            \"| {} | {} |\".format(key, value)\n",
        "            for key, value in self.config.__dict__.items()\n",
        "        ]\n",
        "        writer.add_text(\n",
        "            \"Hyperparameters\",\n",
        "            \"| Parameter | Value |\\n|-------|-------|\\n\" + \"\\n\".join(hp_table),\n",
        "        )\n",
        "        # Loop for monitoring in real time the workers\n",
        "        counter = 0\n",
        "        infos = ray.get(shared_storage_worker.get_infos.remote())\n",
        "        try:\n",
        "            while infos[\"training_step\"] < self.config.training_steps:\n",
        "                # Get and save real time performance\n",
        "                infos = ray.get(shared_storage_worker.get_infos.remote())\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/1.Total reward\", infos[\"total_reward\"], counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/2.Episode length\", infos[\"episode_length\"], counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/3.Player 0 MuZero reward\",\n",
        "                    infos[\"player_0_reward\"],\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/4.Player 1 Random reward\",\n",
        "                    infos[\"player_1_reward\"],\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/1.Self played games\",\n",
        "                    ray.get(replay_buffer_worker.get_self_play_count.remote()),\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/2.Training steps\", infos[\"training_step\"], counter\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/3.Self played games per training step ratio\",\n",
        "                    ray.get(replay_buffer_worker.get_self_play_count.remote())\n",
        "                    / max(1, infos[\"training_step\"]),\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\"2.Workers/4.Learning rate\", infos[\"lr\"], counter)\n",
        "                writer.add_scalar(\n",
        "                    \"3.Loss/1.Total weighted loss\", infos[\"total_loss\"], counter\n",
        "                )\n",
        "                writer.add_scalar(\"3.Loss/Value loss\", infos[\"value_loss\"], counter)\n",
        "                writer.add_scalar(\"3.Loss/Reward loss\", infos[\"reward_loss\"], counter)\n",
        "                writer.add_scalar(\"3.Loss/Policy loss\", infos[\"policy_loss\"], counter)\n",
        "                print(\n",
        "                    \"Last test reward: {0:.2f}. Training step: {1}/{2}. Played games: {3}. Loss: {4:.2f}\".format(\n",
        "                        infos[\"total_reward\"],\n",
        "                        infos[\"training_step\"],\n",
        "                        self.config.training_steps,\n",
        "                        ray.get(replay_buffer_worker.get_self_play_count.remote()),\n",
        "                        infos[\"total_loss\"],\n",
        "                    ),\n",
        "                    end=\"\\r\",\n",
        "                )\n",
        "                counter += 1\n",
        "                time.sleep(0.5)\n",
        "        except KeyboardInterrupt as err:\n",
        "            # Comment the line below to be able to stop the training but keep running\n",
        "            # raise err\n",
        "            pass\n",
        "        self.muzero_weights = ray.get(shared_storage_worker.get_weights.remote())\n",
        "        # End running actors\n",
        "        ray.shutdown()\n",
        "\n",
        "    def test(self, render, opponent, muzero_player):\n",
        "        \"\"\"\n",
        "        Test the model in a dedicated thread.\n",
        "\n",
        "        Args:\n",
        "            render: Boolean to display or not the environment.\n",
        "\n",
        "            opponent: \"self\" for self-play, \"human\" for playing against MuZero and \"random\"\n",
        "            for a random agent.\n",
        "\n",
        "            muzero_player: Integer with the player number of MuZero in case of multiplayer\n",
        "            games, None let MuZero play all players turn by turn.\n",
        "        \"\"\"\n",
        "        print(\"\\nTesting...\")\n",
        "        ray.init()\n",
        "        self_play_workers = self_play.SelfPlay.remote(\n",
        "            copy.deepcopy(self.muzero_weights),\n",
        "            self.Game(self.config.seed + self.config.num_actors),\n",
        "            self.config,\n",
        "        )\n",
        "        history = ray.get(\n",
        "            self_play_workers.play_game.remote(0, 0, render, opponent, muzero_player)\n",
        "        )\n",
        "        ray.shutdown()\n",
        "        return sum(history.reward_history)\n",
        "\n",
        "    def load_model(self, path=None):\n",
        "        if not path:\n",
        "            path = os.path.join(self.config.results_path, \"model.weights\")\n",
        "        try:\n",
        "            self.muzero_weights = torch.load(path)\n",
        "            print(\"\\nUsing weights from {}\".format(path))\n",
        "        except FileNotFoundError:\n",
        "            print(\"\\nThere is no model saved in {}.\".format(path))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nWelcome to MuZero! Here's a list of games:\")\n",
        "    # Let user pick a game\n",
        "    games = [\n",
        "        filename[:-3]\n",
        "        for filename in sorted(os.listdir(\"./games\"))\n",
        "        if filename.endswith(\".py\") and filename != \"abstract_game.py\"\n",
        "    ]\n",
        "    for i in range(len(games)):\n",
        "        print(\"{}. {}\".format(i, games[i]))\n",
        "    choice = input(\"Enter a number to choose the game: \")\n",
        "    valid_inputs = [str(i) for i in range(len(games))]\n",
        "    while choice not in valid_inputs:\n",
        "        choice = input(\"Invalid input, enter a number listed above: \")\n",
        "\n",
        "    # Initialize MuZero\n",
        "    choice = int(choice)\n",
        "    muzero = MuZero(games[choice])\n",
        "\n",
        "    while True:\n",
        "        # Configure running options\n",
        "        options = [\n",
        "            \"Train\",\n",
        "            \"Load pretrained model\",\n",
        "            \"Render some self play games\",\n",
        "            \"Play against MuZero\",\n",
        "            \"Exit\",\n",
        "        ]\n",
        "        print()\n",
        "        for i in range(len(options)):\n",
        "            print(\"{}. {}\".format(i, options[i]))\n",
        "\n",
        "        choice = input(\"Enter a number to choose an action: \")\n",
        "        valid_inputs = [str(i) for i in range(len(options))]\n",
        "        while choice not in valid_inputs:\n",
        "            choice = input(\"Invalid input, enter a number listed above: \")\n",
        "        choice = int(choice)\n",
        "        if choice == 0:\n",
        "            muzero.train()\n",
        "        elif choice == 1:\n",
        "            path = input(\"Enter a path to the model.weights: \")\n",
        "            while not os.path.isfile(path):\n",
        "                path = input(\"Invalid path. Try again: \")\n",
        "            muzero.load_model(path)\n",
        "        elif choice == 2:\n",
        "            muzero.test(render=True, opponent=\"self\", muzero_player=None)\n",
        "        elif choice == 3:\n",
        "            muzero.test(render=True, opponent=\"human\", muzero_player=0)\n",
        "        else:\n",
        "            break\n",
        "        print(\"\\nDone\")\n",
        "\n",
        "    ## Successive training, create a new config file for each experiment\n",
        "    # experiments = [\"cartpole\", \"tictactoe\"]\n",
        "    # for experiment in experiments:\n",
        "    #     print(\"\\nStarting experiment {}\".format(experiment))\n",
        "    #     try:\n",
        "    #         muzero = MuZero(experiment)\n",
        "    #         muzero.train()\n",
        "    #     except:\n",
        "    #         print(\"Skipping {}, an error has occurred.\".format(experiment))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX6d19133bbG",
        "colab_type": "text"
      },
      "source": [
        "#MCTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2App9nS23cey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "EPS = 1e-8\n",
        "\n",
        "class MCTS():\n",
        "    \"\"\"\n",
        "    This class handles the MCTS tree.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "        self.args = args\n",
        "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
        "        self.Nsa = {}       # stores #times edge s,a was visited\n",
        "        self.Ns = {}        # stores #times board s was visited\n",
        "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
        "\n",
        "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
        "        self.Vs = {}        # stores game.getValidMoves for board s\n",
        "        \n",
        "        self.plot = [1000]\n",
        "        self.num_sim = [0]\n",
        "\n",
        "    def getActionProb(self, graphState, temp=1):\n",
        "        \"\"\"\n",
        "        This function performs numMCTSSims simulations of MCTS starting from\n",
        "        canonicalBoard.\n",
        "        Returns:\n",
        "            probs: a policy vector where the probability of the ith action is\n",
        "                   proportional to Nsa[(s,a)]**(1./temp)\n",
        "        \"\"\"\n",
        "        for i in range(self.args.numMCTSSims):\n",
        "            self.search(graphState, i)\n",
        "\n",
        "        s = self.game.stringRepresentation(graphState)\n",
        "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
        "\n",
        "        if temp==0:\n",
        "            bestA = np.argmax(counts)\n",
        "            probs = [0]*len(counts)\n",
        "            probs[bestA]=1\n",
        "            return probs\n",
        "\n",
        "        counts = [x**(1./temp) for x in counts]\n",
        "        counts_sum = float(sum(counts))\n",
        "        probs = [x/counts_sum for x in counts]\n",
        "        return probs\n",
        "\n",
        "\n",
        "    def search(self, graphState, num_sim):\n",
        "        \"\"\"\n",
        "        This function performs one iteration of MCTS. It is recursively called\n",
        "        till a leaf node is found. The action chosen at each node is one that\n",
        "        has the maximum upper confidence bound as in the paper.\n",
        "        Once a leaf node is found, the neural network is called to return an\n",
        "        initial policy P and a value v for the state. This value is propagated\n",
        "        up the search path. In case the leaf node is a terminal state, the\n",
        "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
        "        updated.\n",
        "        NOTE: the return values are the negative of the value of the current\n",
        "        state. This is done since v is in [-1,1] and if v is the value of a\n",
        "        state for the current player, then its value is -v for the other player.\n",
        "        Returns:\n",
        "            v: the negative of the value of the current canonicalBoard\n",
        "        \"\"\"\n",
        "\n",
        "        s = self.game.stringRepresentation(graphState)\n",
        "\n",
        "        if s not in self.Es:\n",
        "            self.Es[s] = self.game.getGameEnded(graphState)\n",
        "        if self.Es[s]!=0:\n",
        "            # terminal node\n",
        "            return 0\n",
        "\n",
        "        if s not in self.Ps:\n",
        "            # leaf node\n",
        "            if self.nnet is not None:\n",
        "                self.Ps[s], v = self.nnet.predict(graphState, self.game.graph)\n",
        "            else:\n",
        "                self.Ps[s] = np.ones(self.game.getActionSize()) # random policy\n",
        "                v = 0\n",
        "            valids = self.game.getValidMoves(graphState)\n",
        "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves\n",
        "            sum_Ps_s = np.sum(self.Ps[s])\n",
        "            if sum_Ps_s > 0:\n",
        "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
        "            else:\n",
        "                # if all valid moves were masked make all valid moves equally probable\n",
        "                \n",
        "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
        "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
        "                print(\"All valid moves were masked, do workaround.\")\n",
        "                self.Ps[s] = self.Ps[s] + valids\n",
        "                self.Ps[s] /= np.sum(self.Ps[s])\n",
        "\n",
        "            self.Vs[s] = valids\n",
        "            self.Ns[s] = 0\n",
        "            return v\n",
        "\n",
        "        valids = self.Vs[s]\n",
        "        cur_best = -float('inf')\n",
        "        best_act = -1\n",
        "\n",
        "        # pick the action with the highest upper confidence bound\n",
        "        for a in range(self.game.getActionSize()):\n",
        "            if valids[a]:\n",
        "                if (s,a) in self.Qsa:\n",
        "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
        "                else:\n",
        "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + EPS)     # Q = 0 ?\n",
        "\n",
        "                if u > cur_best:\n",
        "                    cur_best = u\n",
        "                    best_act = a\n",
        "\n",
        "        a = best_act\n",
        "        next_s, reward = self.game.getNextState(graphState, a)\n",
        "        # next_s = self.game.getCanonicalForm(next_s, next_player)\n",
        "\n",
        "        v = self.search(next_s, num_sim) + reward\n",
        "\n",
        "        if (s,a) in self.Qsa:\n",
        "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
        "            self.Nsa[(s,a)] += 1\n",
        "\n",
        "        else:\n",
        "            self.Qsa[(s,a)] = v\n",
        "            self.Nsa[(s,a)] = 1\n",
        "            \n",
        "        if v > self.game.getActionSize() - self.plot[-1]:\n",
        "            self.plot.append(self.game.getActionSize() - v)\n",
        "            self.num_sim.append(num_sim)\n",
        "\n",
        "        self.Ns[s] += 1\n",
        "        return v"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}