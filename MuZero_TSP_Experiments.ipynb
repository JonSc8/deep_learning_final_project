{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuZero-TSP-Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2-9WODNu4bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import os\n",
        "import gym\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "# from .abstract_game import AbstractGame\n",
        "\n",
        "observation_shape,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        reduced_channels,\n",
        "        fc_reward_layers,\n",
        "        full_support_size,\n",
        "        block_output_size,"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os38wzt_wv8N",
        "colab_type": "text"
      },
      "source": [
        "#MuZero Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE_Dy-lLvvJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MuZeroConfig:\n",
        "    def __init__(self, num_node):\n",
        "        self.seed = 0  # Seed for numpy, torch and the game\n",
        "        ### Game\n",
        "\n",
        "        #What are the observations of our TSP?\n",
        "        self.observation_shape = (1, num_node, 2)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n",
        "        #How many possible actions do we have\n",
        "        self.action_space = [i for i in range(num_node-1)]  # Fixed list of all possible actions. You should only edit the length\n",
        "        #Fixed to single player\n",
        "        self.players = [i for i in range(1)]  # List of players. You should only edit the length\n",
        "        self.stacked_observations = 0  # Number of previous observation and previous actions to add to the current observation\n",
        "\n",
        "\n",
        "\n",
        "        ### Self-Play\n",
        "        self.num_actors = 1  # Number of simultaneous threads self-playing to feed the replay buffer\n",
        "        self.max_moves = num_node  # Maximum number of moves if game is not finished before !!! NEED TO UNDERSTAND WHAT MAX_MOVES means !!!\n",
        "        #To be changed later\n",
        "        self.num_simulations = 25  # Number of future moves self-simulated\n",
        "        self.discount = 1  # Chronological discount of the reward\n",
        "        #What?\n",
        "        self.temperature_threshold = 6  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n",
        "\n",
        "        # Root prior exploration noise\n",
        "        self.root_dirichlet_alpha = 0.1\n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB formula\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "\n",
        "\n",
        "        ### Network\n",
        "        self.network = \"resnet\"  # \"resnet\" / \"fullyconnected\"\n",
        "        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n",
        "\n",
        "        # Residual Network\n",
        "        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n",
        "        self.blocks = 1  # Number of blocks in the ResNet\n",
        "        self.channels = 16  # Number of channels in the ResNet\n",
        "        self.reduced_channels = 16  # Number of channels before heads of dynamic and prediction networks\n",
        "        self.resnet_fc_reward_layers = [8]  # Define the hidden layers in the reward head of the dynamic network\n",
        "        self.resnet_fc_value_layers = [8]  # Define the hidden layers in the value head of the prediction network\n",
        "        self.resnet_fc_policy_layers = [8]  # Define the hidden layers in the policy head of the prediction network\n",
        "\n",
        "        # Fully Connected Network\n",
        "        self.encoding_size = 32\n",
        "        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n",
        "        self.fc_value_layers = []  # Define the hidden layers in the value network\n",
        "        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n",
        "        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n",
        "        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n",
        "\n",
        "\n",
        "\n",
        "        ### Training\n",
        "        self.results_path = os.path.join(os.path.dirname(\"/content/drive/My Drive/deep-learning-final\"), \"/results\", os.path.basename(\"/content/drive/My Drive/deep-learning-final\")[:-3], datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\"))  # Path to store the model weights and TensorBoard logs\n",
        "        self.training_steps = 100000  # Total number of training steps (ie weights update according to a batch)\n",
        "        self.batch_size = 64  # Number of parts of games to train on at each training step\n",
        "        self.checkpoint_interval = 10  # Number of training steps before using the model for sef-playing\n",
        "        self.value_loss_weight = 0.25  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n",
        "        self.training_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Train on GPU if available\n",
        "\n",
        "        self.optimizer = \"Adam\"  # \"Adam\" or \"SGD\". Paper uses SGD\n",
        "        self.weight_decay = 1e-4  # L2 weights regularization\n",
        "        self.momentum = 0.9  # Used only if optimizer is SGD\n",
        "\n",
        "        # Exponential learning rate schedule\n",
        "        self.lr_init = 0.01  # Initial learning rate\n",
        "        self.lr_decay_rate = 1  # Set it to 1 to use a constant learning rate\n",
        "        self.lr_decay_steps = 10000\n",
        "\n",
        "\n",
        "        ### Replay Buffer\n",
        "        self.window_size = 3000  # Number of self-play games to keep in the replay buffer\n",
        "        self.num_unroll_steps = 20  # Number of game moves to keep for every batch element\n",
        "        self.td_steps = 20  # Number of steps in the future to take into account for calculating the target value\n",
        "        self.use_last_model_value = False  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n",
        "\n",
        "        # Prioritized Replay (See paper appendix Training)\n",
        "        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n",
        "        self.use_max_priority = True  # Use the n-step TD error as initial priority. Better for large replay buffer\n",
        "        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n",
        "        self.PER_beta = 1.0\n",
        "\n",
        "\n",
        "        ### Adjust the self play / training ratio to avoid over/underfitting\n",
        "        self.self_play_delay = 0  # Number of seconds to wait after each played game\n",
        "        self.training_delay = 0  # Number of seconds to wait after each training step\n",
        "        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n",
        "\n",
        "        def visit_softmax_temperature_fn(self, trained_steps):\n",
        "          \"\"\"\n",
        "          Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n",
        "          The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n",
        "\n",
        "          Returns:\n",
        "              Positive float.\n",
        "          \"\"\"\n",
        "          return 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I34AgKsGv5CF",
        "colab_type": "code",
        "outputId": "b3538bfd-a952-4905-ac5b-04c6ca3eeff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class Game():\n",
        "    \"\"\"\n",
        "    Game wrapper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_node, seed=None):\n",
        "        print(\"here\")\n",
        "        self.env = TSP(num_node)\n",
        "\n",
        "    def step(self, action, state):\n",
        "        \"\"\"\n",
        "        Apply action to the game.\n",
        "        \n",
        "        Args:\n",
        "            action : action of the action_space to take.\n",
        "\n",
        "        Returns:\n",
        "            The new observation, the reward and a boolean if the game has ended.\n",
        "        \"\"\"\n",
        "        state, reward, done = self.env.step(action, state)\n",
        "        # return state, reward * 20, done ---> need to check why reward*20 \n",
        "        return state, reward, done\n",
        "\n",
        "    def to_play(self):\n",
        "        \"\"\"\n",
        "        Return the current player.\n",
        "\n",
        "        Returns:\n",
        "            The current player, it should be an element of the players list in the config. \n",
        "        \"\"\"\n",
        "        return self.env.to_play()\n",
        "\n",
        "    def legal_actions(self, state):\n",
        "        \"\"\"\n",
        "        Should return the legal actions at each turn, if it is not available, it can return\n",
        "        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n",
        "        \n",
        "        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n",
        "        equal to the action space but to return a negative reward if the action is illegal.\n",
        "    \n",
        "        Returns:\n",
        "            An array of integers, subset of the action space.\n",
        "        \"\"\"\n",
        "        return self.env.legal_actions()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the game for a new game.\n",
        "        \n",
        "        Returns:\n",
        "            Initial state of the game.\n",
        "        \"\"\"\n",
        "        return self.env.reset()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Properly close the game.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def render(self, state):\n",
        "        \"\"\"\n",
        "        Display the game state and graph\n",
        "        \"\"\"\n",
        "        self.env.render()\n",
        "\n",
        "    def encode_board(self):\n",
        "        # return self.env.encode_board()\n",
        "        pass\n",
        "\n",
        "    def human_to_action(self):\n",
        "        \"\"\"\n",
        "        For multiplayer games, ask the user for a legal action\n",
        "        and return the corresponding action number.\n",
        "\n",
        "        Returns:\n",
        "            An integer from the action space.\n",
        "        \"\"\"\n",
        "        # while True:\n",
        "        #     try:\n",
        "        #         row = int(\n",
        "        #             input(\n",
        "        #                 \"Enter the row (1, 2 or 3) to play for the player {}: \".format(\n",
        "        #                     self.to_play()\n",
        "        #                 )\n",
        "        #             )\n",
        "        #         )\n",
        "        #         col = int(\n",
        "        #             input(\n",
        "        #                 \"Enter the column (1, 2 or 3) to play for the player {}: \".format(\n",
        "        #                     self.to_play()\n",
        "        #                 )\n",
        "        #             )\n",
        "        #         )\n",
        "        #         choice = (row - 1) * 3 + (col - 1)\n",
        "        #         if (\n",
        "        #             choice in self.legal_actions()\n",
        "        #             and 1 <= row\n",
        "        #             and 1 <= col\n",
        "        #             and row <= 3\n",
        "        #             and col <= 3\n",
        "        #         ):\n",
        "        #             break\n",
        "        #     except:\n",
        "        #         pass\n",
        "        #     print(\"Wrong input, try again\")\n",
        "        # return choice\n",
        "\n",
        "        pass\n",
        "\n",
        "    def action_to_string(self, action_number):\n",
        "        # \"\"\"\n",
        "        # Convert an action number to a string representing the action.\n",
        "        \n",
        "        # Args:\n",
        "        #     action_number: an integer from the action space.\n",
        "\n",
        "        # Returns:\n",
        "        #     String representing the action.\n",
        "        # \"\"\"\n",
        "        # row = 3 - action_number // 3\n",
        "        # col = action_number % 3 + 1\n",
        "        # return \"Play row {}, column {}\".format(row, col)\n",
        "        pass\n",
        "      \n",
        "    def state_to_string(self, state):\n",
        "      \"\"\"\n",
        "      Input:\n",
        "          state: current state\n",
        "      Returns:\n",
        "          index of state\n",
        "      \"\"\"\n",
        "      s = ''\n",
        "      for i in range(self.num_node):\n",
        "          s += str(int(state[i, 0]))\n",
        "      return s"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP5DK92Iwqlt",
        "colab_type": "text"
      },
      "source": [
        "Class TSP\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPGc-1hdv_9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TSP:\n",
        "    def __init__(self, num_node):\n",
        "        self.graph = np.random.rand(num_node, 2)\n",
        "        self.player = 1\n",
        "        self.num_node = num_node\n",
        "        self.getInitState()\n",
        "      \n",
        "    def getInitState(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            first_state: a representation of the graph\n",
        "            left column representing visited nodes\n",
        "            right column will always have a single 1 and the rest are 0's. index with the 1 in the right column is current node\n",
        "        \"\"\"\n",
        "        \n",
        "        # Always start with first node as current node \n",
        "        first_state = np.zeros([self.num_node, 2])\n",
        "        first_state[0,0] = 1\n",
        "        first_state[0,1] = 1\n",
        "        return first_state\n",
        "\n",
        "    def to_play(self):\n",
        "        return 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.random.rand(self.num_node, 2)\n",
        "        self.player = 1\n",
        "        return self.getInitState()\n",
        "    \n",
        "    def step(self, action, state):\n",
        "      next_s = state.copy()\n",
        "      # zero out current node\n",
        "      next_s[:, 1] = 0\n",
        "      # 1 in left column for visited, 1 in right column for current node\n",
        "      next_s[action, :] = 1\n",
        "      prev_a = np.where(state[:, 1] == 1)[0][0]\n",
        "      # get xy coordinates for prev_node and current_node from the graph\n",
        "      prev_node = self.graph[prev_a]\n",
        "      current_node = self.graph[action]\n",
        "      reward = 1 - np.linalg.norm(current_node - prev_node)\n",
        "      if self.num_node == np.sum(next_s[:, 0]): #end of game\n",
        "          reward += 1 - np.linalg.norm(current_node - self.graph[0])\n",
        "            \n",
        "      return next_s, reward, self.is_finished(state)\n",
        "\n",
        "\n",
        "    def get_observation(self):\n",
        "      # observation == state\n",
        "      \n",
        "      pass\n",
        "\n",
        "    def legal_actions(self, state):\n",
        "      return 1 - state[:, 0]\n",
        "\n",
        "    def is_finished(self, state):\n",
        "      \"\"\"\n",
        "      Input:\n",
        "        state: current state\n",
        "      Returns:\n",
        "        r: 0 if game has not ended. 1 if it has\n",
        "               \n",
        "      \"\"\"\n",
        "      end = False\n",
        "      if self.num_node == np.sum(state[:, 0]):\n",
        "          end = True\n",
        "      return end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL8WKY-PwEVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render(self, state):\n",
        "    print(\"State:\")\n",
        "    print(state)\n",
        "    print(\"Graph:\")\n",
        "    print(self.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps41Nx7sibia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e5d55fe-c6cc-47aa-c64e-bf03e2db5794"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBLyOIYuwlRf",
        "colab_type": "text"
      },
      "source": [
        "#MuZero.Py Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY_-2CqqwgmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "084f6d1b-0f9e-470f-dec6-5471d019a476"
      },
      "source": [
        "import copy\n",
        "import importlib\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import ray\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "class MuZero:\n",
        "    \"\"\"\n",
        "    Main class to manage MuZero.\n",
        "\n",
        "    Args:\n",
        "        game_name (str): Name of the game module, it should match the name of a .py file\n",
        "        in the \"./games\" directory.\n",
        "\n",
        "    Example:\n",
        "        >>> muzero = MuZero(\"cartpole\")\n",
        "        >>> muzero.train()\n",
        "        >>> muzero.test()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game_name, num_node):\n",
        "        self.game_name = game_name\n",
        "\n",
        "        # Load the game and the config from the module with the game name\n",
        "        try:\n",
        "            # game_module = importlib.import_module(\"games.\" + self.game_name)\n",
        "            self.config = MuZeroConfig(num_node)\n",
        "            self.Game = TSP(num_node)\n",
        "        except Exception as err:\n",
        "            print(\n",
        "                '{} is not a supported game name, try \"cartpole\" or refer to the documentation for adding a new game.'.format(\n",
        "                    self.game_name\n",
        "                )\n",
        "            )\n",
        "            raise err\n",
        "\n",
        "        # Fix random generator seed\n",
        "        numpy.random.seed(self.config.seed)\n",
        "        torch.manual_seed(self.config.seed)\n",
        "\n",
        "        # Weights used to initialize components\n",
        "        self.muzero_weights = MuZeroNetwork(self.config).get_weights()\n",
        "\n",
        "    def train(self):\n",
        "        ray.init(ignore_reinit_error=True)\n",
        "        training_worker = Trainer.remote(copy.deepcopy(self.muzero_weights), self.config)\n",
        "        self_play_workers = [\n",
        "            SelfPlay.remote(\n",
        "                copy.deepcopy(self.muzero_weights),\n",
        "                self.Game, #maybe need to add seed back!\n",
        "                self.config,\n",
        "            )\n",
        "            for seed in range(self.config.num_actors)\n",
        "        ]\n",
        "        shared_storage_worker = SharedStorage.remote(\n",
        "            copy.deepcopy(self.muzero_weights), self.game_name, self.config,\n",
        "        )\n",
        "        os.makedirs(self.config.results_path, exist_ok=True)\n",
        "        writer = SummaryWriter(self.config.results_path)\n",
        "\n",
        "        print(\"1\")\n",
        "        replay_buffer_worker = ReplayBuffer.remote(self.config)\n",
        "        print(\"2\")\n",
        "        test_worker = SelfPlay.remote(\n",
        "            copy.deepcopy(self.muzero_weights),\n",
        "            self.Game, # maybe need to put seed back !!!\n",
        "            self.config,\n",
        "        )\n",
        "        print(\"3\")\n",
        "        # Launch workers\n",
        "        [\n",
        "            self_play_worker.continuous_self_play.remote(\n",
        "                shared_storage_worker, replay_buffer_worker\n",
        "            )\n",
        "            for self_play_worker in self_play_workers\n",
        "        ]\n",
        "        print(\"4\")\n",
        "        test_worker.continuous_self_play.remote(shared_storage_worker, None, True)\n",
        "        print(\"5\")\n",
        "        training_worker.continuous_update_weights.remote(\n",
        "            replay_buffer_worker, shared_storage_worker\n",
        "        )\n",
        "        print(\"6\")\n",
        "\n",
        "        print(\n",
        "            \"\\nTraining...\\nRun tensorboard --logdir ./results and go to http://localhost:6006/ to see in real time the training performance.\\n\"\n",
        "        )\n",
        "        # Save hyperparameters to TensorBoard\n",
        "        hp_table = [\n",
        "            \"| {} | {} |\".format(key, value)\n",
        "            for key, value in self.config.__dict__.items()\n",
        "        ]\n",
        "        writer.add_text(\n",
        "            \"Hyperparameters\",\n",
        "            \"| Parameter | Value |\\n|-------|-------|\\n\" + \"\\n\".join(hp_table),\n",
        "        )\n",
        "        # Loop for monitoring in real time the workers\n",
        "        counter = 0\n",
        "        infos = ray.get(shared_storage_worker.get_infos.remote())\n",
        "        try:\n",
        "            while infos[\"training_step\"] < self.config.training_steps:\n",
        "                # Get and save real time performance\n",
        "                infos = ray.get(shared_storage_worker.get_infos.remote())\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/1.Total reward\", infos[\"total_reward\"], counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/2.Episode length\", infos[\"episode_length\"], counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/3.Player 0 MuZero reward\",\n",
        "                    infos[\"player_0_reward\"],\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/4.Player 1 Random reward\",\n",
        "                    infos[\"player_1_reward\"],\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/1.Self played games\",\n",
        "                    ray.get(replay_buffer_worker.get_self_play_count.remote()),\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/2.Training steps\", infos[\"training_step\"], counter\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/3.Self played games per training step ratio\",\n",
        "                    ray.get(replay_buffer_worker.get_self_play_count.remote())\n",
        "                    / max(1, infos[\"training_step\"]),\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\"2.Workers/4.Learning rate\", infos[\"lr\"], counter)\n",
        "                writer.add_scalar(\n",
        "                    \"3.Loss/1.Total weighted loss\", infos[\"total_loss\"], counter\n",
        "                )\n",
        "                writer.add_scalar(\"3.Loss/Value loss\", infos[\"value_loss\"], counter)\n",
        "                writer.add_scalar(\"3.Loss/Reward loss\", infos[\"reward_loss\"], counter)\n",
        "                writer.add_scalar(\"3.Loss/Policy loss\", infos[\"policy_loss\"], counter)\n",
        "                print(\n",
        "                    \"Last test reward: {0:.2f}. Training step: {1}/{2}. Played games: {3}. Loss: {4:.2f}\".format(\n",
        "                        infos[\"total_reward\"],\n",
        "                        infos[\"training_step\"],\n",
        "                        self.config.training_steps,\n",
        "                        ray.get(replay_buffer_worker.get_self_play_count.remote()),\n",
        "                        infos[\"total_loss\"],\n",
        "                    ),\n",
        "                    end=\"\\r\",\n",
        "                )\n",
        "                counter += 1\n",
        "                time.sleep(0.5)\n",
        "        except KeyboardInterrupt as err:\n",
        "            # Comment the line below to be able to stop the training but keep running\n",
        "            # raise err\n",
        "            pass\n",
        "        self.muzero_weights = ray.get(shared_storage_worker.get_weights.remote())\n",
        "        # End running actors\n",
        "        ray.shutdown()\n",
        "\n",
        "    def test(self, render=True, opponent=\"self\", muzero_player=None):\n",
        "        \"\"\"\n",
        "        Test the model in a dedicated thread.\n",
        "\n",
        "        Args:\n",
        "            render: Boolean to display or not the environment.\n",
        "\n",
        "            opponent: \"self\" for self-play, \"human\" for playing against MuZero and \"random\"\n",
        "            for a random agent.\n",
        "\n",
        "            muzero_player: Integer with the player number of MuZero in case of multiplayer\n",
        "            games, None let MuZero play all players turn by turn.\n",
        "        \"\"\"\n",
        "        print(\"\\nTesting...\")\n",
        "        ray.init(ignore_reinit_error=True)\n",
        "        self_play = SelfPlay.remote()\n",
        "        self_play_workers = self_play.remote(\n",
        "            copy.deepcopy(self.muzero_weights),\n",
        "            self.Game, # maybe need to return seed back!!!!\n",
        "            self.config,\n",
        "        )\n",
        "        history = ray.get(\n",
        "            self_play_workers.play_game.remote(0, 0, render, opponent, muzero_player)\n",
        "        )\n",
        "        ray.shutdown()\n",
        "        return sum(history.reward_history)\n",
        "\n",
        "    def load_model(self, path=None):\n",
        "        if not path:\n",
        "            path = os.path.join(self.config.results_path, \"model.weights\")\n",
        "        try:\n",
        "            self.muzero_weights = torch.load(path)\n",
        "            print(\"\\nUsing weights from {}\".format(path))\n",
        "        except FileNotFoundError:\n",
        "            print(\"\\nThere is no model saved in {}.\".format(path))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize MuZero\n",
        "    num_node = 5\n",
        "    muzero = MuZero(\"TSP\", num_node)\n",
        "\n",
        "    while True:\n",
        "        # Configure running options\n",
        "        options = [\n",
        "            \"Train\",\n",
        "            \"Load pretrained model\",\n",
        "            \"Render some self play games\",\n",
        "            \"Exit\",\n",
        "        ]\n",
        "        print()\n",
        "        for i in range(len(options)):\n",
        "            print(\"{}. {}\".format(i, options[i]))\n",
        "\n",
        "        choice = input(\"Enter a number to choose an action: \")\n",
        "        valid_inputs = [str(i) for i in range(len(options))]\n",
        "        while choice not in valid_inputs:\n",
        "            choice = input(\"Invalid input, enter a number listed above: \")\n",
        "        choice = int(choice)\n",
        "        if choice == 0:\n",
        "            muzero.train()\n",
        "        elif choice == 1:\n",
        "            path = input(\"Enter a path to the model.weights: \") ### set 1 path and pass it as const \n",
        "            while not os.path.isfile(path):\n",
        "                path = input(\"Invalid path. Try again: \")\n",
        "            muzero.load_model(path)\n",
        "        elif choice == 2:\n",
        "            muzero.test(render=True, opponent=\"self\", muzero_player=None)\n",
        "        else:\n",
        "            break\n",
        "        print(\"\\nDone\")\n",
        "\n",
        "    ## Successive training, create a new config file for each experiment\n",
        "    # experiments = [\"cartpole\", \"tictactoe\"]\n",
        "    # for experiment in experiments:\n",
        "    #     print(\"\\nStarting experiment {}\".format(experiment))\n",
        "    #     try:\n",
        "    #         muzero = MuZero(experiment)\n",
        "    #         muzero.train()\n",
        "    #     except:\n",
        "    #         print(\"Skipping {}, an error has occurred.\".format(experiment))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0. Train\n",
            "1. Load pretrained model\n",
            "2. Render some self play games\n",
            "3. Exit\n",
            "Enter a number to choose an action: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 15:14:26,373\tINFO resource_spec.py:212 -- Starting Ray with 7.13 GiB memory available for workers and up to 3.57 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-04-05 15:14:26,686\tINFO services.py:1148 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "\n",
            "Training...\n",
            "Run tensorboard --logdir ./results and go to http://localhost:6006/ to see in real time the training performance.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 15:14:30,307\tWARNING worker.py:1072 -- The dashboard on node fac15b5627c8 failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1062, in create_server\n",
            "    sock.bind(sa)\n",
            "OSError: [Errno 99] Cannot assign requested address\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 1142, in <module>\n",
            "    dashboard.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 570, in run\n",
            "    aiohttp.web.run_app(self.app, host=self.host, port=self.port)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 433, in run_app\n",
            "    reuse_port=reuse_port))\n",
            "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 484, in run_until_complete\n",
            "    return future.result()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 359, in _run_app\n",
            "    await site.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web_runner.py\", line 104, in start\n",
            "    reuse_port=self._reuse_port)\n",
            "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1066, in create_server\n",
            "    % (sa, err.strerror.lower()))\n",
            "OSError: [Errno 99] error while attempting to bind on address ('::1', 8265, 0, 0): cannot assign requested address\n",
            "\n",
            "2020-04-05 15:14:30,869\tWARNING worker.py:1072 -- WARNING: 6 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4521)\u001b[0m THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(pid=4506)\u001b[0m tensor([[[1., 1.],\n",
            "\u001b[2m\u001b[36m(pid=4506)\u001b[0m          [0., 0.],\n",
            "\u001b[2m\u001b[36m(pid=4506)\u001b[0m          [0., 0.],\n",
            "\u001b[2m\u001b[36m(pid=4506)\u001b[0m          [0., 0.],\n",
            "\u001b[2m\u001b[36m(pid=4506)\u001b[0m          [0., 0.]]])\n",
            "\u001b[2m\u001b[36m(pid=4542)\u001b[0m tensor([[[1., 1.],\n",
            "\u001b[2m\u001b[36m(pid=4542)\u001b[0m          [0., 0.],\n",
            "\u001b[2m\u001b[36m(pid=4542)\u001b[0m          [0., 0.],\n",
            "\u001b[2m\u001b[36m(pid=4542)\u001b[0m          [0., 0.],\n",
            "\u001b[2m\u001b[36m(pid=4542)\u001b[0m          [0., 0.]]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 15:14:37,217\tERROR worker.py:1012 -- Possible unhandled error from worker: \u001b[36mray::Trainer.__init__()\u001b[39m (pid=4521, ip=172.28.0.2)\n",
            "  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\n",
            "  File \"<ipython-input-22-116e65588825>\", line 27, in __init__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 425, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 223, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 423, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 197, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50\n",
            "2020-04-05 15:14:37,219\tERROR worker.py:1012 -- Possible unhandled error from worker: \u001b[36mray::Trainer.__init__()\u001b[39m (pid=4521, ip=172.28.0.2)\n",
            "  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\n",
            "  File \"<ipython-input-22-116e65588825>\", line 27, in __init__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 425, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 223, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 423, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 197, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 15:14:38,222\tERROR worker.py:1012 -- Possible unhandled error from worker: \u001b[36mray::SelfPlay.continuous_self_play()\u001b[39m (pid=4506, ip=172.28.0.2)\n",
            "  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\n",
            "  File \"<ipython-input-96-79c2928b38c4>\", line 52, in continuous_self_play\n",
            "  File \"<ipython-input-96-79c2928b38c4>\", line 131, in play_game\n",
            "  File \"<ipython-input-96-79c2928b38c4>\", line 250, in run\n",
            "  File \"<ipython-input-93-bf4957b7be6d>\", line 508, in initial_inference\n",
            "  File \"<ipython-input-93-bf4957b7be6d>\", line 433, in representation\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"<ipython-input-93-bf4957b7be6d>\", line 278, in forward\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 345, in forward\n",
            "    return self.conv2d_forward(input, self.weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 342, in conv2d_forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "RuntimeError: Expected 4-dimensional input for 4-dimensional weight 16 1 3 3, but got 3-dimensional input of size [1, 5, 2] instead\n",
            "2020-04-05 15:14:38,223\tERROR worker.py:1012 -- Possible unhandled error from worker: \u001b[36mray::SelfPlay.continuous_self_play()\u001b[39m (pid=4542, ip=172.28.0.2)\n",
            "  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n",
            "  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\n",
            "  File \"<ipython-input-96-79c2928b38c4>\", line 52, in continuous_self_play\n",
            "  File \"<ipython-input-96-79c2928b38c4>\", line 131, in play_game\n",
            "  File \"<ipython-input-96-79c2928b38c4>\", line 250, in run\n",
            "  File \"<ipython-input-93-bf4957b7be6d>\", line 508, in initial_inference\n",
            "  File \"<ipython-input-93-bf4957b7be6d>\", line 433, in representation\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"<ipython-input-93-bf4957b7be6d>\", line 278, in forward\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 345, in forward\n",
            "    return self.conv2d_forward(input, self.weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 342, in conv2d_forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "RuntimeError: Expected 4-dimensional input for 4-dimensional weight 16 1 3 3, but got 3-dimensional input of size [1, 5, 2] instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Done\n",
            "\n",
            "0. Train\n",
            "1. Load pretrained model\n",
            "2. Render some self play games\n",
            "3. Exit\n",
            "Enter a number to choose an action: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkhbDYvB3cPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "99e917e2-05f4-4da2-95e2-8baf9d484993"
      },
      "source": [
        "!tensorboard --logdir ./results"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.2.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao8Iwu6ndf7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class MuZeroNetwork:\n",
        "    def __new__(cls, config):\n",
        "        if config.network == \"fullyconnected\":\n",
        "            return MuZeroFullyConnectedNetwork(\n",
        "                config.observation_shape,\n",
        "                config.stacked_observations,\n",
        "                len(config.action_space),\n",
        "                config.encoding_size,\n",
        "                config.fc_reward_layers,\n",
        "                config.fc_value_layers,\n",
        "                config.fc_policy_layers,\n",
        "                config.fc_representation_layers,\n",
        "                config.fc_dynamics_layers,\n",
        "                config.support_size,\n",
        "            )\n",
        "        elif config.network == \"resnet\":\n",
        "            return MuZeroResidualNetwork(\n",
        "                config.observation_shape,\n",
        "                config.stacked_observations,\n",
        "                len(config.action_space),\n",
        "                config.blocks,\n",
        "                config.channels,\n",
        "                config.reduced_channels,\n",
        "                config.resnet_fc_reward_layers,\n",
        "                config.resnet_fc_value_layers,\n",
        "                config.resnet_fc_policy_layers,\n",
        "                config.support_size,\n",
        "                config.downsample,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'The network parameter should be \"fullyconnected\" or \"resnet\".'\n",
        "            )\n",
        "\n",
        "\n",
        "##################################\n",
        "######## Fully Connected #########\n",
        "\n",
        "\n",
        "class MuZeroFullyConnectedNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        stacked_observations,\n",
        "        action_space_size,\n",
        "        encoding_size,\n",
        "        fc_reward_layers,\n",
        "        fc_value_layers,\n",
        "        fc_policy_layers,\n",
        "        fc_representation_layers,\n",
        "        fc_dynamics_layers,\n",
        "        support_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.action_space_size = action_space_size\n",
        "        self.full_support_size = 2 * support_size + 1\n",
        "\n",
        "        self.representation_network = FullyConnectedNetwork(\n",
        "            observation_shape[0]\n",
        "            * observation_shape[1]\n",
        "            * observation_shape[2]\n",
        "            * (stacked_observations + 1)\n",
        "            + stacked_observations * observation_shape[1] * observation_shape[2],\n",
        "            fc_representation_layers,\n",
        "            encoding_size,\n",
        "        )\n",
        "\n",
        "        self.dynamics_encoded_state_network = FullyConnectedNetwork(\n",
        "            encoding_size + self.action_space_size, fc_dynamics_layers, encoding_size\n",
        "        )\n",
        "        self.dynamics_reward_network = FullyConnectedNetwork(\n",
        "            encoding_size + self.action_space_size,\n",
        "            fc_reward_layers,\n",
        "            self.full_support_size,\n",
        "        )\n",
        "\n",
        "        self.prediction_policy_network = FullyConnectedNetwork(\n",
        "            encoding_size, [], self.action_space_size\n",
        "        )\n",
        "        self.prediction_value_network = FullyConnectedNetwork(\n",
        "            encoding_size, fc_value_layers, self.full_support_size,\n",
        "        )\n",
        "\n",
        "    def prediction(self, encoded_state):\n",
        "        policy_logits = self.prediction_policy_network(encoded_state)\n",
        "        value = self.prediction_value_network(encoded_state)\n",
        "        return policy_logits, value\n",
        "\n",
        "    def representation(self, observation):\n",
        "        encoded_state = self.representation_network(\n",
        "            observation.view(observation.shape[0], -1)\n",
        "        )\n",
        "        # Scale encoded state between [0, 1] (See appendix paper Training)\n",
        "        min_encoded_state = encoded_state.min(1, keepdim=True)[0]\n",
        "        max_encoded_state = encoded_state.max(1, keepdim=True)[0]\n",
        "        scale_encoded_state = max_encoded_state - min_encoded_state\n",
        "        scale_encoded_state[scale_encoded_state == 0] = 1\n",
        "        encoded_state_normalized = (\n",
        "            encoded_state - min_encoded_state\n",
        "        ) / scale_encoded_state\n",
        "        return encoded_state_normalized\n",
        "\n",
        "    def dynamics(self, encoded_state, action):\n",
        "        # Stack encoded_state with a game specific one hot encoded action (See paper appendix Network Architecture)\n",
        "        action_one_hot = (\n",
        "            torch.zeros((action.shape[0], self.action_space_size))\n",
        "            .to(action.device)\n",
        "            .float()\n",
        "        )\n",
        "        action_one_hot.scatter_(1, action.long(), 1.0)\n",
        "        x = torch.cat((encoded_state, action_one_hot), dim=1)\n",
        "\n",
        "        next_encoded_state = self.dynamics_encoded_state_network(x)\n",
        "\n",
        "        # Scale encoded state between [0, 1] (See paper appendix Training)\n",
        "        min_next_encoded_state = next_encoded_state.min(1, keepdim=True)[0]\n",
        "        max_next_encoded_state = next_encoded_state.max(1, keepdim=True)[0]\n",
        "        scale_next_encoded_state = max_next_encoded_state - min_next_encoded_state\n",
        "        scale_next_encoded_state[scale_next_encoded_state == 0] = 1\n",
        "        next_encoded_state_normalized = (\n",
        "            next_encoded_state - min_next_encoded_state\n",
        "        ) / scale_next_encoded_state\n",
        "\n",
        "        reward = self.dynamics_reward_network(x)\n",
        "        return next_encoded_state_normalized, reward\n",
        "\n",
        "    def initial_inference(self, observation):\n",
        "        encoded_state = self.representation(observation)\n",
        "        policy_logits, value = self.prediction(encoded_state)\n",
        "        # reward equal to 0 for consistency\n",
        "        reward = (\n",
        "            torch.zeros(1, self.full_support_size)\n",
        "            .scatter(1, torch.tensor([[self.full_support_size // 2]]).long(), 1.0)\n",
        "            .repeat(len(observation), 1)\n",
        "            .to(observation.device)\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            value,\n",
        "            reward,\n",
        "            policy_logits,\n",
        "            encoded_state,\n",
        "        )\n",
        "\n",
        "    def recurrent_inference(self, encoded_state, action):\n",
        "        next_encoded_state, reward = self.dynamics(encoded_state, action)\n",
        "        policy_logits, value = self.prediction(next_encoded_state)\n",
        "        return value, reward, policy_logits, next_encoded_state\n",
        "\n",
        "    def get_weights(self):\n",
        "        return {key: value.cpu() for key, value in self.state_dict().items()}\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.load_state_dict(weights)\n",
        "\n",
        "\n",
        "###### End Fully Connected #######\n",
        "##################################\n",
        "\n",
        "\n",
        "##################################\n",
        "############# ResNet #############\n",
        "\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return torch.nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "# Residual block\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, num_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(num_channels, num_channels, stride)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(num_channels)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = conv3x3(num_channels, num_channels)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += x\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Downsample observations before representation network (See paper appendix Network Architecture)\n",
        "class DownSample(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels // 2,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.resblocks1 = torch.nn.ModuleList(\n",
        "            [ResidualBlock(out_channels // 2) for _ in range(2)]\n",
        "        )\n",
        "        self.conv2 = torch.nn.Conv2d(\n",
        "            out_channels // 2,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.resblocks2 = torch.nn.ModuleList(\n",
        "            [ResidualBlock(out_channels) for _ in range(3)]\n",
        "        )\n",
        "        self.pooling1 = torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.resblocks3 = torch.nn.ModuleList(\n",
        "            [ResidualBlock(out_channels) for _ in range(3)]\n",
        "        )\n",
        "        self.pooling2 = torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        for block in self.resblocks1:\n",
        "            out = block(out)\n",
        "        out = self.conv2(out)\n",
        "        for block in self.resblocks2:\n",
        "            out = block(out)\n",
        "        out = self.pooling1(out)\n",
        "        for block in self.resblocks3:\n",
        "            out = block(out)\n",
        "        out = self.pooling2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class RepresentationNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        stacked_observations,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        downsample,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_downsample = downsample\n",
        "        if self.use_downsample:\n",
        "            self.downsample = DownSample(\n",
        "                observation_shape[0] * (stacked_observations + 1)\n",
        "                + stacked_observations,\n",
        "                num_channels,\n",
        "            )\n",
        "        self.conv = conv3x3(\n",
        "            num_channels\n",
        "            if downsample\n",
        "            else observation_shape[0] * (stacked_observations + 1)\n",
        "            + stacked_observations,\n",
        "            num_channels,\n",
        "        )\n",
        "        self.bn = torch.nn.BatchNorm2d(num_channels)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.resblocks = torch.nn.ModuleList(\n",
        "            [ResidualBlock(num_channels) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_downsample:\n",
        "            out = self.downsample(x)\n",
        "        else:\n",
        "            out = x\n",
        "        out = self.conv(out)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        for block in self.resblocks:\n",
        "            out = block(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DynamicNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        reduced_channels,\n",
        "        fc_reward_layers,\n",
        "        full_support_size,\n",
        "        block_output_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.observation_shape = observation_shape\n",
        "        self.conv = conv3x3(num_channels, num_channels - 1)\n",
        "        self.bn = torch.nn.BatchNorm2d(num_channels - 1)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.resblocks = torch.nn.ModuleList(\n",
        "            [ResidualBlock(num_channels - 1) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = torch.nn.Conv2d(num_channels - 1, reduced_channels, 1)\n",
        "        self.block_output_size = block_output_size\n",
        "        self.fc = FullyConnectedNetwork(\n",
        "            self.block_output_size,\n",
        "            fc_reward_layers,\n",
        "            full_support_size,\n",
        "            activation=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        for block in self.resblocks:\n",
        "            out = block(out)\n",
        "        state = out\n",
        "        out = self.conv1x1(out)\n",
        "        out = out.view(-1, self.block_output_size)\n",
        "        reward = self.fc(out)\n",
        "        return state, reward\n",
        "\n",
        "\n",
        "class PredictionNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        action_space_size,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        reduced_channels,\n",
        "        fc_value_layers,\n",
        "        fc_policy_layers,\n",
        "        full_support_size,\n",
        "        block_output_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.observation_shape = observation_shape\n",
        "        self.resblocks = torch.nn.ModuleList(\n",
        "            [ResidualBlock(num_channels) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = torch.nn.Conv2d(num_channels, reduced_channels, 1)\n",
        "        self.block_output_size = block_output_size\n",
        "        self.fc_value = FullyConnectedNetwork(\n",
        "            self.block_output_size, fc_value_layers, full_support_size, activation=None,\n",
        "        )\n",
        "        self.fc_policy = FullyConnectedNetwork(\n",
        "            self.block_output_size,\n",
        "            fc_policy_layers,\n",
        "            action_space_size,\n",
        "            activation=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for block in self.resblocks:\n",
        "            out = block(out)\n",
        "        out = self.conv1x1(out)\n",
        "        out = out.view(-1, self.block_output_size)\n",
        "        value = self.fc_value(out)\n",
        "        policy = self.fc_policy(out)\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "class MuZeroResidualNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        stacked_observations,\n",
        "        action_space_size,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        reduced_channels,\n",
        "        fc_reward_layers,\n",
        "        fc_value_layers,\n",
        "        fc_policy_layers,\n",
        "        support_size,\n",
        "        downsample,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.action_space_size = action_space_size\n",
        "        self.full_support_size = 2 * support_size + 1\n",
        "        block_output_size = (\n",
        "            (\n",
        "                reduced_channels\n",
        "                * (observation_shape[1] // 16)\n",
        "                * (observation_shape[2] // 16)\n",
        "            )\n",
        "            if downsample\n",
        "            else (reduced_channels * observation_shape[1] * observation_shape[2])\n",
        "        )\n",
        "\n",
        "        self.representation_network = RepresentationNetwork(\n",
        "            observation_shape,\n",
        "            stacked_observations,\n",
        "            num_blocks,\n",
        "            num_channels,\n",
        "            downsample,\n",
        "        )\n",
        "\n",
        "        self.dynamics_network = DynamicNetwork(\n",
        "            observation_shape,\n",
        "            num_blocks,\n",
        "            num_channels + 1,\n",
        "            reduced_channels,\n",
        "            fc_reward_layers,\n",
        "            self.full_support_size,\n",
        "            block_output_size,\n",
        "        )\n",
        "\n",
        "        self.prediction_network = PredictionNetwork(\n",
        "            observation_shape,\n",
        "            action_space_size,\n",
        "            num_blocks,\n",
        "            num_channels,\n",
        "            reduced_channels,\n",
        "            fc_value_layers,\n",
        "            fc_policy_layers,\n",
        "            self.full_support_size,\n",
        "            block_output_size,\n",
        "        )\n",
        "\n",
        "    def prediction(self, encoded_state):\n",
        "        policy, value = self.prediction_network(encoded_state)\n",
        "        return policy, value\n",
        "\n",
        "    def representation(self, observation):\n",
        "        encoded_state = self.representation_network(observation)\n",
        "\n",
        "        # Scale encoded state between [0, 1] (See appendix paper Training)\n",
        "        min_encoded_state = (\n",
        "            encoded_state.view(\n",
        "                -1,\n",
        "                encoded_state.shape[1],\n",
        "                encoded_state.shape[2] * encoded_state.shape[3],\n",
        "            )\n",
        "            .min(2, keepdim=True)[0]\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        max_encoded_state = (\n",
        "            encoded_state.view(\n",
        "                -1,\n",
        "                encoded_state.shape[1],\n",
        "                encoded_state.shape[2] * encoded_state.shape[3],\n",
        "            )\n",
        "            .max(2, keepdim=True)[0]\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        scale_encoded_state = max_encoded_state - min_encoded_state\n",
        "        scale_encoded_state[scale_encoded_state == 0] = 1\n",
        "        encoded_state_normalized = (\n",
        "            encoded_state - min_encoded_state\n",
        "        ) / scale_encoded_state\n",
        "        return encoded_state_normalized\n",
        "\n",
        "    def dynamics(self, encoded_state, action):\n",
        "        # Stack encoded_state with a game specific one hot encoded action (See paper appendix Network Architecture)\n",
        "        action_one_hot = (\n",
        "            torch.ones(\n",
        "                (\n",
        "                    encoded_state.shape[0],\n",
        "                    1,\n",
        "                    encoded_state.shape[2],\n",
        "                    encoded_state.shape[3],\n",
        "                )\n",
        "            )\n",
        "            .to(action.device)\n",
        "            .float()\n",
        "        )\n",
        "        action_one_hot = (\n",
        "            action[:, :, None, None] * action_one_hot / self.action_space_size\n",
        "        )\n",
        "        x = torch.cat((encoded_state, action_one_hot), dim=1)\n",
        "        next_encoded_state, reward = self.dynamics_network(x)\n",
        "\n",
        "        # Scale encoded state between [0, 1] (See paper appendix Training)\n",
        "        min_next_encoded_state = (\n",
        "            next_encoded_state.view(\n",
        "                -1,\n",
        "                next_encoded_state.shape[1],\n",
        "                next_encoded_state.shape[2] * next_encoded_state.shape[3],\n",
        "            )\n",
        "            .min(2, keepdim=True)[0]\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        max_next_encoded_state = (\n",
        "            next_encoded_state.view(\n",
        "                -1,\n",
        "                next_encoded_state.shape[1],\n",
        "                next_encoded_state.shape[2] * next_encoded_state.shape[3],\n",
        "            )\n",
        "            .max(2, keepdim=True)[0]\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        scale_next_encoded_state = max_next_encoded_state - min_next_encoded_state\n",
        "        scale_next_encoded_state[scale_next_encoded_state == 0] = 1\n",
        "        next_encoded_state_normalized = (\n",
        "            next_encoded_state - min_next_encoded_state\n",
        "        ) / scale_next_encoded_state\n",
        "        return next_encoded_state_normalized, reward\n",
        "\n",
        "    def initial_inference(self, observation):\n",
        "        encoded_state = self.representation(observation)\n",
        "        policy_logits, value = self.prediction(encoded_state)\n",
        "        # reward equal to 0 for consistency\n",
        "        reward = (\n",
        "            torch.zeros(1, self.full_support_size)\n",
        "            .scatter(1, torch.tensor([[self.full_support_size // 2]]).long(), 1.0)\n",
        "            .repeat(len(observation), 1)\n",
        "            .to(observation.device)\n",
        "        )\n",
        "        return (\n",
        "            value,\n",
        "            reward,\n",
        "            policy_logits,\n",
        "            encoded_state,\n",
        "        )\n",
        "\n",
        "    def recurrent_inference(self, encoded_state, action):\n",
        "        next_encoded_state, reward = self.dynamics(encoded_state, action)\n",
        "        policy_logits, value = self.prediction(next_encoded_state)\n",
        "        return value, reward, policy_logits, next_encoded_state\n",
        "\n",
        "    def get_weights(self):\n",
        "        return {key: value.cpu() for key, value in self.state_dict().items()}\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.load_state_dict(weights)\n",
        "\n",
        "\n",
        "########### End ResNet ###########\n",
        "##################################\n",
        "\n",
        "\n",
        "class FullyConnectedNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_size, layer_sizes, output_size, activation=None):\n",
        "        super().__init__()\n",
        "        size_list = [input_size] + layer_sizes\n",
        "        layers = []\n",
        "        if 1 < len(size_list):\n",
        "            for i in range(len(size_list) - 1):\n",
        "                layers.extend(\n",
        "                    [\n",
        "                        torch.nn.Linear(size_list[i], size_list[i + 1]),\n",
        "                        torch.nn.LeakyReLU(),\n",
        "                    ]\n",
        "                )\n",
        "        layers.append(torch.nn.Linear(size_list[-1], output_size))\n",
        "        if activation:\n",
        "            layers.append(activation)\n",
        "        self.layers = torch.nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def support_to_scalar(logits, support_size):\n",
        "    \"\"\"\n",
        "    Transform a categorical representation to a scalar\n",
        "    See paper appendix Network Architecture\n",
        "    \"\"\"\n",
        "    # Decode to a scalar\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    support = (\n",
        "        torch.tensor([x for x in range(-support_size, support_size + 1)])\n",
        "        .expand(probabilities.shape)\n",
        "        .float()\n",
        "        .to(device=probabilities.device)\n",
        "    )\n",
        "    x = torch.sum(support * probabilities, dim=1, keepdim=True)\n",
        "\n",
        "    # Invert the scaling (defined in https://arxiv.org/abs/1805.11593)\n",
        "    x = torch.sign(x) * (\n",
        "        ((torch.sqrt(1 + 4 * 0.001 * (torch.abs(x) + 1 + 0.001)) - 1) / (2 * 0.001))\n",
        "        ** 2\n",
        "        - 1\n",
        "    )\n",
        "    return x\n",
        "\n",
        "\n",
        "def scalar_to_support(x, support_size):\n",
        "    \"\"\"\n",
        "    Transform a scalar to a categorical representation with (2 * support_size + 1) categories\n",
        "    See paper appendix Network Architecture\n",
        "    \"\"\"\n",
        "    # Reduce the scale (defined in https://arxiv.org/abs/1805.11593)\n",
        "    x = torch.sign(x) * (torch.sqrt(torch.abs(x) + 1) - 1) + 0.001 * x\n",
        "\n",
        "    # Encode on a vector\n",
        "    x = torch.clamp(x, -support_size, support_size)\n",
        "    floor = x.floor()\n",
        "    prob = x - floor\n",
        "    logits = torch.zeros(x.shape[0], x.shape[1], 2 * support_size + 1).to(x.device)\n",
        "    logits.scatter_(\n",
        "        2, (floor + support_size).long().unsqueeze(-1), (1 - prob).unsqueeze(-1)\n",
        "    )\n",
        "    indexes = floor + support_size + 1\n",
        "    prob = prob.masked_fill_(2 * support_size < indexes, 0.0)\n",
        "    indexes = indexes.masked_fill_(2 * support_size < indexes, 0.0)\n",
        "    logits.scatter_(2, indexes.long().unsqueeze(-1), prob.unsqueeze(-1))\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYx4UphveR4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ray\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class SharedStorage:\n",
        "    \"\"\"\n",
        "    Class which run in a dedicated thread to store the network weights and some information.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights, game_name, config):\n",
        "        self.config = config\n",
        "        self.game_name = game_name\n",
        "        self.weights = weights\n",
        "        self.infos = {\n",
        "            \"total_reward\": 0,\n",
        "            \"player_0_reward\": 0,\n",
        "            \"player_1_reward\": 0,\n",
        "            \"episode_length\": 0,\n",
        "            \"training_step\": 0,\n",
        "            \"lr\": 0,\n",
        "            \"total_loss\": 0,\n",
        "            \"value_loss\": 0,\n",
        "            \"reward_loss\": 0,\n",
        "            \"policy_loss\": 0,\n",
        "        }\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def set_weights(self, weights, path=None):\n",
        "        self.weights = weights\n",
        "        if not path:\n",
        "            path = os.path.join(self.config.results_path, \"model.weights\")\n",
        "\n",
        "        torch.save(self.weights, path)\n",
        "\n",
        "    def get_infos(self):\n",
        "        return self.infos\n",
        "\n",
        "    def set_infos(self, key, value):\n",
        "        self.infos[key] = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WtQWh5-fF_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import time\n",
        "\n",
        "import numpy\n",
        "import ray\n",
        "import torch\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class SelfPlay:\n",
        "    \"\"\"\n",
        "    Class which run in a dedicated thread to play games and save them to the replay-buffer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initial_weights, game, config):\n",
        "        self.config = config\n",
        "        self.game = game\n",
        "\n",
        "        # Fix random generator seed\n",
        "        numpy.random.seed(self.config.seed)\n",
        "        torch.manual_seed(self.config.seed)\n",
        "\n",
        "        # Initialize the network\n",
        "        self.model = MuZeroNetwork(self.config)\n",
        "        self.model.set_weights(initial_weights)\n",
        "        self.model.to(torch.device(\"cpu\"))\n",
        "        self.model.eval()\n",
        "\n",
        "    def continuous_self_play(self, shared_storage, replay_buffer, test_mode=False):\n",
        "        while True:\n",
        "            self.model.set_weights(\n",
        "                copy.deepcopy(ray.get(shared_storage.get_weights.remote()))\n",
        "            )\n",
        "\n",
        "            # Take the best action (no exploration) in test mode\n",
        "            # temperature = (\n",
        "            #     0\n",
        "            #     if test_mode\n",
        "            #     else self.config.visit_softmax_temperature_fn(\n",
        "            #         trained_steps=ray.get(shared_storage.get_infos.remote())[\n",
        "            #             \"training_step\"\n",
        "            #         ]\n",
        "            #     )\n",
        "            # )\n",
        "            # SCHORY WE NEED TO CHANGE 0.5 to temperature\n",
        "            game_history = self.play_game(\n",
        "                0.5,\n",
        "                self.config.temperature_threshold,\n",
        "                False,\n",
        "                \"self\",\n",
        "                0,\n",
        "            )\n",
        "\n",
        "            # Save to the shared storage\n",
        "            if test_mode:\n",
        "                shared_storage.set_infos.remote(\n",
        "                    \"total_reward\", sum(game_history.reward_history)\n",
        "                )\n",
        "                shared_storage.set_infos.remote(\n",
        "                    \"episode_length\", len(game_history.action_history)\n",
        "                )\n",
        "                if 1 < len(self.config.players):\n",
        "                    shared_storage.set_infos.remote(\n",
        "                        \"player_0_reward\",\n",
        "                        sum(\n",
        "                            [\n",
        "                                reward\n",
        "                                for i, reward in enumerate(game_history.reward_history)\n",
        "                                if game_history.to_play_history[i] == 1\n",
        "                            ]\n",
        "                        ),\n",
        "                    )\n",
        "                    shared_storage.set_infos.remote(\n",
        "                        \"player_1_reward\",\n",
        "                        sum(\n",
        "                            [\n",
        "                                reward\n",
        "                                for i, reward in enumerate(game_history.reward_history)\n",
        "                                if game_history.to_play_history[i] == 0\n",
        "                            ]\n",
        "                        ),\n",
        "                    )\n",
        "            if not test_mode:\n",
        "                replay_buffer.save_game.remote(game_history)\n",
        "\n",
        "            # Managing the self-play / training ratio\n",
        "            if not test_mode and self.config.self_play_delay:\n",
        "                time.sleep(self.config.self_play_delay)\n",
        "            if not test_mode and self.config.ratio:\n",
        "                while (\n",
        "                    ray.get(replay_buffer.get_self_play_count.remote())\n",
        "                    / max(\n",
        "                        1, ray.get(shared_storage.get_infos.remote())[\"training_step\"]\n",
        "                    )\n",
        "                    > self.config.ratio\n",
        "                ):\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "    def play_game(\n",
        "        self, temperature, temperature_threshold, render, opponent, muzero_player\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Play one game with actions based on the Monte Carlo tree search at each moves.\n",
        "        \"\"\"\n",
        "        game_history = GameHistory()\n",
        "        observation = self.game.reset()\n",
        "        game_history.action_history.append(0)\n",
        "        game_history.observation_history.append(observation)\n",
        "        game_history.reward_history.append(0)\n",
        "        game_history.to_play_history.append(self.game.to_play())\n",
        "\n",
        "        done = False\n",
        "\n",
        "        if render:\n",
        "            self.game.render()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            while (\n",
        "                not done and len(game_history.action_history) <= self.config.max_moves\n",
        "            ):\n",
        "                stacked_observations = game_history.get_stacked_observations(\n",
        "                    -1, self.config.stacked_observations,\n",
        "                )\n",
        "\n",
        "                root, priority, tree_depth = MCTS(self.config).run(\n",
        "                    self.model,\n",
        "                    stacked_observations,\n",
        "                    self.game.legal_actions(observation),\n",
        "                    self.game.to_play(),\n",
        "                    False if temperature == 0 else True,\n",
        "                )\n",
        "\n",
        "                if render:\n",
        "                    print(\"Tree depth: {}\".format(tree_depth))\n",
        "                    print(\n",
        "                        \"Root value for player {0}: {1:.2f}\".format(\n",
        "                            self.game.to_play(), root.value()\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # Choose the action\n",
        "                if opponent == \"self\" or muzero_player == self.game.to_play():\n",
        "                    action = self.select_action(\n",
        "                        root,\n",
        "                        temperature\n",
        "                        if not temperature_threshold\n",
        "                        or len(game_history.action_history) < temperature_threshold\n",
        "                        else 0,\n",
        "                    )\n",
        "                elif opponent == \"human\":\n",
        "                    print(\n",
        "                        \"Player {} turn. MuZero suggests {}\".format(\n",
        "                            self.game.to_play(),\n",
        "                            self.game.action_to_string(self.select_action(root, 0)),\n",
        "                        )\n",
        "                    )\n",
        "                    action = self.game.human_to_action()\n",
        "                elif opponent == \"random\":\n",
        "                    action = numpy.random.choice(self.game.legal_actions(observation))\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        'Wrong argument: \"opponent\" argument should be \"self\", \"human\" or \"random\"'\n",
        "                    )\n",
        "\n",
        "                observation, reward, done = self.game.step(action, observation)\n",
        "\n",
        "                if render:\n",
        "                    print(\n",
        "                        \"Played action: {}\".format(self.game.action_to_string(action))\n",
        "                    )\n",
        "                    self.game.render()\n",
        "\n",
        "                game_history.store_search_statistics(root, self.config.action_space)\n",
        "                if not self.config.use_max_priority:\n",
        "                    game_history.priorities.append(priority)\n",
        "\n",
        "                # Next batch\n",
        "                game_history.action_history.append(action)\n",
        "                game_history.observation_history.append(observation)\n",
        "                game_history.reward_history.append(reward)\n",
        "                game_history.to_play_history.append(self.game.to_play())\n",
        "\n",
        "        self.game.close()\n",
        "        return game_history\n",
        "\n",
        "    @staticmethod\n",
        "    def select_action(node, temperature):\n",
        "        \"\"\"\n",
        "        Select action according to the visit count distribution and the temperature.\n",
        "        The temperature is changed dynamically with the visit_softmax_temperature function \n",
        "        in the config.\n",
        "        \"\"\"\n",
        "        visit_counts = numpy.array(\n",
        "            [child.visit_count for child in node.children.values()]\n",
        "        )\n",
        "        actions = [action for action in node.children.keys()]\n",
        "        if temperature == 0:\n",
        "            action = actions[numpy.argmax(visit_counts)]\n",
        "        elif temperature == float(\"inf\"):\n",
        "            action = numpy.random.choice(actions)\n",
        "        else:\n",
        "            # See paper appendix Data Generation\n",
        "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
        "            visit_count_distribution = visit_count_distribution / sum(\n",
        "                visit_count_distribution\n",
        "            )\n",
        "            action = numpy.random.choice(actions, p=visit_count_distribution)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "# Game independent\n",
        "class MCTS:\n",
        "    \"\"\"\n",
        "    Core Monte Carlo Tree Search algorithm.\n",
        "    To decide on an action, we run N simulations, always starting at the root of\n",
        "    the search tree and traversing the tree according to the UCB formula until we\n",
        "    reach a leaf node.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def run(self, model, observation, legal_actions, to_play, add_exploration_noise):\n",
        "        \"\"\"\n",
        "        At the root of the search tree we use the representation function to obtain a\n",
        "        hidden state given the current observation.\n",
        "        We then run a Monte Carlo Tree Search using only action sequences and the model\n",
        "        learned by the network.\n",
        "        \"\"\"\n",
        "        root = Node(0)\n",
        "        dn = DynamicNetwork(self.config.observation_shape, self.config.blocks, \n",
        "                            self.config.channels, self.config.reduced_channels, \n",
        "                            self.config.resnet_fc_reward_layers, self.config.support_size, \n",
        "                            self.config.blocks)\n",
        "        observation = (\n",
        "            torch.tensor(observation)\n",
        "            .float()\n",
        "            .unsqueeze(0)\n",
        "            .to(next(model.parameters()).device)\n",
        "        )\n",
        "        # SCHORY\n",
        "        print(observation)\n",
        "        (\n",
        "            root_predicted_value,\n",
        "            reward,\n",
        "            policy_logits,\n",
        "            hidden_state,\n",
        "        ) = model.initial_inference(observation)\n",
        "        root_predicted_value = dn.support_to_scalar(\n",
        "            root_predicted_value, self.config.support_size\n",
        "        ).item()\n",
        "        reward = dn.support_to_scalar(reward, self.config.support_size).item()\n",
        "        root.expand(\n",
        "            legal_actions, to_play, reward, policy_logits, hidden_state,\n",
        "        )\n",
        "        if add_exploration_noise:\n",
        "            root.add_exploration_noise(\n",
        "                dirichlet_alpha=self.config.root_dirichlet_alpha,\n",
        "                exploration_fraction=self.config.root_exploration_fraction,\n",
        "            )\n",
        "\n",
        "        min_max_stats = MinMaxStats()\n",
        "\n",
        "        max_tree_depth = 0\n",
        "        for _ in range(self.config.num_simulations):\n",
        "            virtual_to_play = to_play\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "            current_tree_depth = 0\n",
        "\n",
        "            while node.expanded():\n",
        "                current_tree_depth += 1\n",
        "                action, node = self.select_child(node, min_max_stats)\n",
        "                search_path.append(node)\n",
        "\n",
        "                # Players play turn by turn\n",
        "                if virtual_to_play + 1 < len(self.config.players):\n",
        "                    virtual_to_play = self.config.players[virtual_to_play + 1]\n",
        "                else:\n",
        "                    virtual_to_play = self.config.players[0]\n",
        "\n",
        "            # Inside the search tree we use the dynamics function to obtain the next hidden\n",
        "            # state given an action and the previous hidden state\n",
        "            parent = search_path[-2]\n",
        "            value, reward, policy_logits, hidden_state = model.recurrent_inference(\n",
        "                parent.hidden_state,\n",
        "                torch.tensor([[action]]).to(parent.hidden_state.device),\n",
        "            )\n",
        "            value = dn.support_to_scalar(value, self.config.support_size).item()\n",
        "            reward = dn.support_to_scalar(reward, self.config.support_size).item()\n",
        "            node.expand(\n",
        "                self.config.action_space,\n",
        "                virtual_to_play,\n",
        "                reward,\n",
        "                policy_logits,\n",
        "                hidden_state,\n",
        "            )\n",
        "\n",
        "            self.backpropagate(search_path, value, virtual_to_play, min_max_stats)\n",
        "\n",
        "            max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
        "\n",
        "        priority = (\n",
        "            None\n",
        "            if self.config.use_max_priority\n",
        "            else numpy.abs(root_predicted_value - root.value()) ** self.config.PER_alpha\n",
        "        )\n",
        "\n",
        "        return root, priority, max_tree_depth\n",
        "\n",
        "    def select_child(self, node, min_max_stats):\n",
        "        \"\"\"\n",
        "        Select the child with the highest UCB score.\n",
        "        \"\"\"\n",
        "        _, action, child = max(\n",
        "            (self.ucb_score(node, child, min_max_stats), action, child)\n",
        "            for action, child in node.children.items()\n",
        "        )\n",
        "        return action, child\n",
        "\n",
        "    def ucb_score(self, parent, child, min_max_stats):\n",
        "        \"\"\"\n",
        "        The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
        "        \"\"\"\n",
        "        pb_c = (\n",
        "            math.log(\n",
        "                (parent.visit_count + self.config.pb_c_base + 1) / self.config.pb_c_base\n",
        "            )\n",
        "            + self.config.pb_c_init\n",
        "        )\n",
        "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "        prior_score = pb_c * child.prior\n",
        "\n",
        "        if child.visit_count > 0:\n",
        "            # mean value Q\n",
        "            value_score = min_max_stats.normalize(\n",
        "                child.reward + self.config.discount * child.value()\n",
        "            )\n",
        "        else:\n",
        "            value_score = 0\n",
        "\n",
        "        return prior_score + value_score\n",
        "\n",
        "    def backpropagate(self, search_path, value, to_play, min_max_stats):\n",
        "        \"\"\"\n",
        "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
        "        to the root.\n",
        "        \"\"\"\n",
        "        for node in reversed(search_path):\n",
        "            node.value_sum += value if node.to_play == to_play else -value\n",
        "            node.visit_count += 1\n",
        "            min_max_stats.update(node.reward + self.config.discount * node.value())\n",
        "\n",
        "            value = node.reward + self.config.discount * value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, prior):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "    def expand(self, actions, to_play, reward, policy_logits, hidden_state):\n",
        "        \"\"\"\n",
        "        We expand a node using the value, reward and policy prediction obtained from the\n",
        "        neural network.\n",
        "        \"\"\"\n",
        "        self.to_play = to_play\n",
        "        self.reward = reward\n",
        "        self.hidden_state = hidden_state\n",
        "        policy = {}\n",
        "        for a in actions:\n",
        "            try:\n",
        "                policy[a] = 1 / sum(torch.exp(policy_logits[0] - policy_logits[0][a]))\n",
        "            except OverflowError:\n",
        "                print(\"Warning: prior has been approximated\")\n",
        "                policy[a] = 0.0\n",
        "        for action, p in policy.items():\n",
        "            self.children[action] = Node(p)\n",
        "\n",
        "    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n",
        "        \"\"\"\n",
        "        At the start of each search, we add dirichlet noise to the prior of the root to\n",
        "        encourage the search to explore new actions.\n",
        "        \"\"\"\n",
        "        actions = list(self.children.keys())\n",
        "        noise = numpy.random.dirichlet([dirichlet_alpha] * len(actions))\n",
        "        frac = exploration_fraction\n",
        "        for a, n in zip(actions, noise):\n",
        "            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "class GameHistory:\n",
        "    \"\"\"\n",
        "    Store only usefull information of a self-play game.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.observation_history = []\n",
        "        self.action_history = []\n",
        "        self.reward_history = []\n",
        "        self.to_play_history = []\n",
        "        self.child_visits = []\n",
        "        self.root_values = []\n",
        "        self.priorities = []\n",
        "\n",
        "    def store_search_statistics(self, root, action_space):\n",
        "        # Turn visit count from root into a policy\n",
        "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "        self.child_visits.append(\n",
        "            [\n",
        "                root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "                for a in action_space\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.root_values.append(root.value())\n",
        "\n",
        "    def get_stacked_observations(self, index, num_stacked_observations):\n",
        "        \"\"\"\n",
        "        Generate a new observation with the observation at the index position\n",
        "        and num_stacked_observations past observations and actions stacked.\n",
        "        \"\"\"\n",
        "        # Convert to positive index\n",
        "        index = index % len(self.observation_history)\n",
        "\n",
        "        stacked_observations = self.observation_history[index].copy()\n",
        "        for past_observation_index in reversed(\n",
        "            range(index - num_stacked_observations, index)\n",
        "        ):\n",
        "            if 0 <= past_observation_index:\n",
        "                previous_observation = numpy.concatenate(\n",
        "                    (\n",
        "                        self.observation_history[past_observation_index],\n",
        "                        [\n",
        "                            numpy.ones_like(stacked_observations[0])\n",
        "                            * self.action_history[past_observation_index + 1]\n",
        "                        ],\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                previous_observation = numpy.concatenate(\n",
        "                    (\n",
        "                        numpy.zeros_like(self.observation_history[index]),\n",
        "                        [numpy.zeros_like(stacked_observations[0])],\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            stacked_observations = numpy.concatenate(\n",
        "                (stacked_observations, previous_observation)\n",
        "            )\n",
        "\n",
        "        return stacked_observations\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"\n",
        "    A class that holds the min-max values of the tree.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.maximum = -float(\"inf\")\n",
        "        self.minimum = float(\"inf\")\n",
        "\n",
        "    def update(self, value):\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value):\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCr-dWrjbtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "import numpy\n",
        "import ray\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Class which run in a dedicated thread to store played games and generate batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.buffer = []\n",
        "        self.game_priorities = []\n",
        "        self.max_recorded_game_priority = 1.0\n",
        "        self.self_play_count = 0\n",
        "\n",
        "        self.model = MuZeroNetwork(self.config)\n",
        "\n",
        "        # Fix random generator seed\n",
        "        numpy.random.seed(self.config.seed)\n",
        "        torch.manual_seed(self.config.seed)\n",
        "\n",
        "    def save_game(self, game_history):\n",
        "        if len(self.buffer) > self.config.window_size:\n",
        "            self.buffer.pop(0)\n",
        "            self.game_priorities.pop(0)\n",
        "\n",
        "        if self.config.use_max_priority:\n",
        "            game_history.priorities = (\n",
        "                numpy.ones(len(game_history.root_values))\n",
        "                * self.max_recorded_game_priority\n",
        "            )\n",
        "        self.buffer.append(game_history)\n",
        "        self.game_priorities.append(numpy.mean(game_history.priorities))\n",
        "        self.self_play_count += 1\n",
        "\n",
        "    def get_self_play_count(self):\n",
        "        return self.self_play_count\n",
        "\n",
        "    def get_batch(self, model_weights):\n",
        "        (\n",
        "            index_batch,\n",
        "            observation_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            value_batch,\n",
        "            policy_batch,\n",
        "            weight_batch,\n",
        "            gradient_scale_batch,\n",
        "        ) = ([], [], [], [], [], [], [], [])\n",
        "\n",
        "        total_samples = sum(\n",
        "            (len(game_history.priorities) for game_history in self.buffer)\n",
        "        )\n",
        "\n",
        "        if self.config.use_last_model_value:\n",
        "            self.model.set_weights(model_weights)\n",
        "\n",
        "        for _ in range(self.config.batch_size):\n",
        "            game_index, game_history, game_prob = self.sample_game(self.buffer)\n",
        "            game_pos, pos_prob = self.sample_position(game_history)\n",
        "\n",
        "            values, rewards, policies, actions = self.make_target(\n",
        "                game_history, game_pos\n",
        "            )\n",
        "\n",
        "            index_batch.append([game_index, game_pos])\n",
        "            observation_batch.append(game_history.get_stacked_observations(game_pos, self.config.stacked_observations))\n",
        "            action_batch.append(actions)\n",
        "            value_batch.append(values)\n",
        "            reward_batch.append(rewards)\n",
        "            policy_batch.append(policies)\n",
        "            weight_batch.append(\n",
        "                (total_samples * game_prob * pos_prob) ** (-self.config.PER_beta)\n",
        "            )\n",
        "            gradient_scale_batch.append(\n",
        "                [\n",
        "                    min(\n",
        "                        self.config.num_unroll_steps,\n",
        "                        len(game_history.action_history) - game_pos,\n",
        "                    )\n",
        "                ]\n",
        "                * len(actions)\n",
        "            )\n",
        "\n",
        "        weight_batch = numpy.array(weight_batch) / max(weight_batch)\n",
        "\n",
        "        # observation_batch: batch, channels, height, width\n",
        "        # action_batch: batch, num_unroll_steps+1\n",
        "        # value_batch: batch, num_unroll_steps+1\n",
        "        # reward_batch: batch, num_unroll_steps+1\n",
        "        # policy_batch: batch, num_unroll_steps+1, len(action_space)\n",
        "        # weight_batch: batch\n",
        "        # gradient_scale_batch: batch, num_unroll_steps+1\n",
        "        return (\n",
        "            index_batch,\n",
        "            (\n",
        "                observation_batch,\n",
        "                action_batch,\n",
        "                value_batch,\n",
        "                reward_batch,\n",
        "                policy_batch,\n",
        "                weight_batch,\n",
        "                gradient_scale_batch,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def sample_game(self, buffer):\n",
        "        \"\"\"\n",
        "        Sample game from buffer either uniformly or according to some priority.\n",
        "        See paper appendix Training.\n",
        "        \"\"\"\n",
        "        game_probs = numpy.array(self.game_priorities) / sum(self.game_priorities)\n",
        "        game_index_candidates = numpy.arange(0, len(self.buffer), dtype=int)\n",
        "        game_index = numpy.random.choice(game_index_candidates, p=game_probs)\n",
        "        game_prob = game_probs[game_index]\n",
        "\n",
        "        return game_index, self.buffer[game_index], game_prob\n",
        "\n",
        "    def sample_position(self, game_history):\n",
        "        \"\"\"\n",
        "        Sample position from game either uniformly or according to some priority.\n",
        "        See paper appendix Training.\n",
        "        \"\"\"\n",
        "        position_probs = numpy.array(game_history.priorities) / sum(\n",
        "            game_history.priorities\n",
        "        )\n",
        "        position_index_candidates = numpy.arange(0, len(position_probs), dtype=int)\n",
        "        position_index = numpy.random.choice(\n",
        "            position_index_candidates, p=position_probs\n",
        "        )\n",
        "        position_prob = position_probs[position_index]\n",
        "\n",
        "        return position_index, position_prob\n",
        "\n",
        "    def update_priorities(self, priorities, index_info):\n",
        "        \"\"\"\n",
        "        Update game and position priorities with priorities calculated during the training.\n",
        "        See Distributed Prioritized Experience Replay https://arxiv.org/abs/1803.00933\n",
        "        \"\"\"\n",
        "        for i in range(len(index_info)):\n",
        "            game_index, game_pos = index_info[i]\n",
        "\n",
        "            # update position priorities\n",
        "            priority = priorities[i, :]\n",
        "            start_index = game_pos\n",
        "            end_index = min(\n",
        "                game_pos + len(priority), len(self.buffer[game_index].priorities)\n",
        "            )\n",
        "            self.buffer[game_index].priorities[start_index:end_index] = priority[\n",
        "                : end_index - start_index\n",
        "            ]\n",
        "\n",
        "            # update game priorities\n",
        "            self.game_priorities[game_index] = numpy.max(\n",
        "                self.buffer[game_index].priorities\n",
        "            )  # option: mean, sum, max\n",
        "\n",
        "            self.max_recorded_game_priority = numpy.max(self.game_priorities)\n",
        "\n",
        "    def make_target(self, game_history, state_index):\n",
        "        \"\"\"\n",
        "        Generate targets for every unroll steps.\n",
        "        \"\"\"\n",
        "        dn = DynamicNetwork()\n",
        "        target_values, target_rewards, target_policies, actions = [], [], [], []\n",
        "        for current_index in range(\n",
        "            state_index, state_index + self.config.num_unroll_steps + 1\n",
        "        ):\n",
        "            # The value target is the discounted root value of the search tree td_steps into the\n",
        "            # future, plus the discounted sum of all rewards until then.\n",
        "            bootstrap_index = current_index + self.config.td_steps\n",
        "            if bootstrap_index < len(game_history.root_values):\n",
        "                if self.config.use_last_model_value:\n",
        "                    # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n",
        "                    observation = torch.tensor(\n",
        "                        game_history.get_stacked_observations(bootstrap_index, self.config.stacked_observations)\n",
        "                    ).float()\n",
        "                    last_step_value = dn.support_to_scalar(\n",
        "                        self.model.initial_inference(observation)[0],\n",
        "                        self.config.support_size,\n",
        "                    ).item()\n",
        "                else:\n",
        "                    last_step_value = game_history.root_values[bootstrap_index]\n",
        "\n",
        "                value = last_step_value * self.config.discount ** self.config.td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(\n",
        "                game_history.reward_history[current_index + 1 : bootstrap_index + 1]\n",
        "            ):\n",
        "                value += (\n",
        "                    reward\n",
        "                    if game_history.to_play_history[current_index]\n",
        "                    == game_history.to_play_history[current_index + 1 + i]\n",
        "                    else -reward\n",
        "                ) * self.config.discount ** i\n",
        "\n",
        "            if current_index < len(game_history.root_values):\n",
        "                target_values.append(value)\n",
        "                target_rewards.append(game_history.reward_history[current_index])\n",
        "                target_policies.append(game_history.child_visits[current_index])\n",
        "                actions.append(game_history.action_history[current_index])\n",
        "            elif current_index == len(game_history.root_values):\n",
        "                target_values.append(0)\n",
        "                target_rewards.append(game_history.reward_history[current_index])\n",
        "                # Uniform policy\n",
        "                target_policies.append(\n",
        "                    [\n",
        "                        1 / len(game_history.child_visits[0])\n",
        "                        for _ in range(len(game_history.child_visits[0]))\n",
        "                    ]\n",
        "                )\n",
        "                actions.append(game_history.action_history[current_index])\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states\n",
        "                target_values.append(0)\n",
        "                target_rewards.append(0)\n",
        "                # Uniform policy\n",
        "                target_policies.append(\n",
        "                    [\n",
        "                        1 / len(game_history.child_visits[0])\n",
        "                        for _ in range(len(game_history.child_visits[0]))\n",
        "                    ]\n",
        "                )\n",
        "                actions.append(numpy.random.choice(game_history.action_history))\n",
        "\n",
        "        return target_values, target_rewards, target_policies, actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH0MZlxQlJ-w",
        "colab_type": "text"
      },
      "source": [
        "**Trainer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf_GeH6fk-dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import numpy\n",
        "import ray\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Class which run in a dedicated thread to train a neural network and save it\n",
        "    in the shared storage.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initial_weights, config):\n",
        "        self.config = config\n",
        "        self.training_step = 0\n",
        "\n",
        "        # Fix random generator seed\n",
        "        numpy.random.seed(self.config.seed)\n",
        "        torch.manual_seed(self.config.seed)\n",
        "\n",
        "        # Initialize the network\n",
        "        self.model = MuZeroNetwork(self.config)\n",
        "        self.model.set_weights(initial_weights)\n",
        "        self.model.to(torch.device(config.training_device))\n",
        "        self.model.train()\n",
        "\n",
        "        if self.config.optimizer == \"SGD\":\n",
        "            self.optimizer = torch.optim.SGD(\n",
        "                self.model.parameters(),\n",
        "                lr=self.config.lr_init,\n",
        "                momentum=self.config.momentum,\n",
        "                weight_decay=self.config.weight_decay,\n",
        "            )\n",
        "        elif self.config.optimizer == \"Adam\":\n",
        "            self.optimizer = torch.optim.Adam(\n",
        "                self.model.parameters(),\n",
        "                lr=self.config.lr_init,\n",
        "                weight_decay=self.config.weight_decay,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"{} is not implemented. You can change the optimizer manually in trainer.py.\"\n",
        "            )\n",
        "\n",
        "    def continuous_update_weights(self, replay_buffer, shared_storage_worker):\n",
        "        # Wait for the replay buffer to be filled\n",
        "        while ray.get(replay_buffer.get_self_play_count.remote()) < 1:\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Training loop\n",
        "        while True:\n",
        "            index_batch, batch = ray.get(replay_buffer.get_batch.remote(self.model.get_weights()))\n",
        "            self.update_lr()\n",
        "            (\n",
        "                priorities,\n",
        "                total_loss,\n",
        "                value_loss,\n",
        "                reward_loss,\n",
        "                policy_loss,\n",
        "            ) = self.update_weights(batch)\n",
        "\n",
        "            if self.config.PER:\n",
        "                # Save new priorities in the replay buffer (See https://arxiv.org/abs/1803.00933)\n",
        "                replay_buffer.update_priorities.remote(priorities, index_batch)\n",
        "\n",
        "            # Save to the shared storage\n",
        "            if self.training_step % self.config.checkpoint_interval == 0:\n",
        "                shared_storage_worker.set_weights.remote(self.model.get_weights())\n",
        "            shared_storage_worker.set_infos.remote(\"training_step\", self.training_step)\n",
        "            shared_storage_worker.set_infos.remote(\n",
        "                \"lr\", self.optimizer.param_groups[0][\"lr\"]\n",
        "            )\n",
        "            shared_storage_worker.set_infos.remote(\"total_loss\", total_loss)\n",
        "            shared_storage_worker.set_infos.remote(\"value_loss\", value_loss)\n",
        "            shared_storage_worker.set_infos.remote(\"reward_loss\", reward_loss)\n",
        "            shared_storage_worker.set_infos.remote(\"policy_loss\", policy_loss)\n",
        "\n",
        "            # Managing the self-play / training ratio\n",
        "            if self.config.training_delay:\n",
        "                time.sleep(self.config.training_delay)\n",
        "            if self.config.ratio:\n",
        "                while (\n",
        "                    ray.get(replay_buffer.get_self_play_count.remote())\n",
        "                    / max(1, self.training_step)\n",
        "                    < self.config.ratio\n",
        "                ):\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "    def update_weights(self, batch):\n",
        "        \"\"\"\n",
        "        Perform one training step.\n",
        "        \"\"\"\n",
        "\n",
        "        (\n",
        "            observation_batch,\n",
        "            action_batch,\n",
        "            target_value,\n",
        "            target_reward,\n",
        "            target_policy,\n",
        "            weight_batch,\n",
        "            gradient_scale_batch,\n",
        "        ) = batch\n",
        "\n",
        "        # Keep values as scalars for calculating the priorities for the prioritized replay\n",
        "        target_value_scalar = numpy.array(target_value)\n",
        "        priorities = numpy.zeros_like(target_value_scalar)\n",
        "        dn = DynamicNetwork()\n",
        "\n",
        "        device = next(self.model.parameters()).device\n",
        "        weight_batch = torch.tensor(weight_batch).float().to(device)\n",
        "        observation_batch = torch.tensor(observation_batch).float().to(device)\n",
        "        action_batch = torch.tensor(action_batch).float().to(device).unsqueeze(-1)\n",
        "        target_value = torch.tensor(target_value).float().to(device)\n",
        "        target_reward = torch.tensor(target_reward).float().to(device)\n",
        "        target_policy = torch.tensor(target_policy).float().to(device)\n",
        "        gradient_scale_batch = torch.tensor(gradient_scale_batch).float().to(device)\n",
        "        # observation_batch: batch, channels, height, width\n",
        "        # action_batch: batch, num_unroll_steps+1, 1 (unsqueeze)\n",
        "        # target_value: batch, num_unroll_steps+1\n",
        "        # target_reward: batch, num_unroll_steps+1\n",
        "        # target_policy: batch, num_unroll_steps+1, len(action_space)\n",
        "        # gradient_scale_batch: batch, num_unroll_steps+1\n",
        "\n",
        "        target_value = dn.scalar_to_support(target_value, self.config.support_size)\n",
        "        target_reward = dn.scalar_to_support(target_reward, self.config.support_size)\n",
        "        # target_value: batch, num_unroll_steps+1, 2*support_size+1\n",
        "        # target_reward: batch, num_unroll_steps+1, 2*support_size+1\n",
        "\n",
        "        ## Generate predictions\n",
        "        value, reward, policy_logits, hidden_state = self.model.initial_inference(\n",
        "            observation_batch\n",
        "        )\n",
        "        predictions = [(value, reward, policy_logits)]\n",
        "        for i in range(1, action_batch.shape[1]):\n",
        "            value, reward, policy_logits, hidden_state = self.model.recurrent_inference(\n",
        "                hidden_state, action_batch[:, i]\n",
        "            )\n",
        "            # Scale the gradient at the start of the dynamics function (See paper appendix Training)\n",
        "            hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "            predictions.append((value, reward, policy_logits))\n",
        "        # predictions: num_unroll_steps+1, 3, batch, 2*support_size+1 | 2*support_size+1 | 9 (according to the 2nd dim)\n",
        "\n",
        "        ## Compute losses\n",
        "        value_loss, reward_loss, policy_loss = (0, 0, 0)\n",
        "        value, reward, policy_logits = predictions[0]\n",
        "        # Ignore reward loss for the first batch step\n",
        "        current_value_loss, _, current_policy_loss = self.loss_function(\n",
        "            value.squeeze(-1),\n",
        "            reward.squeeze(-1),\n",
        "            policy_logits,\n",
        "            target_value[:, 0],\n",
        "            target_reward[:, 0],\n",
        "            target_policy[:, 0],\n",
        "        )\n",
        "        value_loss += current_value_loss\n",
        "        policy_loss += current_policy_loss\n",
        "        # Compute priorities for the prioritized replay (See paper appendix Training)\n",
        "        pred_value_scalar = (\n",
        "            models.support_to_scalar(value, self.config.support_size)\n",
        "            .detach()\n",
        "            .cpu()\n",
        "            .numpy()\n",
        "            .squeeze()\n",
        "        )\n",
        "        priorities[:, 0] = (\n",
        "            numpy.abs(pred_value_scalar - target_value_scalar[:, 0])\n",
        "            ** self.config.PER_alpha\n",
        "        )\n",
        "\n",
        "        for i in range(1, len(predictions)):\n",
        "            value, reward, policy_logits = predictions[i]\n",
        "            (\n",
        "                current_value_loss,\n",
        "                current_reward_loss,\n",
        "                current_policy_loss,\n",
        "            ) = self.loss_function(\n",
        "                value.squeeze(-1),\n",
        "                reward.squeeze(-1),\n",
        "                policy_logits,\n",
        "                target_value[:, i],\n",
        "                target_reward[:, i],\n",
        "                target_policy[:, i],\n",
        "            )\n",
        "\n",
        "            # Scale gradient by the number of unroll steps (See paper appendix Training)\n",
        "            current_value_loss.register_hook(\n",
        "                lambda grad: grad / gradient_scale_batch[:, i]\n",
        "            )\n",
        "            current_reward_loss.register_hook(\n",
        "                lambda grad: grad / gradient_scale_batch[:, i]\n",
        "            )\n",
        "            current_policy_loss.register_hook(\n",
        "                lambda grad: grad / gradient_scale_batch[:, i]\n",
        "            )\n",
        "\n",
        "            value_loss += current_value_loss\n",
        "            reward_loss += current_reward_loss\n",
        "            policy_loss += current_policy_loss\n",
        "\n",
        "            # Compute priorities for the prioritized replay (See paper appendix Training)\n",
        "            pred_value_scalar = (\n",
        "                models.support_to_scalar(value, self.config.support_size)\n",
        "                .detach()\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "                .squeeze()\n",
        "            )\n",
        "            priorities[:, i] = (\n",
        "                numpy.abs(pred_value_scalar - target_value_scalar[:, i])\n",
        "                ** self.config.PER_alpha\n",
        "            )\n",
        "\n",
        "        # Scale the value loss, paper recommends by 0.25 (See paper appendix Reanalyze)\n",
        "        loss = value_loss * self.config.value_loss_weight + reward_loss + policy_loss\n",
        "        if self.config.PER:\n",
        "            # Correct PER bias by using importance-sampling (IS) weights\n",
        "            loss *= weight_batch\n",
        "        # Mean over batch dimension (pseudocode do a sum)\n",
        "        loss = loss.mean()\n",
        "\n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.training_step += 1\n",
        "\n",
        "        return (\n",
        "            priorities,\n",
        "            # For log purpose\n",
        "            loss.item(),\n",
        "            value_loss.mean().item(),\n",
        "            reward_loss.mean().item(),\n",
        "            policy_loss.mean().item(),\n",
        "        )\n",
        "\n",
        "    def update_lr(self):\n",
        "        \"\"\"\n",
        "        Update learning rate\n",
        "        \"\"\"\n",
        "        lr = self.config.lr_init * self.config.lr_decay_rate ** (\n",
        "            self.training_step / self.config.lr_decay_steps\n",
        "        )\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_function(\n",
        "        value, reward, policy_logits, target_value, target_reward, target_policy,\n",
        "    ):\n",
        "        # Cross-entropy seems to have a better convergence than MSE\n",
        "        value_loss = (-target_value * torch.nn.LogSoftmax(dim=1)(value)).sum(1)\n",
        "        reward_loss = (-target_reward * torch.nn.LogSoftmax(dim=1)(reward)).sum(1)\n",
        "        policy_loss = (-target_policy * torch.nn.LogSoftmax(dim=1)(policy_logits)).sum(\n",
        "            1\n",
        "        )\n",
        "        return value_loss, reward_loss, policy_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX6d19133bbG",
        "colab_type": "text"
      },
      "source": [
        "#MCTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2App9nS23cey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import math\n",
        "# import numpy as np\n",
        "# EPS = 1e-8\n",
        "\n",
        "# class MCTS():\n",
        "#     \"\"\"\n",
        "#     This class handles the MCTS tree.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, game, nnet, args):\n",
        "#         self.game = game\n",
        "#         self.nnet = nnet\n",
        "#         self.args = args\n",
        "#         self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
        "#         self.Nsa = {}       # stores #times edge s,a was visited\n",
        "#         self.Ns = {}        # stores #times board s was visited\n",
        "#         self.Ps = {}        # stores initial policy (returned by neural net)\n",
        "\n",
        "#         self.Es = {}        # stores game.getGameEnded ended for board s\n",
        "#         self.Vs = {}        # stores game.getValidMoves for board s\n",
        "        \n",
        "#         self.plot = [1000]\n",
        "#         self.num_sim = [0]\n",
        "\n",
        "#     def getActionProb(self, graphState, temp=1):\n",
        "#         \"\"\"\n",
        "#         This function performs numMCTSSims simulations of MCTS starting from\n",
        "#         canonicalBoard.\n",
        "#         Returns:\n",
        "#             probs: a policy vector where the probability of the ith action is\n",
        "#                    proportional to Nsa[(s,a)]**(1./temp)\n",
        "#         \"\"\"\n",
        "#         for i in range(self.args.numMCTSSims):\n",
        "#             self.search(graphState, i)\n",
        "\n",
        "#         s = self.game.stringRepresentation(graphState)\n",
        "#         counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
        "\n",
        "#         if temp==0:\n",
        "#             bestA = np.argmax(counts)\n",
        "#             probs = [0]*len(counts)\n",
        "#             probs[bestA]=1\n",
        "#             return probs\n",
        "\n",
        "#         counts = [x**(1./temp) for x in counts]\n",
        "#         counts_sum = float(sum(counts))\n",
        "#         probs = [x/counts_sum for x in counts]\n",
        "#         return probs\n",
        "\n",
        "\n",
        "#     def search(self, graphState, num_sim):\n",
        "#         \"\"\"\n",
        "#         This function performs one iteration of MCTS. It is recursively called\n",
        "#         till a leaf node is found. The action chosen at each node is one that\n",
        "#         has the maximum upper confidence bound as in the paper.\n",
        "#         Once a leaf node is found, the neural network is called to return an\n",
        "#         initial policy P and a value v for the state. This value is propagated\n",
        "#         up the search path. In case the leaf node is a terminal state, the\n",
        "#         outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
        "#         updated.\n",
        "#         NOTE: the return values are the negative of the value of the current\n",
        "#         state. This is done since v is in [-1,1] and if v is the value of a\n",
        "#         state for the current player, then its value is -v for the other player.\n",
        "#         Returns:\n",
        "#             v: the negative of the value of the current canonicalBoard\n",
        "#         \"\"\"\n",
        "\n",
        "#         s = self.game.stringRepresentation(graphState)\n",
        "\n",
        "#         if s not in self.Es:\n",
        "#             self.Es[s] = self.game.getGameEnded(graphState)\n",
        "#         if self.Es[s]!=0:\n",
        "#             # terminal node\n",
        "#             return 0\n",
        "\n",
        "#         if s not in self.Ps:\n",
        "#             # leaf node\n",
        "#             if self.nnet is not None:\n",
        "#                 self.Ps[s], v = self.nnet.predict(graphState, self.game.graph)\n",
        "#             else:\n",
        "#                 self.Ps[s] = np.ones(self.game.getActionSize()) # random policy\n",
        "#                 v = 0\n",
        "#             valids = self.game.getValidMoves(graphState)\n",
        "#             self.Ps[s] = self.Ps[s]*valids      # masking invalid moves\n",
        "#             sum_Ps_s = np.sum(self.Ps[s])\n",
        "#             if sum_Ps_s > 0:\n",
        "#                 self.Ps[s] /= sum_Ps_s    # renormalize\n",
        "#             else:\n",
        "#                 # if all valid moves were masked make all valid moves equally probable\n",
        "                \n",
        "#                 # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
        "#                 # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
        "#                 print(\"All valid moves were masked, do workaround.\")\n",
        "#                 self.Ps[s] = self.Ps[s] + valids\n",
        "#                 self.Ps[s] /= np.sum(self.Ps[s])\n",
        "\n",
        "#             self.Vs[s] = valids\n",
        "#             self.Ns[s] = 0\n",
        "#             return v\n",
        "\n",
        "#         valids = self.Vs[s]\n",
        "#         cur_best = -float('inf')\n",
        "#         best_act = -1\n",
        "\n",
        "#         # pick the action with the highest upper confidence bound\n",
        "#         for a in range(self.game.getActionSize()):\n",
        "#             if valids[a]:\n",
        "#                 if (s,a) in self.Qsa:\n",
        "#                     u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
        "#                 else:\n",
        "#                     u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + EPS)     # Q = 0 ?\n",
        "\n",
        "#                 if u > cur_best:\n",
        "#                     cur_best = u\n",
        "#                     best_act = a\n",
        "\n",
        "#         a = best_act\n",
        "#         next_s, reward = self.game.getNextState(graphState, a)\n",
        "#         # next_s = self.game.getCanonicalForm(next_s, next_player)\n",
        "\n",
        "#         v = self.search(next_s, num_sim) + reward\n",
        "\n",
        "#         if (s,a) in self.Qsa:\n",
        "#             self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
        "#             self.Nsa[(s,a)] += 1\n",
        "\n",
        "#         else:\n",
        "#             self.Qsa[(s,a)] = v\n",
        "#             self.Nsa[(s,a)] = 1\n",
        "            \n",
        "#         if v > self.game.getActionSize() - self.plot[-1]:\n",
        "#             self.plot.append(self.game.getActionSize() - v)\n",
        "#             self.num_sim.append(num_sim)\n",
        "\n",
        "#         self.Ns[s] += 1\n",
        "#         return v"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}