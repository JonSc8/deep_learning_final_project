{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Final-TSP-MCTS-CNN-JS^2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqiNC1Cybew5",
        "colab_type": "text"
      },
      "source": [
        "Deep Learning Exercise\n",
        "---\n",
        "Jonathan Schwartz & Jonathan Schory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7deoyHHPMpr",
        "colab_type": "text"
      },
      "source": [
        "MCTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-gBF5q7yyI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "EPS = 1e-8\n",
        "\n",
        "class MCTS():\n",
        "    \"\"\"\n",
        "    This class handles the MCTS tree.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "        self.args = args\n",
        "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
        "        self.Nsa = {}       # stores #times edge s,a was visited\n",
        "        self.Ns = {}        # stores #times board s was visited\n",
        "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
        "\n",
        "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
        "        self.Vs = {}        # stores game.getValidMoves for board s\n",
        "        \n",
        "        self.plot = [1000]\n",
        "        self.num_sim = [0]\n",
        "\n",
        "    def getActionProb(self, graphState, temp=1):\n",
        "        \"\"\"\n",
        "        This function performs numMCTSSims simulations of MCTS starting from\n",
        "        canonicalBoard.\n",
        "        Returns:\n",
        "            probs: a policy vector where the probability of the ith action is\n",
        "                   proportional to Nsa[(s,a)]**(1./temp)\n",
        "        \"\"\"\n",
        "        for i in range(self.args.numMCTSSims):\n",
        "            self.search(graphState, i)\n",
        "\n",
        "        s = self.game.stringRepresentation(graphState)\n",
        "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
        "\n",
        "        if temp==0:\n",
        "            bestA = np.argmax(counts)\n",
        "            probs = [0]*len(counts)\n",
        "            probs[bestA]=1\n",
        "            return probs\n",
        "\n",
        "        counts = [x**(1./temp) for x in counts]\n",
        "        counts_sum = float(sum(counts))\n",
        "        probs = [x/counts_sum for x in counts]\n",
        "        return probs\n",
        "\n",
        "\n",
        "    def search(self, graphState, num_sim):\n",
        "        \"\"\"\n",
        "        This function performs one iteration of MCTS. It is recursively called\n",
        "        till a leaf node is found. The action chosen at each node is one that\n",
        "        has the maximum upper confidence bound as in the paper.\n",
        "        Once a leaf node is found, the neural network is called to return an\n",
        "        initial policy P and a value v for the state. This value is propagated\n",
        "        up the search path. In case the leaf node is a terminal state, the\n",
        "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
        "        updated.\n",
        "        NOTE: the return values are the negative of the value of the current\n",
        "        state. This is done since v is in [-1,1] and if v is the value of a\n",
        "        state for the current player, then its value is -v for the other player.\n",
        "        Returns:\n",
        "            v: the negative of the value of the current canonicalBoard\n",
        "        \"\"\"\n",
        "\n",
        "        s = self.game.stringRepresentation(graphState)\n",
        "\n",
        "        if s not in self.Es:\n",
        "            self.Es[s] = self.game.getGameEnded(graphState)\n",
        "        if self.Es[s]!=0:\n",
        "            # terminal node\n",
        "            return 0\n",
        "\n",
        "        if s not in self.Ps:\n",
        "            # leaf node\n",
        "            if self.nnet is not None:\n",
        "                self.Ps[s], v = self.nnet.predict(graphState, self.game.graph)\n",
        "            else:\n",
        "                self.Ps[s] = np.ones(self.game.getActionSize()) # random policy\n",
        "                v = 0\n",
        "            valids = self.game.getValidMoves(graphState)\n",
        "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves\n",
        "            sum_Ps_s = np.sum(self.Ps[s])\n",
        "            if sum_Ps_s > 0:\n",
        "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
        "            else:\n",
        "                # if all valid moves were masked make all valid moves equally probable\n",
        "                \n",
        "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
        "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
        "                print(\"All valid moves were masked, do workaround.\")\n",
        "                self.Ps[s] = self.Ps[s] + valids\n",
        "                self.Ps[s] /= np.sum(self.Ps[s])\n",
        "\n",
        "            self.Vs[s] = valids\n",
        "            self.Ns[s] = 0\n",
        "            return v\n",
        "\n",
        "        valids = self.Vs[s]\n",
        "        cur_best = -float('inf')\n",
        "        best_act = -1\n",
        "\n",
        "        # pick the action with the highest upper confidence bound\n",
        "        for a in range(self.game.getActionSize()):\n",
        "            if valids[a]:\n",
        "                if (s,a) in self.Qsa:\n",
        "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
        "                else:\n",
        "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + EPS)     # Q = 0 ?\n",
        "\n",
        "                if u > cur_best:\n",
        "                    cur_best = u\n",
        "                    best_act = a\n",
        "\n",
        "        a = best_act\n",
        "        next_s, reward = self.game.getNextState(graphState, a)\n",
        "        # next_s = self.game.getCanonicalForm(next_s, next_player)\n",
        "\n",
        "        v = self.search(next_s, num_sim) + reward\n",
        "\n",
        "        if (s,a) in self.Qsa:\n",
        "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
        "            self.Nsa[(s,a)] += 1\n",
        "\n",
        "        else:\n",
        "            self.Qsa[(s,a)] = v\n",
        "            self.Nsa[(s,a)] = 1\n",
        "            \n",
        "        if v > self.game.getActionSize() - self.plot[-1]:\n",
        "            self.plot.append(self.game.getActionSize() - v)\n",
        "            self.num_sim.append(num_sim)\n",
        "\n",
        "        self.Ns[s] += 1\n",
        "        return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRKfej2YPUJx",
        "colab_type": "text"
      },
      "source": [
        "TSP Game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuGlOq6WbTCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import permutations \n",
        "\n",
        "class TSPGame():\n",
        "\n",
        "    def __init__(self, num_node):\n",
        "        self.num_node = num_node # number of nodes in the graph\n",
        "        self.graph = np.random.rand(self.num_node, 2) # each index representing xy coordinates \n",
        "\n",
        "    def getInitState(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            first_state: a representation of the graph\n",
        "            left column representing visited nodes\n",
        "            right column will always have a single 1 and the rest are 0's. index with the 1 in the right column is current node\n",
        "        \"\"\"\n",
        "        first_state = np.zeros([self.num_node, 2])\n",
        "        # Always start with first node as current node \n",
        "        first_state[0][0] = 1\n",
        "        first_state[0][1] = 1\n",
        "        return first_state\n",
        "\n",
        "    def getBoardSize(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            (x,y): a tuple of board dimensions\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getActionSize(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            actionSize: number of all possible actions\n",
        "        \"\"\"\n",
        "        return self.num_node\n",
        "    \n",
        "    def getNextState(self, state, action):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "            action: action taken by current player\n",
        "        Returns:\n",
        "            next_s: graph after applying action\n",
        "            reward: reward from action\n",
        "        \"\"\"\n",
        "        \n",
        "        next_s = state.copy()\n",
        "        # zero out current node\n",
        "        for n in next_s:\n",
        "          if n[1] == 1:\n",
        "            prev_action = np.where(next_s == n)[0][0]\n",
        "            n[1] = 0\n",
        "        # 1 in left column for visited, 1 in right column for current node\n",
        "        next_s[action] = 1\n",
        "        \n",
        "        # get xy coordinates for prev_node and current_node from the graph\n",
        "        prev_node = self.graph[prev_action]\n",
        "        current_node = self.graph[action]\n",
        "          \n",
        "        reward = 1 - np.linalg.norm(current_node - prev_node)\n",
        "        if self.getCountVisitedNodes(next_s) == self.num_node: #end of game\n",
        "            reward += 1 - np.linalg.norm(current_node - self.graph[0])\n",
        "            \n",
        "        return next_s, reward\n",
        "\n",
        "    def getValidMoves(self, state):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "        Returns:\n",
        "            list of valid moves, 1 for valid, 0 for invalid \n",
        "        \"\"\"\n",
        "        return 1 - state[:, 0]\n",
        "\n",
        "    def getCountVisitedNodes(self, state):\n",
        "      count_visited = 0\n",
        "      for n in state:\n",
        "        if n[0] == 1:\n",
        "          count_visited += 1\n",
        "      \n",
        "      return count_visited\n",
        "    \n",
        "    def getGameEnded(self, state):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "        Returns:\n",
        "            r: 0 if game has not ended. 1 if it has\n",
        "               \n",
        "        \"\"\"\n",
        "        end = 0\n",
        "        if self.getCountVisitedNodes(state) == self.num_node:\n",
        "            end = 1\n",
        "        return end\n",
        "\n",
        "    def getCanonicalForm(self, board, player):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            board: current board\n",
        "            player: current player (1 or -1)\n",
        "        Returns:\n",
        "            canonicalBoard: returns canonical form of board. The canonical form\n",
        "                            should be independent of player. For e.g. in chess,\n",
        "                            the canonical form can be chosen to be from the pov\n",
        "                            of white. When the player is white, we can return\n",
        "                            board as is. When the player is black, we can invert\n",
        "                            the colors and return the board.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getSymmetries(self, board, pi):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            board: current board\n",
        "            pi: policy vector of size self.getActionSize()\n",
        "        Returns:\n",
        "            symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n",
        "                       form of the board and the corresponding pi vector. This\n",
        "                       is used when training the neural network from examples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def stringRepresentation(self, state):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "        Returns:\n",
        "            index of state\n",
        "        \"\"\"\n",
        "        s = ''\n",
        "        for i in range(self.num_node):\n",
        "            s += str(int(state[i][0]))\n",
        "        return s\n",
        "    \n",
        "    def optimal(self):\n",
        "        seq = np.arange(self.num_node)[1:]\n",
        "        perm = permutations(seq)\n",
        "        graph = self.graph\n",
        "        reward = 0\n",
        "        optimal = 10000\n",
        "        action = []\n",
        "        for p in list(perm):\n",
        "            reward = 0\n",
        "            reward += np.linalg.norm(graph[0] - graph[p[0]])\n",
        "            reward += np.linalg.norm(graph[p[-1]] - graph[0])\n",
        "            \n",
        "            for k in range(len(p) - 1):\n",
        "                i = p[k]\n",
        "                j = p[k+1]\n",
        "                reward += np.linalg.norm(graph[j] - graph[i])\n",
        "            \n",
        "            if reward < optimal:\n",
        "                optimal = reward\n",
        "                action = p\n",
        "        \n",
        "        return optimal, action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrTTX6B0Phrf",
        "colab_type": "text"
      },
      "source": [
        "Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2quDyDMDbTCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MCTS_args(object):\n",
        "    def __init__(self, num_sim, cpuct):\n",
        "        self.numMCTSSims = num_sim\n",
        "        self.cpuct = cpuct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIB4kC3xbTCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_TSP_MCTS(num_node, args):\n",
        "    game = TSPGame(num_node)\n",
        "    mcts = MCTS(game, None, args)\n",
        "    state = game.getInitState()\n",
        "    r = 0\n",
        "    actions = []\n",
        "    optimal_val, optimal_path = game.optimal()\n",
        "    \n",
        "    while not game.getGameEnded(state):\n",
        "        action = np.argmax(mcts.getActionProb(state))\n",
        "        state, reward = game.getNextState(state, action)\n",
        "        actions.append(action)\n",
        "        r += reward\n",
        "    \n",
        "    return mcts.plot, mcts.num_sim, optimal_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu1sBOUNbTC7",
        "colab_type": "code",
        "outputId": "8a065154-2b20-4c38-c620-cef6f4854d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "optimal_list = []\n",
        "mcts_dict = {}\n",
        "num_node = 10\n",
        "args = MCTS_args(num_sim = 20000, cpuct = 1)\n",
        "for i in range(100):\n",
        "  vals, num_s, optimalReward = play_TSP_MCTS(num_node, args)\n",
        "  optimal_list.append(optimalReward)\n",
        "  mcts_dict[i] = {tuple(num_s): vals}\n",
        "\n",
        "print(optimal_list)\n",
        "print(mcts_dict)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5ae256a2e41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmcts_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpuct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimalReward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_TSP_MCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MCTS_args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbpefDE5Hq7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "33631ce0-5614-420b-c548-f2c888a2f94c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(num_s[1:]), vals[1:])\n",
        "plt.xlabel(\"num_simulations\", fontsize=12)\n",
        "plt.ylabel(\"path_cost\", fontsize=12)\n",
        "plt.hlines(optimalReward, 0, num_s[-1])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.LineCollection at 0x7fc798e350b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEJCAYAAAB11IfBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHDOzAnCxNzMrABd7M7MCcLE3MyuA/w/bJebqB7/0EQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea_rbJhCW-ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "columns = list(range(100))\n",
        "df = pd.DataFrame(columns=columns)\n",
        "i = 0\n",
        "for result in mcts_dict:\n",
        "  sims = list(mcts_dict[result].keys())[0]\n",
        "  vals = list(mcts_dict[result].values())[0]\n",
        "  col = np.zeros(20000)\n",
        "  j = 0\n",
        "  for sim in sims:\n",
        "    col[sim] = vals[j]\n",
        "    j += 1\n",
        "  df[i] = col\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT9EcubrXN7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5ddbc0a3-d2d5-43be-d35d-809776917b4e"
      },
      "source": [
        "for col in df:\n",
        "  df[col] = df[col].replace(0.0, pd.np.nan)\n",
        "  df[col] = df[col].ffill()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he8pJ6QVXQP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(df.head(1).index, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVEkze1dXQ0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_val_row = pd.DataFrame([pd.Series(optimal_list)], index=[\"opt\"], dtype=float)\n",
        "df = pd.concat([opt_val_row, df])\n",
        "df.to_csv(\"100-games.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t7TRz1KXTUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_vals = df[0:1]\n",
        "opt_vals = opt_vals.iloc[0]\n",
        "df.drop(df.head(1).index, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1jiiagaXT6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "2a6f59ed-ad51-4269-b504-fd8f8a2037c1"
      },
      "source": [
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.480523</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.021568</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.442377</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.495787</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.698695</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19994</th>\n",
              "      <td>3.075367</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>3.075367</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>3.075367</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>3.075367</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>3.075367</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19999 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0   1   2   3   4   5   6   7   ...  12  13  14  15  16  17  18  19\n",
              "0      9.480523 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "1      9.021568 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "2      8.442377 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "3      7.495787 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "4      6.698695 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "...         ...  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..\n",
              "19994  3.075367 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "19995  3.075367 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "19996  3.075367 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "19997  3.075367 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "19998  3.075367 NaN NaN NaN NaN NaN NaN NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN\n",
              "\n",
              "[19999 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1TxMwyCXWmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "within_factor = []\n",
        "for i in range(19999):\n",
        "  row = list(df.iloc[i])\n",
        "  counter = 0\n",
        "  for j in range(len(row)):\n",
        "    if row[j] / opt_vals[j] <= 1.1:\n",
        "      counter += 1\n",
        "  within_factor += [counter / 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2CGSw_7XW62",
        "colab_type": "code",
        "outputId": "77a21a0d-a45f-4a99-979f-2959df5e8ea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(range(19999)), within_factor)\n",
        "plt.title(\"Percentage of random instances which MCTS solved within a 1.1 factor of the optimal solution as a function of simulations\")\n",
        "plt.xlabel(\"num_simulations\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.ylim(top=within_factor[-1] + 0.05)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05500000000000001, 0.05)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvEAAAEXCAYAAAAkzRvjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntFE8qQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsEB3ioOC2Yb",
        "colab_type": "text"
      },
      "source": [
        "**CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OFL6ZnmPoso",
        "colab_type": "text"
      },
      "source": [
        "Network Arguments "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7HSZvnyHXm1P",
        "colab": {}
      },
      "source": [
        "class dotdict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "\n",
        "args = dotdict({\n",
        "    # MCTS args\n",
        "    'numMCTSSims': 20000,          # Number of games moves for MCTS to simulate.\n",
        "    'num_node': 10,                 # Number of nodes in the graph (game)\n",
        "    \n",
        "    # Train args\n",
        "    'numIters': 10,              # Number of episods to play (5 times 10 episodes)\n",
        "    'numEps': 20,              # Number of complete self-play games to simulate during a new iteration.\n",
        "    'tempThreshold': 15,        #\n",
        "    'updateThreshold': 0.6,     # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
        "    'maxlenOfQueue': 200000,    # Number of game examples to train the neural networks.\n",
        "    'arenaCompare': 40,         # Number of games to play during arena play to determine if new net will be accepted.\n",
        "    'cpuct': 1,\n",
        "    'numItersForTrainExamplesHistory': 25,\n",
        "    \n",
        "    # NN args\n",
        "    'lr': 0.001,\n",
        "    'dropout': 0.3,\n",
        "    'epochs': 15,\n",
        "    'batch_size': 64,\n",
        "    'cuda': False,\n",
        "    'num_channels': 512,\n",
        "    })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68AWFhIdPx3b",
        "colab_type": "text"
      },
      "source": [
        "Create Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fttq0wxZLlXd",
        "colab_type": "code",
        "outputId": "5604f788-8ecb-46f3-8161-410b80aeb7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "game = TSPGame(args.num_node)\n",
        "net = NNet(game, args)\n",
        "data = []\n",
        "for i in range(2):\n",
        "    coach = Coach2(game, net, args)\n",
        "    data.append(coach.learn())\n",
        "    game = TSPGame(args.num_node)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------ITER 1------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-99836fa624a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcoach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoach2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSPGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-e05a18cb8197>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumEps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# reset search tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0miterationTrainExamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteEpisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainExamplesHistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterationTrainExamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-e05a18cb8197>\u001b[0m in \u001b[0;36mexecuteEpisode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodeStep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtempThreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetActionProb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e9551e80bb73>\u001b[0m in \u001b[0;36mgetActionProb\u001b[0;34m(self, graphState, temp)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \"\"\"\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumMCTSSims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstringRepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e9551e80bb73>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, graphState, num_sim)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# next_s = self.game.getCanonicalForm(next_s, next_player)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQsa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e9551e80bb73>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, graphState, num_sim)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# next_s = self.game.getCanonicalForm(next_s, next_player)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQsa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e9551e80bb73>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, graphState, num_sim)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# next_s = self.game.getCanonicalForm(next_s, next_player)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQsa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e9551e80bb73>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, graphState, num_sim)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# next_s = self.game.getCanonicalForm(next_s, next_player)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQsa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e9551e80bb73>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, graphState, num_sim)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# next_s = self.game.getCanonicalForm(next_s, next_player)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQsa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e9551e80bb73>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, graphState, num_sim)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# next_s = self.game.getCanonicalForm(next_s, next_player)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQsa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e9551e80bb73>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, graphState, num_sim)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNextState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;31m# next_s = self.game.getCanonicalForm(next_s, next_player)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-91f3d69009ee>\u001b[0m in \u001b[0;36mgetNextState\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mprev_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_s\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# 1 in left column for visited, 1 in right column for current node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76U50UFgP1E3",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lRbquIwLlXh",
        "colab_type": "code",
        "outputId": "37dbd474-6d99-4bc4-8fcc-e2d7914f567e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "net = NNet(t2, args)\n",
        "examples = []\n",
        "for example in data:\n",
        "    for e in example:\n",
        "        examples.append(e)\n",
        "n2.train(examples)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch 1/15\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.9283 - pi_loss: 0.7930 - v_loss: 0.1352\n",
            "Epoch 2/15\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7254 - pi_loss: 0.5750 - v_loss: 0.1503\n",
            "Epoch 3/15\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6487 - pi_loss: 0.5030 - v_loss: 0.1457\n",
            "Epoch 4/15\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5941 - pi_loss: 0.4610 - v_loss: 0.1332\n",
            "Epoch 5/15\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6008 - pi_loss: 0.4754 - v_loss: 0.1254\n",
            "Epoch 6/15\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5821 - pi_loss: 0.4556 - v_loss: 0.1264\n",
            "Epoch 7/15\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5406 - pi_loss: 0.4202 - v_loss: 0.1204\n",
            "Epoch 8/15\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5174 - pi_loss: 0.4034 - v_loss: 0.1140\n",
            "Epoch 9/15\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5124 - pi_loss: 0.3983 - v_loss: 0.1141\n",
            "Epoch 10/15\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5356 - pi_loss: 0.4221 - v_loss: 0.1135\n",
            "Epoch 11/15\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4951 - pi_loss: 0.3853 - v_loss: 0.1098\n",
            "Epoch 12/15\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4960 - pi_loss: 0.3980 - v_loss: 0.0980\n",
            "Epoch 13/15\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5127 - pi_loss: 0.4172 - v_loss: 0.0954\n",
            "Epoch 14/15\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5075 - pi_loss: 0.3994 - v_loss: 0.1081\n",
            "Epoch 15/15\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5058 - pi_loss: 0.4119 - v_loss: 0.0939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKZ8EXF0LlXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Coach2():\n",
        "    \"\"\"\n",
        "    This class executes the self-play + learning. It uses the functions defined\n",
        "    in Game and NeuralNet. args are specified in main.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "#         self.pnet = self.nnet.__class__(self.game)  # the competitor network\n",
        "        self.args = args\n",
        "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
        "        self.trainExamplesHistory = []    # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
        "        self.skipFirstSelfPlay = False    # can be overriden in loadTrainExamples()\n",
        "\n",
        "    def executeEpisode(self):\n",
        "        \"\"\"\n",
        "        This function executes one episode of self-play, starting with player 1.\n",
        "        As the game is played, each turn is added as a training example to\n",
        "        trainExamples. The game is played till the game ends. After the game\n",
        "        ends, the outcome of the game is used to assign values to each example\n",
        "        in trainExamples.\n",
        "\n",
        "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
        "        uses temp=0.\n",
        "\n",
        "        Returns:\n",
        "            trainExamples: a list of examples of the form (canonicalBoard,pi,v)\n",
        "                           pi is the MCTS informed policy vector, v is +1 if\n",
        "                           the player eventually won the game, else -1.\n",
        "        \"\"\"\n",
        "        trainExamples = []\n",
        "        board = self.game.getInitState()\n",
        "#         self.curPlayer = 1\n",
        "        episodeStep = 0\n",
        "\n",
        "        while True:\n",
        "            episodeStep += 1\n",
        "            temp = int(episodeStep < self.args.tempThreshold)\n",
        "\n",
        "            pi = self.mcts.getActionProb(board, temp=temp)\n",
        "\n",
        "            action = np.random.choice(len(pi), p=pi)\n",
        "            next_board, reward = self.game.getNextState(board, action)\n",
        "            trainExamples.append([board, self.game.graph, pi, reward])\n",
        "\n",
        "            r = self.game.getGameEnded(next_board)\n",
        "            \n",
        "            if r!=0:\n",
        "                return [tuple(x) for x in trainExamples]\n",
        "            \n",
        "            board = next_board\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Performs numIters iterations with numEps episodes of self-play in each\n",
        "        iteration. After every iteration, it retrains neural network with\n",
        "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
        "        It then pits the new neural network against the old one and accepts it\n",
        "        only if it wins >= updateThreshold fraction of games.\n",
        "        \"\"\"\n",
        "        for i in range(1, self.args.numIters+1):\n",
        "            # bookkeeping\n",
        "            print('------ITER ' + str(i) + '------')\n",
        "            # examples of the iteration\n",
        "            if not self.skipFirstSelfPlay or i>1:\n",
        "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
        "\n",
        "                for eps in range(self.args.numEps):\n",
        "                    self.mcts = MCTS(self.game, self.nnet, self.args)   # reset search tree\n",
        "                    iterationTrainExamples += self.executeEpisode()\n",
        "\n",
        "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
        "                \n",
        "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
        "                print(\"len(trainExamplesHistory) =\", len(self.trainExamplesHistory), \" => remove the oldest trainExamples\")\n",
        "                self.trainExamplesHistory.pop(0)\n",
        "\n",
        "            # shuffle examples before training\n",
        "            trainExamples = []\n",
        "            for e in self.trainExamplesHistory:\n",
        "                trainExamples.extend(e)\n",
        "            shuffle(trainExamples)\n",
        "            \n",
        "            # train\n",
        "            # self.nnet.train(trainExamples)\n",
        "        return trainExamples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mLll7wsgmnQ5",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import time, os, sys\n",
        "from pickle import Pickler, Unpickler\n",
        "from random import shuffle\n",
        "\n",
        "\n",
        "class Coach():\n",
        "    \"\"\"\n",
        "    This class executes the self-play + learning. It uses the functions defined\n",
        "    in Game and NeuralNet. args are specified in main.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "#         self.pnet = self.nnet.__class__(self.game)  # the competitor network\n",
        "        self.args = args\n",
        "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
        "        self.trainExamplesHistory = []    # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
        "        self.skipFirstSelfPlay = False    # can be overriden in loadTrainExamples()\n",
        "\n",
        "    def executeEpisode(self):\n",
        "        \"\"\"\n",
        "        This function executes one episode of self-play, starting with player 1.\n",
        "        As the game is played, each turn is added as a training example to\n",
        "        trainExamples. The game is played till the game ends. After the game\n",
        "        ends, the outcome of the game is used to assign values to each example\n",
        "        in trainExamples.\n",
        "\n",
        "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
        "        uses temp=0.\n",
        "\n",
        "        Returns:\n",
        "            trainExamples: a list of examples of the form (canonicalBoard,pi,v)\n",
        "                           pi is the MCTS informed policy vector, v is +1 if\n",
        "                           the player eventually won the game, else -1.\n",
        "        \"\"\"\n",
        "        trainExamples = []\n",
        "        board = self.game.getInitState()\n",
        "#         self.curPlayer = 1\n",
        "        episodeStep = 0\n",
        "\n",
        "        while True:\n",
        "            episodeStep += 1\n",
        "            temp = int(episodeStep < self.args.tempThreshold)\n",
        "\n",
        "            pi = self.mcts.getActionProb(board, temp=temp)\n",
        "\n",
        "            action = np.random.choice(len(pi), p=pi)\n",
        "            next_board, reward = self.game.getNextState(board, action)\n",
        "            trainExamples.append([board, pi, reward])\n",
        "\n",
        "            r = self.game.getGameEnded(next_board)\n",
        "            \n",
        "            if r!=0:\n",
        "                return [tuple(x) for x in trainExamples]\n",
        "            \n",
        "            board = next_board\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Performs numIters iterations with numEps episodes of self-play in each\n",
        "        iteration. After every iteration, it retrains neural network with\n",
        "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
        "        It then pits the new neural network against the old one and accepts it\n",
        "        only if it wins >= updateThreshold fraction of games.\n",
        "        \"\"\"\n",
        "        for i in range(1, self.args.numIters+1):\n",
        "            # bookkeeping\n",
        "            print('------ITER ' + str(i) + '------')\n",
        "            # examples of the iteration\n",
        "            if not self.skipFirstSelfPlay or i>1:\n",
        "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
        "\n",
        "                for eps in range(self.args.numEps):\n",
        "                    self.mcts = MCTS(self.game, self.nnet, self.args)   # reset search tree\n",
        "                    iterationTrainExamples += self.executeEpisode()\n",
        "\n",
        "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
        "                \n",
        "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
        "                print(\"len(trainExamplesHistory) =\", len(self.trainExamplesHistory), \" => remove the oldest trainExamples\")\n",
        "                self.trainExamplesHistory.pop(0)\n",
        "\n",
        "            # shuffle examples before training\n",
        "            trainExamples = []\n",
        "            for e in self.trainExamplesHistory:\n",
        "                trainExamples.extend(e)\n",
        "            shuffle(trainExamples)\n",
        "            \n",
        "            # train\n",
        "            self.nnet.train(trainExamples)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OWO89Uw8mnTW",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BiQOBbM6mnTr",
        "colab": {}
      },
      "source": [
        "class NNet():\n",
        "    def __init__(self, game, args):\n",
        "        self.b = game.num_node\n",
        "        self.action_size = game.getActionSize()\n",
        "        \n",
        "        self.create_net(game)\n",
        "\n",
        "    def train(self, examples):\n",
        "        \"\"\"\n",
        "        examples: list of examples, each example is of form (board, pi, v)\n",
        "        \"\"\"\n",
        "        print(\"Training...\")\n",
        "        input_boards, input_graphs, target_pis, target_vs = list(zip(*examples))\n",
        "        input_boards = np.asarray(input_boards)\n",
        "        input_graphs = np.asarray(input_graphs)\n",
        "        target_pis = np.asarray(target_pis)\n",
        "        target_vs = np.asarray(target_vs)\n",
        "        self.model.fit(x = [input_boards, input_graphs], y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n",
        "\n",
        "    def predict(self, board, graph):\n",
        "        \"\"\"\n",
        "        board: np array with board\n",
        "        \"\"\"\n",
        "        # preparing input\n",
        "        board = board[np.newaxis, :, :]\n",
        "        graph = graph[np.newaxis, :, :]\n",
        "\n",
        "        # run\n",
        "        pi, v = self.model.predict([board, graph])\n",
        "\n",
        "        #print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n",
        "        return pi[0], v[0]\n",
        "    \n",
        "    def create_net(self, game):\n",
        "        # Neural Net\n",
        "        self.input_boards = Input(shape=game.getInitState().shape)    # s: batch_size x board_x x board_y\n",
        "\n",
        "        x_image = Reshape((args['num_node'], 2, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n",
        "        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(x_image)))         # batch_size  x board_x x board_y x num_channels\n",
        "        h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n",
        "        h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(h_conv2)))        # batch_size  x (board_x) x (board_y) x num_channels\n",
        "        h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='valid')(h_conv3)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n",
        "        h_conv4_flat = Flatten()(h_conv4)       \n",
        "        s_fc1 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024)(h_conv4_flat))))  # batch_size x 1024\n",
        "        s_fc2 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512)(s_fc1))))          # batch_size x 1024\n",
        "        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size\n",
        "        self.v = Dense(1, activation='tanh', name='v')(s_fc2)                    # batch_size x 1\n",
        "\n",
        "        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n",
        "        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "txeK0iKtmnUC",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# t = TSPGame(args.num_node)\n",
        "# n = NNet(t, args)\n",
        "# c = Coach(t, n, args)\n",
        "# c.learn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVKaiF1KQCL-",
        "colab_type": "text"
      },
      "source": [
        "MCTS with trained net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C70qEEn0mnUa",
        "colab": {}
      },
      "source": [
        "mcts = MCTS(t, n, args)\n",
        "state = t.getInitState()\n",
        "mcts_reward = 0\n",
        "mcts_actions = []\n",
        "optimal_val, optimal_path = t.optimal()\n",
        "\n",
        "while not t.getGameEnded(state):\n",
        "    p = mcts.getActionProb(state)\n",
        "    print(p)\n",
        "    action = np.argmax(p)\n",
        "    state, reward = t.getNextState(state, action)\n",
        "    mcts_actions.append(action)\n",
        "    mcts_reward += reward\n",
        "\n",
        "print('Optimal Solution:', optimal_val)\n",
        "print('Optimal Action:', optimal_path)\n",
        "print('MCTS Reward:', t.getActionSize() - mcts_reward)\n",
        "print('MCTS Action:', mcts_actions)\n",
        "\n",
        "mcts.plot, mcts.num_sim, optimal_val"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}