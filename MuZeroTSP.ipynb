{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuZeroTSP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh4d1C3rKS_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "import gym\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "from .abstract_game import AbstractGame\n",
        "import TSPGame \n",
        "\n",
        "\n",
        "class MuZeroConfig:\n",
        "    def __init__(self):\n",
        "        self.seed = 0  # Seed for numpy, torch and the game\n",
        "\n",
        "\n",
        "        ### Game\n",
        "        self.observation_shape = (3, 3, 3)  # Dimensions of the game observation, must be 3D. For a 1D array, please reshape it to (1, 1, length of array)\n",
        "        self.action_space = [i for i in range(10)]  # Fixed list of all possible actions. You should only edit the length\n",
        "        self.players = [i for i in range(1)]  # List of players. You should only edit the length\n",
        "        self.stacked_observations = 0  # Number of previous observation to add to the current observation\n",
        "\n",
        "\n",
        "        ### Self-Play\n",
        "        self.num_actors = 1  # Number of simultaneous threads self-playing to feed the replay buffer\n",
        "        self.max_moves = 12  # Maximum number of moves if game is not finished before\n",
        "        self.num_simulations = 25  # Number of future moves self-simulated\n",
        "        self.discount = 1  # Chronological discount of the reward\n",
        "        self.temperature_threshold = 6  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n",
        "        self.self_play_delay = 0  # Number of seconds to wait after each played game to adjust the self play / training ratio to avoid over/underfitting\n",
        "\n",
        "        # Root prior exploration noise\n",
        "        self.root_dirichlet_alpha = 0.1\n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB formula\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "\n",
        "        ### Network\n",
        "        self.network = \"resnet\"  # \"resnet\" / \"fullyconnected\"\n",
        "        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n",
        "\n",
        "        # Residual Network\n",
        "        self.blocks = 1  # Number of blocks in the ResNet\n",
        "        self.channels = 16  # Number of channels in the ResNet\n",
        "        self.reduced_channels = 16  # Number of channels before heads of dynamic and prediction networks\n",
        "        self.resnet_fc_reward_layers = [8]  # Define the hidden layers in the reward head of the dynamic network\n",
        "        self.resnet_fc_value_layers = [8]  # Define the hidden layers in the value head of the prediction network\n",
        "        self.resnet_fc_policy_layers = [8]  # Define the hidden layers in the policy head of the prediction network\n",
        "\n",
        "        # Fully Connected Network\n",
        "        self.encoding_size = 32\n",
        "        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n",
        "        self.fc_value_layers = []  # Define the hidden layers in the value network\n",
        "        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n",
        "        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n",
        "        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n",
        "\n",
        "\n",
        "        ### Training\n",
        "        self.results_path = os.path.join(os.path.dirname(__file__), \"../results\", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\"))  # Path to store the model weights and TensorBoard logs\n",
        "        self.training_steps = 100000  # Total number of training steps (ie weights update according to a batch)\n",
        "        self.batch_size = 64  # Number of parts of games to train on at each training step\n",
        "        self.num_unroll_steps = 20  # Number of game moves to keep for every batch element\n",
        "        self.checkpoint_interval = 10  # Number of training steps before using the model for sef-playing\n",
        "        self.window_size = 3000  # Number of self-play games to keep in the replay buffer\n",
        "        self.td_steps = 20  # Number of steps in the future to take into account for calculating the target value\n",
        "        self.training_delay = 0  # Number of seconds to wait after each training to adjust the self play / training ratio to avoid over/underfitting\n",
        "        self.value_loss_weight = 0.25  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n",
        "        self.training_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Train on GPU if available\n",
        "\n",
        "        self.weight_decay = 1e-4  # L2 weights regularization\n",
        "        self.momentum = 0.9\n",
        "\n",
        "        # Prioritized Replay (See paper appendix Training)\n",
        "        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n",
        "        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n",
        "        self.PER_beta = 1.0\n",
        "\n",
        "        # Exponential learning rate schedule\n",
        "        self.lr_init = 0.01  # Initial learning rate\n",
        "        self.lr_decay_rate = 1  # Set it to 1 to use a constant learning rate\n",
        "        self.lr_decay_steps = 10000\n",
        "\n",
        "\n",
        "        ### Test\n",
        "        self.test_episodes = 2  # Number of games rendered when calling the MuZero test method\n",
        "\n",
        "\n",
        "    def visit_softmax_temperature_fn(self, trained_steps):\n",
        "        \"\"\"\n",
        "        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n",
        "        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n",
        "\n",
        "        Returns:\n",
        "            Positive float.\n",
        "        \"\"\"\n",
        "        return 1\n",
        "\n",
        "\n",
        "class TSP(AbstractGame):\n",
        "    \"\"\"\n",
        "    Game wrapper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seed=None):\n",
        "        self.env = TSPGame()\n",
        "\n",
        "    def step(self, state, action):\n",
        "        \"\"\"\n",
        "        Apply action to the game.\n",
        "        \n",
        "        Args:\n",
        "            action : action of the action_space to take.\n",
        "\n",
        "        Returns:\n",
        "            The new observation, the reward and a boolean if the game has ended.\n",
        "        \"\"\"\n",
        "        observation, reward = self.env.getNextState(state, action)\n",
        "        done = self.env.getGameEnded(state)\n",
        "        return observation, reward*20, done\n",
        "\n",
        "    def to_play(self):\n",
        "        \"\"\"\n",
        "        Return the current player.\n",
        "\n",
        "        Returns:\n",
        "            The current player, it should be an element of the players list in the config. \n",
        "        \"\"\"\n",
        "        return 0\n",
        "\n",
        "    def legal_actions(self):\n",
        "        \"\"\"\n",
        "        Should return the legal actions at each turn, if it is not available, it can return\n",
        "        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n",
        "        \n",
        "        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n",
        "        equal to the action space but to return a negative reward if the action is illegal.\n",
        "    \n",
        "        Returns:\n",
        "            An array of integers, subset of the action space.\n",
        "        \"\"\"\n",
        "        return self.env.getValidMoves()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the game for a new game.\n",
        "        \n",
        "        Returns:\n",
        "            Initial observation of the game.\n",
        "        \"\"\"\n",
        "        return self.env.reset()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Properly close the game.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    # def render(self):\n",
        "    #     \"\"\"\n",
        "    #     Display the game observation.\n",
        "    #     \"\"\"\n",
        "    #     self.env.render()\n",
        "    #     input(\"Press enter to take a step \")\n",
        "\n",
        "    def encode_board(self):\n",
        "        return self.env.encode_board()\n",
        "\n",
        "    # def human_action(self):\n",
        "    #     \"\"\"\n",
        "    #     For multiplayer games, ask the user for a legal action\n",
        "    #     and return the corresponding action number.\n",
        "\n",
        "    #     Returns:\n",
        "    #         An integer from the action space.\n",
        "    #     \"\"\"\n",
        "    #     choice = input(\n",
        "    #         \"Enter the column to play for the player {}: \".format(self.to_play())\n",
        "    #     )\n",
        "    #     while choice not in [str(action) for action in self.legal_actions()]:\n",
        "    #         choice = input(\"Enter another column : \")\n",
        "    #     return int(choice)\n",
        "\n",
        "    def print_action(self, action_number):\n",
        "        \"\"\"\n",
        "        Convert an action number to a string representing the action.\n",
        "        \n",
        "        Args:\n",
        "            action_number: an integer from the action space.\n",
        "\n",
        "        Returns:\n",
        "            String representing the action.\n",
        "        \"\"\"\n",
        "        return \"Play column {}\".format(action_number)\n",
        "\n",
        "\n",
        "# class TicTacToe:\n",
        "#     def __init__(self):\n",
        "#         self.board = numpy.zeros((3, 3)).astype(int)\n",
        "#         self.player = 1\n",
        "\n",
        "#     def to_play(self):\n",
        "#         return 0 if self.player == 1 else 1\n",
        "\n",
        "#     def reset(self):\n",
        "#         self.board = numpy.zeros((3, 3)).astype(int)\n",
        "#         self.player = 1\n",
        "#         return self.get_observation()\n",
        "\n",
        "#     def step(self, action):\n",
        "#         row = action // 3\n",
        "#         col = action % 3\n",
        "#         self.board[row, col] = self.player\n",
        "\n",
        "#         done = self.is_finished()\n",
        "\n",
        "#         reward = 1 if done and 0 < len(self.legal_actions()) else 0\n",
        "\n",
        "#         self.player *= -1\n",
        "\n",
        "#         return self.get_observation(), reward, done\n",
        "\n",
        "#     def get_observation(self):\n",
        "#         board_player1 = numpy.where(self.board == 1, 1.0, 0.0)\n",
        "#         board_player2 = numpy.where(self.board == -1, 1.0, 0.0)\n",
        "#         board_to_play = numpy.full((3, 3), self.player).astype(float)\n",
        "#         return numpy.array([board_player1, board_player2, board_to_play])\n",
        "\n",
        "#     def legal_actions(self):\n",
        "#         legal = []\n",
        "#         for i in range(9):\n",
        "#             row = i // 3\n",
        "#             col = i % 3\n",
        "#             if self.board[row, col] == 0:\n",
        "#                 legal.append(i)\n",
        "#         return legal\n",
        "\n",
        "#     def is_finished(self):\n",
        "#         # Horizontal and vertical checks\n",
        "#         for i in range(3):\n",
        "#             if (self.board[i, :] == self.player * numpy.ones(3).astype(int)).all():\n",
        "#                 return True\n",
        "#             if (self.board[:, i] == self.player * numpy.ones(3).astype(int)).all():\n",
        "#                 return True\n",
        "\n",
        "#         # Diagonal checks\n",
        "#         if (\n",
        "#             self.board[0, 0] == self.player\n",
        "#             and self.board[1, 1] == self.player\n",
        "#             and self.board[2, 2] == self.player\n",
        "#         ):\n",
        "#             return True\n",
        "#         if (\n",
        "#             self.board[2, 0] == self.player\n",
        "#             and self.board[1, 1] == self.player\n",
        "#             and self.board[0, 2] == self.player\n",
        "#         ):\n",
        "#             return True\n",
        "\n",
        "#         # No legal actions means a draw\n",
        "#         if len(self.legal_actions()) == 0:\n",
        "#             return True\n",
        "\n",
        "#         return False\n",
        "\n",
        "#     def render(self):\n",
        "#         print(self.board[::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}