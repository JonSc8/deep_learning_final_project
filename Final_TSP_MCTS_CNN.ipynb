{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Final-TSP-MCTS-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqiNC1Cybew5",
        "colab_type": "text"
      },
      "source": [
        "Deep Learning Exercise\n",
        "---\n",
        "Jonathan Schwartz & Jonathan Schory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7deoyHHPMpr",
        "colab_type": "text"
      },
      "source": [
        "MCTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-gBF5q7yyI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "EPS = 1e-8\n",
        "\n",
        "class MCTS():\n",
        "    \"\"\"\n",
        "    This class handles the MCTS tree.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "        self.args = args\n",
        "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
        "        self.Nsa = {}       # stores #times edge s,a was visited\n",
        "        self.Ns = {}        # stores #times board s was visited\n",
        "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
        "\n",
        "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
        "        self.Vs = {}        # stores game.getValidMoves for board s\n",
        "        \n",
        "        self.plot = [1000]\n",
        "        self.num_sim = [0]\n",
        "\n",
        "    def getActionProb(self, graphState, temp=1):\n",
        "        \"\"\"\n",
        "        This function performs numMCTSSims simulations of MCTS starting from\n",
        "        canonicalBoard.\n",
        "        Returns:\n",
        "            probs: a policy vector where the probability of the ith action is\n",
        "                   proportional to Nsa[(s,a)]**(1./temp)\n",
        "        \"\"\"\n",
        "        for i in range(self.args.numMCTSSims):\n",
        "            self.search(graphState, i)\n",
        "\n",
        "        s = self.game.stringRepresentation(graphState)\n",
        "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
        "\n",
        "        if temp==0:\n",
        "            bestA = np.argmax(counts)\n",
        "            probs = [0]*len(counts)\n",
        "            probs[bestA]=1\n",
        "            return probs\n",
        "\n",
        "        counts = [x**(1./temp) for x in counts]\n",
        "        counts_sum = float(sum(counts))\n",
        "        probs = [x/counts_sum for x in counts]\n",
        "        return probs\n",
        "\n",
        "\n",
        "    def search(self, graphState, num_sim):\n",
        "        \"\"\"\n",
        "        This function performs one iteration of MCTS. It is recursively called\n",
        "        till a leaf node is found. The action chosen at each node is one that\n",
        "        has the maximum upper confidence bound as in the paper.\n",
        "        Once a leaf node is found, the neural network is called to return an\n",
        "        initial policy P and a value v for the state. This value is propagated\n",
        "        up the search path. In case the leaf node is a terminal state, the\n",
        "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
        "        updated.\n",
        "        NOTE: the return values are the negative of the value of the current\n",
        "        state. This is done since v is in [-1,1] and if v is the value of a\n",
        "        state for the current player, then its value is -v for the other player.\n",
        "        Returns:\n",
        "            v: the negative of the value of the current canonicalBoard\n",
        "        \"\"\"\n",
        "\n",
        "        s = self.game.stringRepresentation(graphState)\n",
        "\n",
        "        if s not in self.Es:\n",
        "            self.Es[s] = self.game.getGameEnded(graphState)\n",
        "        if self.Es[s]!=0:\n",
        "            # terminal node\n",
        "            return 0\n",
        "\n",
        "        if s not in self.Ps:\n",
        "            # leaf node\n",
        "            if self.nnet is not None:\n",
        "                self.Ps[s], v = self.nnet.predict(graphState, self.game.graph)\n",
        "            else:\n",
        "                self.Ps[s] = np.ones(self.game.getActionSize()) # random policy\n",
        "                v = 0\n",
        "            valids = self.game.getValidMoves(graphState)\n",
        "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves\n",
        "            sum_Ps_s = np.sum(self.Ps[s])\n",
        "            if sum_Ps_s > 0:\n",
        "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
        "            else:\n",
        "                # if all valid moves were masked make all valid moves equally probable\n",
        "                \n",
        "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
        "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
        "                print(\"All valid moves were masked, do workaround.\")\n",
        "                self.Ps[s] = self.Ps[s] + valids\n",
        "                self.Ps[s] /= np.sum(self.Ps[s])\n",
        "\n",
        "            self.Vs[s] = valids\n",
        "            self.Ns[s] = 0\n",
        "            return v\n",
        "\n",
        "        valids = self.Vs[s]\n",
        "        cur_best = -float('inf')\n",
        "        best_act = -1\n",
        "\n",
        "        # pick the action with the highest upper confidence bound\n",
        "        for a in range(self.game.getActionSize()):\n",
        "            if valids[a]:\n",
        "                if (s,a) in self.Qsa:\n",
        "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
        "                else:\n",
        "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + EPS)     # Q = 0 ?\n",
        "\n",
        "                if u > cur_best:\n",
        "                    cur_best = u\n",
        "                    best_act = a\n",
        "\n",
        "        a = best_act\n",
        "        next_s, reward = self.game.getNextState(graphState, a)\n",
        "        # next_s = self.game.getCanonicalForm(next_s, next_player)\n",
        "\n",
        "        v = self.search(next_s, num_sim) + reward\n",
        "\n",
        "        if (s,a) in self.Qsa:\n",
        "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
        "            self.Nsa[(s,a)] += 1\n",
        "\n",
        "        else:\n",
        "            self.Qsa[(s,a)] = v\n",
        "            self.Nsa[(s,a)] = 1\n",
        "            \n",
        "        if v > self.game.getActionSize() - self.plot[-1]:\n",
        "            self.plot.append(self.game.getActionSize() - v)\n",
        "            self.num_sim.append(num_sim)\n",
        "\n",
        "        self.Ns[s] += 1\n",
        "        return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRKfej2YPUJx",
        "colab_type": "text"
      },
      "source": [
        "TSP Game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuGlOq6WbTCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import permutations \n",
        "\n",
        "class TSPGame():\n",
        "\n",
        "    def __init__(self, num_node):\n",
        "        self.num_node = num_node # number of nodes in the graph\n",
        "        self.graph = np.random.rand(self.num_node, 2) # each index representing xy coordinates \n",
        "\n",
        "    def getInitState(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            first_state: a representation of the graph\n",
        "            left column representing visited nodes\n",
        "            right column will always have a single 1 and the rest are 0's. index with the 1 in the right column is current node\n",
        "        \"\"\"\n",
        "        first_state = np.zeros([self.num_node, 2])\n",
        "        # Always start with first node as current node \n",
        "        first_state[0][0] = 1\n",
        "        first_state[0][1] = 1\n",
        "        return first_state\n",
        "\n",
        "    def getBoardSize(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            (x,y): a tuple of board dimensions\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getActionSize(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            actionSize: number of all possible actions\n",
        "        \"\"\"\n",
        "        return self.num_node\n",
        "    \n",
        "    def getNextState(self, state, action):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "            action: action taken by current player\n",
        "        Returns:\n",
        "            next_s: graph after applying action\n",
        "            reward: reward from action\n",
        "        \"\"\"\n",
        "        \n",
        "        next_s = state.copy()\n",
        "        # zero out current node\n",
        "        for n in next_s:\n",
        "          if n[1] == 1:\n",
        "            prev_action = np.where(next_s == n)[0][0]\n",
        "            n[1] = 0\n",
        "        # 1 in left column for visited, 1 in right column for current node\n",
        "        next_s[action] = 1\n",
        "        \n",
        "        # get xy coordinates for prev_node and current_node from the graph\n",
        "        prev_node = self.graph[prev_action]\n",
        "        current_node = self.graph[action]\n",
        "          \n",
        "        reward = 1 - np.linalg.norm(current_node - prev_node)\n",
        "        if self.getCountVisitedNodes(next_s) == self.num_node: #end of game\n",
        "            reward += 1 - np.linalg.norm(current_node - self.graph[0])\n",
        "            \n",
        "        return next_s, reward\n",
        "\n",
        "    def getValidMoves(self, state):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "        Returns:\n",
        "            list of valid moves, 1 for valid, 0 for invalid \n",
        "        \"\"\"\n",
        "        return 1 - state[:, 0]\n",
        "\n",
        "    def getCountVisitedNodes(self, state):\n",
        "      count_visited = 0\n",
        "      for n in state:\n",
        "        if n[0] == 1:\n",
        "          count_visited += 1\n",
        "      \n",
        "      return count_visited\n",
        "    \n",
        "    def getGameEnded(self, state):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "        Returns:\n",
        "            r: 0 if game has not ended. 1 if it has\n",
        "               \n",
        "        \"\"\"\n",
        "        end = 0\n",
        "        if self.getCountVisitedNodes(state) == self.num_node:\n",
        "            end = 1\n",
        "        return end\n",
        "\n",
        "    def getCanonicalForm(self, board, player):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            board: current board\n",
        "            player: current player (1 or -1)\n",
        "        Returns:\n",
        "            canonicalBoard: returns canonical form of board. The canonical form\n",
        "                            should be independent of player. For e.g. in chess,\n",
        "                            the canonical form can be chosen to be from the pov\n",
        "                            of white. When the player is white, we can return\n",
        "                            board as is. When the player is black, we can invert\n",
        "                            the colors and return the board.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getSymmetries(self, board, pi):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            board: current board\n",
        "            pi: policy vector of size self.getActionSize()\n",
        "        Returns:\n",
        "            symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n",
        "                       form of the board and the corresponding pi vector. This\n",
        "                       is used when training the neural network from examples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def stringRepresentation(self, state):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            state: current state\n",
        "        Returns:\n",
        "            index of state\n",
        "        \"\"\"\n",
        "        s = ''\n",
        "        for i in range(self.num_node):\n",
        "            s += str(int(state[i][0]))\n",
        "        return s\n",
        "    \n",
        "    def optimal(self):\n",
        "        seq = np.arange(self.num_node)[1:]\n",
        "        perm = permutations(seq)\n",
        "        graph = self.graph\n",
        "        reward = 0\n",
        "        optimal = 10000\n",
        "        action = []\n",
        "        for p in list(perm):\n",
        "            reward = 0\n",
        "            reward += np.linalg.norm(graph[0] - graph[p[0]])\n",
        "            reward += np.linalg.norm(graph[p[-1]] - graph[0])\n",
        "            \n",
        "            for k in range(len(p) - 1):\n",
        "                i = p[k]\n",
        "                j = p[k+1]\n",
        "                reward += np.linalg.norm(graph[j] - graph[i])\n",
        "            \n",
        "            if reward < optimal:\n",
        "                optimal = reward\n",
        "                action = p\n",
        "        \n",
        "        return optimal, action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrTTX6B0Phrf",
        "colab_type": "text"
      },
      "source": [
        "Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2quDyDMDbTCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MCTS_args(object):\n",
        "    def __init__(self, num_sim, cpuct):\n",
        "        self.numMCTSSims = num_sim\n",
        "        self.cpuct = cpuct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIB4kC3xbTCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_TSP_MCTS(num_node, args):\n",
        "    game = TSPGame(num_node)\n",
        "    mcts = MCTS(game, None, args) \n",
        "    state = game.getInitState()\n",
        "    r = 0\n",
        "    actions = []\n",
        "    optimal_val, optimal_path = game.optimal()\n",
        "    \n",
        "    while not game.getGameEnded(state):\n",
        "        action = np.argmax(mcts.getActionProb(state))\n",
        "        state, reward = game.getNextState(state, action)\n",
        "        actions.append(action)\n",
        "        r += reward\n",
        "    \n",
        "    return mcts.plot, mcts.num_sim, optimal_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu1sBOUNbTC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimal_list = []\n",
        "mcts_dict = {}\n",
        "num_node = 10\n",
        "args = MCTS_args(num_sim = 20000, cpuct = 1)\n",
        "for i in range(100):\n",
        "  vals, num_s, optimalReward = play_TSP_MCTS(num_node, args)\n",
        "  optimal_list.append(optimalReward)\n",
        "  mcts_dict[i] = {tuple(num_s): vals}\n",
        "\n",
        "print(optimal_list)\n",
        "print(mcts_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbpefDE5Hq7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(num_s[1:]), vals[1:])\n",
        "plt.xlabel(\"num_simulations\", fontsize=12)\n",
        "plt.ylabel(\"path_cost\", fontsize=12)\n",
        "plt.hlines(optimalReward, 0, num_s[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea_rbJhCW-ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "columns = list(range(100))\n",
        "df = pd.DataFrame(columns=columns)\n",
        "i = 0\n",
        "for result in mcts_dict:\n",
        "  sims = list(mcts_dict[result].keys())[0]\n",
        "  vals = list(mcts_dict[result].values())[0]\n",
        "  col = np.zeros(20000)\n",
        "  j = 0\n",
        "  for sim in sims:\n",
        "    col[sim] = vals[j]\n",
        "    j += 1\n",
        "  df[i] = col\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT9EcubrXN7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in df:\n",
        "  df[col] = df[col].replace(0.0, pd.np.nan)\n",
        "  df[col] = df[col].ffill()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he8pJ6QVXQP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(df.head(1).index, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVEkze1dXQ0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_val_row = pd.DataFrame([pd.Series(optimal_list)], index=[\"opt\"], dtype=float)\n",
        "df = pd.concat([opt_val_row, df])\n",
        "df.to_csv(\"games.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t7TRz1KXTUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_vals = df[0:1]\n",
        "opt_vals = opt_vals.iloc[0]\n",
        "df.drop(df.head(1).index, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1jiiagaXT6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1TxMwyCXWmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "within_factor = []\n",
        "for i in range(19999):\n",
        "  row = list(df.iloc[i])\n",
        "  counter = 0\n",
        "  for j in range(len(row)):\n",
        "    if row[j] / opt_vals[j] <= 1.1:\n",
        "      counter += 1\n",
        "  within_factor += [counter / 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2CGSw_7XW62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(range(19999)), within_factor)\n",
        "plt.title(\"Percentage of random instances which MCTS solved within a 1.1 factor of the optimal solution as a function of simulations\")\n",
        "plt.xlabel(\"num_simulations\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.ylim(top=within_factor[-1] + 0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsEB3ioOC2Yb",
        "colab_type": "text"
      },
      "source": [
        "**CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OFL6ZnmPoso",
        "colab_type": "text"
      },
      "source": [
        "Network Arguments "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7HSZvnyHXm1P",
        "colab": {}
      },
      "source": [
        "class dotdict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "\n",
        "args = dotdict({\n",
        "    # MCTS args\n",
        "    'numMCTSSims': 20000,          # Number of games moves for MCTS to simulate.\n",
        "    'num_node': 10,                 # Number of nodes in the graph (game)\n",
        "    \n",
        "    # Train args\n",
        "    'numIters': 10,              # Number of episods to play (5 times 10 episodes)\n",
        "    'numEps': 20,              # Number of complete self-play games to simulate during a new iteration.\n",
        "    'tempThreshold': 15,        #\n",
        "    'updateThreshold': 0.6,     # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
        "    'maxlenOfQueue': 200000,    # Number of game examples to train the neural networks.\n",
        "    'arenaCompare': 40,         # Number of games to play during arena play to determine if new net will be accepted.\n",
        "    'cpuct': 1,\n",
        "    'numItersForTrainExamplesHistory': 25,\n",
        "    \n",
        "    # NN args\n",
        "    'lr': 0.001,\n",
        "    'dropout': 0.3,\n",
        "    'epochs': 15,\n",
        "    'batch_size': 64,\n",
        "    'cuda': False,\n",
        "    'num_channels': 512,\n",
        "    })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68AWFhIdPx3b",
        "colab_type": "text"
      },
      "source": [
        "Create Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fttq0wxZLlXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game = TSPGame(args.num_node)\n",
        "net = NNet(game, args)\n",
        "data = []\n",
        "for i in range(2):\n",
        "    coach = Coach2(game, net, args)\n",
        "    data.append(coach.learn())\n",
        "    game = TSPGame(args.num_node)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76U50UFgP1E3",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lRbquIwLlXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = NNet(t2, args)\n",
        "examples = []\n",
        "for example in data:\n",
        "    for e in example:\n",
        "        examples.append(e)\n",
        "n2.train(examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKZ8EXF0LlXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Coach2():\n",
        "    \"\"\"\n",
        "    This class executes the self-play + learning. It uses the functions defined\n",
        "    in Game and NeuralNet. args are specified in main.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "#         self.pnet = self.nnet.__class__(self.game)  # the competitor network\n",
        "        self.args = args\n",
        "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
        "        self.trainExamplesHistory = []    # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
        "        self.skipFirstSelfPlay = False    # can be overriden in loadTrainExamples()\n",
        "\n",
        "    def executeEpisode(self):\n",
        "        \"\"\"\n",
        "        This function executes one episode of self-play, starting with player 1.\n",
        "        As the game is played, each turn is added as a training example to\n",
        "        trainExamples. The game is played till the game ends. After the game\n",
        "        ends, the outcome of the game is used to assign values to each example\n",
        "        in trainExamples.\n",
        "\n",
        "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
        "        uses temp=0.\n",
        "\n",
        "        Returns:\n",
        "            trainExamples: a list of examples of the form (canonicalBoard,pi,v)\n",
        "                           pi is the MCTS informed policy vector, v is +1 if\n",
        "                           the player eventually won the game, else -1.\n",
        "        \"\"\"\n",
        "        trainExamples = []\n",
        "        board = self.game.getInitState()\n",
        "#         self.curPlayer = 1\n",
        "        episodeStep = 0\n",
        "\n",
        "        while True:\n",
        "            episodeStep += 1\n",
        "            temp = int(episodeStep < self.args.tempThreshold)\n",
        "\n",
        "            pi = self.mcts.getActionProb(board, temp=temp)\n",
        "\n",
        "            action = np.random.choice(len(pi), p=pi)\n",
        "            next_board, reward = self.game.getNextState(board, action)\n",
        "            trainExamples.append([board, self.game.graph, pi, reward])\n",
        "\n",
        "            r = self.game.getGameEnded(next_board)\n",
        "            \n",
        "            if r!=0:\n",
        "                return [tuple(x) for x in trainExamples]\n",
        "            \n",
        "            board = next_board\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Performs numIters iterations with numEps episodes of self-play in each\n",
        "        iteration. After every iteration, it retrains neural network with\n",
        "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
        "        It then pits the new neural network against the old one and accepts it\n",
        "        only if it wins >= updateThreshold fraction of games.\n",
        "        \"\"\"\n",
        "        for i in range(1, self.args.numIters+1):\n",
        "            # bookkeeping\n",
        "            print('------ITER ' + str(i) + '------')\n",
        "            # examples of the iteration\n",
        "            if not self.skipFirstSelfPlay or i>1:\n",
        "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
        "\n",
        "                for eps in range(self.args.numEps):\n",
        "                    self.mcts = MCTS(self.game, self.nnet, self.args)   # reset search tree\n",
        "                    iterationTrainExamples += self.executeEpisode()\n",
        "\n",
        "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
        "                \n",
        "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
        "                print(\"len(trainExamplesHistory) =\", len(self.trainExamplesHistory), \" => remove the oldest trainExamples\")\n",
        "                self.trainExamplesHistory.pop(0)\n",
        "\n",
        "            # shuffle examples before training\n",
        "            trainExamples = []\n",
        "            for e in self.trainExamplesHistory:\n",
        "                trainExamples.extend(e)\n",
        "            shuffle(trainExamples)\n",
        "            \n",
        "            # train\n",
        "            # self.nnet.train(trainExamples)\n",
        "        return trainExamples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mLll7wsgmnQ5",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import time, os, sys\n",
        "from pickle import Pickler, Unpickler\n",
        "from random import shuffle\n",
        "\n",
        "\n",
        "class Coach():\n",
        "    \"\"\"\n",
        "    This class executes the self-play + learning. It uses the functions defined\n",
        "    in Game and NeuralNet. args are specified in main.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "#         self.pnet = self.nnet.__class__(self.game)  # the competitor network\n",
        "        self.args = args\n",
        "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
        "        self.trainExamplesHistory = []    # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
        "        self.skipFirstSelfPlay = False    # can be overriden in loadTrainExamples()\n",
        "\n",
        "    def executeEpisode(self):\n",
        "        \"\"\"\n",
        "        This function executes one episode of self-play, starting with player 1.\n",
        "        As the game is played, each turn is added as a training example to\n",
        "        trainExamples. The game is played till the game ends. After the game\n",
        "        ends, the outcome of the game is used to assign values to each example\n",
        "        in trainExamples.\n",
        "\n",
        "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
        "        uses temp=0.\n",
        "\n",
        "        Returns:\n",
        "            trainExamples: a list of examples of the form (canonicalBoard,pi,v)\n",
        "                           pi is the MCTS informed policy vector, v is +1 if\n",
        "                           the player eventually won the game, else -1.\n",
        "        \"\"\"\n",
        "        trainExamples = []\n",
        "        board = self.game.getInitState()\n",
        "#         self.curPlayer = 1\n",
        "        episodeStep = 0\n",
        "\n",
        "        while True:\n",
        "            episodeStep += 1\n",
        "            temp = int(episodeStep < self.args.tempThreshold)\n",
        "\n",
        "            pi = self.mcts.getActionProb(board, temp=temp)\n",
        "\n",
        "            action = np.random.choice(len(pi), p=pi)\n",
        "            next_board, reward = self.game.getNextState(board, action)\n",
        "            trainExamples.append([board, pi, reward])\n",
        "\n",
        "            r = self.game.getGameEnded(next_board)\n",
        "            \n",
        "            if r!=0:\n",
        "                return [tuple(x) for x in trainExamples]\n",
        "            \n",
        "            board = next_board\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Performs numIters iterations with numEps episodes of self-play in each\n",
        "        iteration. After every iteration, it retrains neural network with\n",
        "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
        "        It then pits the new neural network against the old one and accepts it\n",
        "        only if it wins >= updateThreshold fraction of games.\n",
        "        \"\"\"\n",
        "        for i in range(1, self.args.numIters+1):\n",
        "            # bookkeeping\n",
        "            print('------ITER ' + str(i) + '------')\n",
        "            # examples of the iteration\n",
        "            if not self.skipFirstSelfPlay or i>1:\n",
        "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
        "\n",
        "                for eps in range(self.args.numEps):\n",
        "                    self.mcts = MCTS(self.game, self.nnet, self.args)   # reset search tree\n",
        "                    iterationTrainExamples += self.executeEpisode()\n",
        "\n",
        "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
        "                \n",
        "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
        "                print(\"len(trainExamplesHistory) =\", len(self.trainExamplesHistory), \" => remove the oldest trainExamples\")\n",
        "                self.trainExamplesHistory.pop(0)\n",
        "\n",
        "            # shuffle examples before training\n",
        "            trainExamples = []\n",
        "            for e in self.trainExamplesHistory:\n",
        "                trainExamples.extend(e)\n",
        "            shuffle(trainExamples)\n",
        "            \n",
        "            # train\n",
        "            self.nnet.train(trainExamples)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OWO89Uw8mnTW",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BiQOBbM6mnTr",
        "colab": {}
      },
      "source": [
        "class NNet():\n",
        "    def __init__(self, game, args):\n",
        "        self.b = game.num_node\n",
        "        self.action_size = game.getActionSize()\n",
        "        \n",
        "        self.create_net(game)\n",
        "\n",
        "    def train(self, examples):\n",
        "        \"\"\"\n",
        "        examples: list of examples, each example is of form (board, pi, v)\n",
        "        \"\"\"\n",
        "        print(\"Training...\")\n",
        "        input_boards, input_graphs, target_pis, target_vs = list(zip(*examples))\n",
        "        input_boards = np.asarray(input_boards)\n",
        "        input_graphs = np.asarray(input_graphs)\n",
        "        target_pis = np.asarray(target_pis)\n",
        "        target_vs = np.asarray(target_vs)\n",
        "        self.model.fit(x = [input_boards, input_graphs], y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n",
        "\n",
        "    def predict(self, board, graph):\n",
        "        \"\"\"\n",
        "        board: np array with board\n",
        "        \"\"\"\n",
        "        # preparing input\n",
        "        board = board[np.newaxis, :, :]\n",
        "        graph = graph[np.newaxis, :, :]\n",
        "\n",
        "        # run\n",
        "        pi, v = self.model.predict([board, graph])\n",
        "\n",
        "        #print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n",
        "        return pi[0], v[0]\n",
        "    \n",
        "    def create_net(self, game):\n",
        "        # Neural Net\n",
        "        self.input_boards = Input(shape=game.getInitState().shape)    # s: batch_size x board_x x board_y\n",
        "\n",
        "        x_image = Reshape((args['num_node'], 2, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n",
        "        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(x_image)))         # batch_size  x board_x x board_y x num_channels\n",
        "        h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n",
        "        h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(h_conv2)))        # batch_size  x (board_x) x (board_y) x num_channels\n",
        "        h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='valid')(h_conv3)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n",
        "        h_conv4_flat = Flatten()(h_conv4)       \n",
        "        s_fc1 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024)(h_conv4_flat))))  # batch_size x 1024\n",
        "        s_fc2 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512)(s_fc1))))          # batch_size x 1024\n",
        "        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size\n",
        "        self.v = Dense(1, activation='tanh', name='v')(s_fc2)                    # batch_size x 1\n",
        "\n",
        "        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n",
        "        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVKaiF1KQCL-",
        "colab_type": "text"
      },
      "source": [
        "MCTS with trained net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k1fSG24OBMMv",
        "colab": {}
      },
      "source": [
        "def play_TSP_MCTS(num_node, args):\n",
        "    game = TSPGame(num_node)\n",
        "    mcts = MCTS(game, net, args) \n",
        "    state = game.getInitState()\n",
        "    r = 0\n",
        "    actions = []\n",
        "    optimal_val, optimal_path = game.optimal()\n",
        "    \n",
        "    while not game.getGameEnded(state):\n",
        "        action = np.argmax(mcts.getActionProb(state))\n",
        "        state, reward = game.getNextState(state, action)\n",
        "        actions.append(action)\n",
        "        r += reward\n",
        "    \n",
        "    return mcts.plot, mcts.num_sim, optimal_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YFmJK6d3BRPo",
        "colab": {}
      },
      "source": [
        "optimal_list = []\n",
        "mcts_dict = {}\n",
        "num_node = 10\n",
        "args = MCTS_args(num_sim = 20000, cpuct = 1)\n",
        "for i in range(100):\n",
        "  vals, num_s, optimalReward = play_TSP_MCTS(num_node, args)\n",
        "  optimal_list.append(optimalReward)\n",
        "  mcts_dict[i] = {tuple(num_s): vals}\n",
        "\n",
        "print(optimal_list)\n",
        "print(mcts_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TZQRbIS-BT39",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(num_s[1:]), vals[1:])\n",
        "plt.xlabel(\"num_simulations\", fontsize=12)\n",
        "plt.ylabel(\"path_cost\", fontsize=12)\n",
        "plt.hlines(optimalReward, 0, num_s[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aekoU-WWBXW6",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "columns = list(range(100))\n",
        "df = pd.DataFrame(columns=columns)\n",
        "i = 0\n",
        "for result in mcts_dict:\n",
        "  sims = list(mcts_dict[result].keys())[0]\n",
        "  vals = list(mcts_dict[result].values())[0]\n",
        "  col = np.zeros(20000)\n",
        "  j = 0\n",
        "  for sim in sims:\n",
        "    col[sim] = vals[j]\n",
        "    j += 1\n",
        "  df[i] = col\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YBRxGTU9Balr",
        "colab": {}
      },
      "source": [
        "for col in df:\n",
        "  df[col] = df[col].replace(0.0, pd.np.nan)\n",
        "  df[col] = df[col].ffill()\n",
        "\n",
        "df.drop(df.head(1).index, inplace=True)\n",
        "\n",
        "opt_val_row = pd.DataFrame([pd.Series(optimal_list)], index=[\"opt\"], dtype=float)\n",
        "df = pd.concat([opt_val_row, df])\n",
        "df.to_csv(\"games.csv\")\n",
        "\n",
        "opt_vals = df[0:1]\n",
        "opt_vals = opt_vals.iloc[0]\n",
        "df.drop(df.head(1).index, inplace=True)\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "within_factor = []\n",
        "for i in range(19999):\n",
        "  row = list(df.iloc[i])\n",
        "  counter = 0\n",
        "  for j in range(len(row)):\n",
        "    if row[j] / opt_vals[j] <= 1.1:\n",
        "      counter += 1\n",
        "  within_factor += [counter / 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLI6ibbjBz_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(range(19999)), within_factor)\n",
        "plt.title(\"Percentage of random instances which MCTS with NNet solved within a 1.1 factor of the optimal solution as a function of simulations\")\n",
        "plt.xlabel(\"num_simulations\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.ylim(top=within_factor[-1] + 0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}