{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Muzero_TSP_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "65Byzcph87Ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJYUoyxy9BZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2-9WODNu4bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import os\n",
        "import gym\n",
        "import numpy\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os38wzt_wv8N",
        "colab_type": "text"
      },
      "source": [
        "#MuZero Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE_Dy-lLvvJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MuZeroConfig:\n",
        "    def __init__(self, num_node):\n",
        "        self.seed = 0  # Seed for numpy, torch and the game\n",
        "\n",
        "        self.observation_shape = (1, num_node, 2)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n",
        "\n",
        "        self.action_space = [i for i in range(num_node)]  # Fixed list of all possible actions. You should only edit the length\n",
        "        print(\"Action space\", self.action_space)\n",
        "        self.players = [i for i in range(1)]  # List of players. You should only edit the length\n",
        "        self.stacked_observations = 0  # Number of previous observation and previous actions to add to the current observation\n",
        "\n",
        "\n",
        "        ### Self-Play\n",
        "        self.num_actors = 1  # Number of simultaneous threads self-playing to feed the replay buffer\n",
        "        self.max_moves = num_node + 1   # Maximum number of moves if game is not finished before \n",
        "        self.num_simulations = 15  # Number of future moves self-simulated\n",
        "        self.discount = 1  # Chronological discount of the reward\n",
        "        self.temperature_threshold = 100  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n",
        "\n",
        "        # Root prior exploration noise\n",
        "        self.root_dirichlet_alpha = 0.1 \n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB formula\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "\n",
        "\n",
        "        ### Network\n",
        "        self.network = \"resnet\"  # \"resnet\" / \"fullyconnected\"\n",
        "        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n",
        "\n",
        "        # Residual Network\n",
        "        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n",
        "        self.blocks = 1  # Number of blocks in the ResNet\n",
        "        self.channels = 16  # Number of channels in the ResNet\n",
        "        self.reduced_channels = 16  # Number of channels before heads of dynamic and prediction networks\n",
        "        self.resnet_fc_reward_layers = [8]  # Define the hidden layers in the reward head of the dynamic network\n",
        "        self.resnet_fc_value_layers = [8]  # Define the hidden layers in the value head of the prediction network\n",
        "        self.resnet_fc_policy_layers = [8]  # Define the hidden layers in the policy head of the prediction network\n",
        "\n",
        "        # Fully Connected Network\n",
        "        self.encoding_size = 32\n",
        "        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n",
        "        self.fc_value_layers = []  # Define the hidden layers in the value network\n",
        "        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n",
        "        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n",
        "        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n",
        "\n",
        "\n",
        "        ### Training\n",
        "        self.results_path = os.path.join(os.path.dirname(\"/content/drive/My Drive/deep-learning-final\"), \"/results\", os.path.basename(\"/content/drive/My Drive/deep-learning-final\")[:-3], datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\"))  # Path to store the model weights and TensorBoard logs\n",
        "        self.training_steps = 100000  # Total number of training steps (ie weights update according to a batch)\n",
        "        self.batch_size = 64  # Number of parts of games to train on at each training step\n",
        "        self.checkpoint_interval = 10  # Number of training steps before using the model for sef-playing\n",
        "        self.value_loss_weight = 0.25  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n",
        "        self.training_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Train on GPU if available\n",
        "\n",
        "        self.optimizer = \"SGD\"  # \"Adam\" or \"SGD\". Paper uses SGD\n",
        "        self.weight_decay = 1e-4  # L2 weights regularization\n",
        "        self.momentum = 0.9  # Used only if optimizer is SGD\n",
        "\n",
        "        # Exponential learning rate schedule\n",
        "        self.lr_init = 0.01  # Initial learning rate\n",
        "        self.lr_decay_rate = 1  # Set it to 1 to use a constant learning rate\n",
        "        self.lr_decay_steps = 10000\n",
        "\n",
        "\n",
        "        ### Replay Buffer\n",
        "        self.window_size = 3000  # Number of self-play games to keep in the replay buffer\n",
        "        self.num_unroll_steps = 20  # Number of game moves to keep for every batch element\n",
        "        self.td_steps = 20  # Number of steps in the future to take into account for calculating the target value\n",
        "        self.use_last_model_value = False  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n",
        "\n",
        "        # Prioritized Replay (See paper appendix Training)\n",
        "        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n",
        "        self.use_max_priority = True  # Use the n-step TD error as initial priority. Better for large replay buffer\n",
        "        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n",
        "        self.PER_beta = 1.0\n",
        "\n",
        "\n",
        "        ### Adjust the self play / training ratio to avoid over/underfitting\n",
        "        self.self_play_delay = 0  # Number of seconds to wait after each played game\n",
        "        self.training_delay = 0  # Number of seconds to wait after each training step\n",
        "        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n",
        "\n",
        "    def visit_softmax_temperature_fn(self, trained_steps):\n",
        "      # Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n",
        "      # The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n",
        "\n",
        "      # Returns:\n",
        "      # Positive float.\n",
        "      if trained_steps < 0.5 * self.training_steps:\n",
        "          return 0.5\n",
        "      elif trained_steps < 0.75 * self.training_steps:\n",
        "          return 0.25\n",
        "      else:\n",
        "          return 0.01\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Pw3EuXz-Eb",
        "colab_type": "text"
      },
      "source": [
        "#Class Game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I34AgKsGv5CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Game():\n",
        "    \"\"\"\n",
        "    Game wrapper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_node, seed=None):\n",
        "        self.env = TSP(num_node)\n",
        "\n",
        "    def step(self, action, state):\n",
        "        \"\"\"\n",
        "        Apply action to the game.\n",
        "        \n",
        "        Args:\n",
        "            action : action of the action_space to take.\n",
        "\n",
        "        Returns:\n",
        "            The new observation, the reward and a boolean if the game has ended.\n",
        "        \"\"\"\n",
        "        state, reward, done = self.env.step(action, state)\n",
        "        return state, reward, done\n",
        "\n",
        "    def to_play(self):\n",
        "        \"\"\"\n",
        "        Return the current player.\n",
        "\n",
        "        Returns:\n",
        "            The current player, it should be an element of the players list in the config. \n",
        "        \"\"\"\n",
        "        return self.env.to_play()\n",
        "\n",
        "    def legal_actions(self, state):\n",
        "        \"\"\"\n",
        "        Should return the legal actions at each turn, if it is not available, it can return\n",
        "        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n",
        "        \n",
        "        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n",
        "        equal to the action space but to return a negative reward if the action is illegal.\n",
        "    \n",
        "        Returns:\n",
        "            An array of integers, subset of the action space.\n",
        "        \"\"\"\n",
        "        return self.env.legal_actions()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the game for a new game.\n",
        "        \n",
        "        Returns:\n",
        "            Initial state of the game.\n",
        "        \"\"\"\n",
        "        return self.env.reset()\n",
        "\n",
        "    def close(self, action_history, reward_history):\n",
        "        \"\"\"\n",
        "        Properly close the game.\n",
        "        \"\"\"\n",
        "        self.env.close(action_history, reward_history)\n",
        "\n",
        "    def render(self, state):\n",
        "        \"\"\"\n",
        "        Display the game state and graph\n",
        "        \"\"\"\n",
        "        self.env.render()\n",
        "\n",
        "    def encode_board(self):\n",
        "        pass\n",
        "\n",
        "    def human_to_action(self):\n",
        "        \"\"\"\n",
        "        For multiplayer games, ask the user for a legal action\n",
        "        and return the corresponding action number.\n",
        "\n",
        "        Returns:\n",
        "            An integer from the action space.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def action_to_string(self, action_number):\n",
        "        return self.env.action_to_string(action_number)\n",
        "      \n",
        "    def state_to_string(self, state):\n",
        "      \"\"\"\n",
        "      Input:\n",
        "          state: current state\n",
        "      Returns:\n",
        "          index of state\n",
        "      \"\"\"\n",
        "      s = ''\n",
        "      for i in range(self.num_node):\n",
        "          s += str(int(state[i, 0]))\n",
        "      return s\n",
        "\n",
        "    def optimal(self):\n",
        "      return self.env.optimal()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP5DK92Iwqlt",
        "colab_type": "text"
      },
      "source": [
        "#Class TSP\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPGc-1hdv_9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import permutations\n",
        "\n",
        "class TSP:\n",
        "    def __init__(self, num_node):\n",
        "        self.graph = np.random.rand(num_node, 2)\n",
        "        self.player = 1\n",
        "        self.num_node = num_node\n",
        "        self.getInitState()\n",
        "      \n",
        "    def getInitState(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            first_state: a representation of the graph\n",
        "            left column representing visited nodes\n",
        "            right column will always have a single 1 and the rest are 0's. index with the 1 in the right column is current node\n",
        "        \"\"\"\n",
        "        # Always start with first node as current node \n",
        "        first_state = np.zeros([self.num_node, 2])\n",
        "        first_state[0,0] = 1\n",
        "        first_state[0,1] = 1\n",
        "        return first_state\n",
        "\n",
        "    def to_play(self):\n",
        "        return 0\n",
        "    \n",
        "    def action_to_string(self, action_number):\n",
        "\n",
        "        return action_number\n",
        "    \n",
        "    def close(self, action_history, reward_history):\n",
        "        \"\"\"\n",
        "        Properly close the game.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        self.graph = np.random.rand(self.num_node, 2)\n",
        "        self.player = 1\n",
        "        return self.getInitState()\n",
        "    \n",
        "    def step(self, action, state):\n",
        "      next_s = state.copy()\n",
        "      # zero out current node\n",
        "      next_s[:, 1] = 0\n",
        "      # 1 in left column for visited, 1 in right column for current node\n",
        "      next_s[action, :] = 1\n",
        "      prev_a = np.where(state[:, 1] == 1)[0][0]\n",
        "      # get xy coordinates for prev_node and current_node from the graph\n",
        "      prev_node = self.graph[prev_a]\n",
        "      current_node = self.graph[action]\n",
        "      reward = 1 - np.linalg.norm(current_node - prev_node)\n",
        "\n",
        "      return next_s, reward, self.is_finished(next_s)\n",
        "\n",
        "\n",
        "    def get_observation(self):      \n",
        "      pass\n",
        "\n",
        "    def legal_actions(self, state):\n",
        "      legal_actions = 1 - state[:, 0]\n",
        "      return numpy.where(legal_actions == 1)\n",
        "\n",
        "    def is_finished(self, state):\n",
        "      \"\"\"\n",
        "      Input:\n",
        "        state: current state\n",
        "      Returns:\n",
        "        r: 0 if game has not ended. 1 if it has\n",
        "               \n",
        "      \"\"\"\n",
        "      end = False\n",
        "      if self.num_node == np.sum(state[:, 0]):\n",
        "          end = True\n",
        "      return end\n",
        "\n",
        "    def render(self, state):\n",
        "      print(\"State:\")\n",
        "      print(state)\n",
        "      print(\"Graph:\")\n",
        "      print(self.graph)\n",
        "\n",
        "    def optimal(self):\n",
        "      seq = np.arange(self.num_node)[1:]\n",
        "      perm = permutations(seq)\n",
        "      graph = self.graph\n",
        "      reward = 0\n",
        "      optimal = 10000\n",
        "      action = []\n",
        "      for p in list(perm):\n",
        "          reward = 0\n",
        "          reward += np.linalg.norm(graph[0] - graph[p[0]])\n",
        "          reward += np.linalg.norm(graph[p[-1]] - graph[0])\n",
        "            \n",
        "          for k in range(len(p) - 1):\n",
        "              i = p[k]\n",
        "              j = p[k+1]\n",
        "              reward += np.linalg.norm(graph[j] - graph[i])\n",
        "            \n",
        "          if reward < optimal:\n",
        "              optimal = reward\n",
        "              action = p\n",
        "        \n",
        "      return optimal, action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nwh2nYlGwCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render(self, state):\n",
        "    print(\"State:\")\n",
        "    print(state)\n",
        "    print(\"Graph:\")\n",
        "    print(self.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps41Nx7sibia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwAvNpy2c_a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x20zK0m8ArH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLobip6u0ONr",
        "colab_type": "text"
      },
      "source": [
        "#Muzero Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao8Iwu6ndf7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class MuZeroNetwork:\n",
        "    def __new__(cls, config):\n",
        "        if config.network == \"fullyconnected\":\n",
        "            return MuZeroFullyConnectedNetwork(\n",
        "                config.observation_shape,\n",
        "                config.stacked_observations,\n",
        "                len(config.action_space),\n",
        "                config.encoding_size,\n",
        "                config.fc_reward_layers,\n",
        "                config.fc_value_layers,\n",
        "                config.fc_policy_layers,\n",
        "                config.fc_representation_layers,\n",
        "                config.fc_dynamics_layers,\n",
        "                config.support_size,\n",
        "            )\n",
        "        elif config.network == \"resnet\":\n",
        "            return MuZeroResidualNetwork(\n",
        "                config.observation_shape,\n",
        "                config.stacked_observations,\n",
        "                len(config.action_space),\n",
        "                config.blocks,\n",
        "                config.channels,\n",
        "                config.reduced_channels,\n",
        "                config.resnet_fc_reward_layers,\n",
        "                config.resnet_fc_value_layers,\n",
        "                config.resnet_fc_policy_layers,\n",
        "                config.support_size,\n",
        "                config.downsample,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'The network parameter should be \"fullyconnected\" or \"resnet\".'\n",
        "            )\n",
        "\n",
        "\n",
        "##################################\n",
        "######## Fully Connected #########\n",
        "\n",
        "\n",
        "class MuZeroFullyConnectedNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        stacked_observations,\n",
        "        action_space_size,\n",
        "        encoding_size,\n",
        "        fc_reward_layers,\n",
        "        fc_value_layers,\n",
        "        fc_policy_layers,\n",
        "        fc_representation_layers,\n",
        "        fc_dynamics_layers,\n",
        "        support_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.action_space_size = action_space_size\n",
        "        self.full_support_size = 2 * support_size + 1\n",
        "\n",
        "        self.representation_network = FullyConnectedNetwork(\n",
        "            observation_shape[0]\n",
        "            * observation_shape[1]\n",
        "            * observation_shape[2]\n",
        "            * (stacked_observations + 1)\n",
        "            + stacked_observations * observation_shape[1] * observation_shape[2],\n",
        "            fc_representation_layers,\n",
        "            encoding_size,\n",
        "        )\n",
        "\n",
        "        self.dynamics_encoded_state_network = FullyConnectedNetwork(\n",
        "            encoding_size + self.action_space_size, fc_dynamics_layers, encoding_size\n",
        "        )\n",
        "        self.dynamics_reward_network = FullyConnectedNetwork(\n",
        "            encoding_size + self.action_space_size,\n",
        "            fc_reward_layers,\n",
        "            self.full_support_size,\n",
        "        )\n",
        "\n",
        "        self.prediction_policy_network = FullyConnectedNetwork(\n",
        "            encoding_size, [], self.action_space_size\n",
        "        )\n",
        "        self.prediction_value_network = FullyConnectedNetwork(\n",
        "            encoding_size, fc_value_layers, self.full_support_size,\n",
        "        )\n",
        "\n",
        "    def prediction(self, encoded_state):\n",
        "        policy_logits = self.prediction_policy_network(encoded_state)\n",
        "        value = self.prediction_value_network(encoded_state)\n",
        "        return policy_logits, value\n",
        "\n",
        "    def representation(self, observation):\n",
        "        encoded_state = self.representation_network(\n",
        "            observation.view(observation.shape[0], -1)\n",
        "        )\n",
        "        # Scale encoded state between [0, 1] (See appendix paper Training)\n",
        "        min_encoded_state = encoded_state.min(1, keepdim=True)[0]\n",
        "        max_encoded_state = encoded_state.max(1, keepdim=True)[0]\n",
        "        scale_encoded_state = max_encoded_state - min_encoded_state\n",
        "        scale_encoded_state[scale_encoded_state == 0] = 1\n",
        "        encoded_state_normalized = (\n",
        "            encoded_state - min_encoded_state\n",
        "        ) / scale_encoded_state\n",
        "        return encoded_state_normalized\n",
        "\n",
        "    def dynamics(self, encoded_state, action):\n",
        "        # Stack encoded_state with a game specific one hot encoded action (See paper appendix Network Architecture)\n",
        "        action_one_hot = (\n",
        "            torch.zeros((action.shape[0], self.action_space_size))\n",
        "            .to(action.device)\n",
        "            .float()\n",
        "        )\n",
        "        action_one_hot.scatter_(1, action.long(), 1.0)\n",
        "        x = torch.cat((encoded_state, action_one_hot), dim=1)\n",
        "\n",
        "        next_encoded_state = self.dynamics_encoded_state_network(x)\n",
        "\n",
        "        # Scale encoded state between [0, 1] (See paper appendix Training)\n",
        "        min_next_encoded_state = next_encoded_state.min(1, keepdim=True)[0]\n",
        "        max_next_encoded_state = next_encoded_state.max(1, keepdim=True)[0]\n",
        "        scale_next_encoded_state = max_next_encoded_state - min_next_encoded_state\n",
        "        scale_next_encoded_state[scale_next_encoded_state == 0] = 1\n",
        "        next_encoded_state_normalized = (\n",
        "            next_encoded_state - min_next_encoded_state\n",
        "        ) / scale_next_encoded_state\n",
        "\n",
        "        reward = self.dynamics_reward_network(x)\n",
        "        return next_encoded_state_normalized, reward\n",
        "\n",
        "    def initial_inference(self, observation):\n",
        "        encoded_state = self.representation(observation)\n",
        "        policy_logits, value = self.prediction(encoded_state)\n",
        "        reward = (\n",
        "            torch.zeros(1, self.full_support_size)\n",
        "            .scatter(1, torch.tensor([[self.full_support_size // 2]]).long(), 1.0)\n",
        "            .repeat(len(observation), 1)\n",
        "            .to(observation.device)\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            value,\n",
        "            reward,\n",
        "            policy_logits,\n",
        "            encoded_state,\n",
        "        )\n",
        "\n",
        "    def recurrent_inference(self, encoded_state, action):\n",
        "        next_encoded_state, reward = self.dynamics(encoded_state, action)\n",
        "        policy_logits, value = self.prediction(next_encoded_state)\n",
        "        return value, reward, policy_logits, next_encoded_state\n",
        "\n",
        "    def get_weights(self):\n",
        "        return {key: value.cpu() for key, value in self.state_dict().items()}\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.load_state_dict(weights)\n",
        "\n",
        "\n",
        "###### End Fully Connected #######\n",
        "##################################\n",
        "\n",
        "\n",
        "##################################\n",
        "############# ResNet #############\n",
        "\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return torch.nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "# Residual block\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, num_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(num_channels, num_channels, stride)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(num_channels)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = conv3x3(num_channels, num_channels)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += x\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Downsample observations before representation network (See paper appendix Network Architecture)\n",
        "class DownSample(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels // 2,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.resblocks1 = torch.nn.ModuleList(\n",
        "            [ResidualBlock(out_channels // 2) for _ in range(2)]\n",
        "        )\n",
        "        self.conv2 = torch.nn.Conv2d(\n",
        "            out_channels // 2,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.resblocks2 = torch.nn.ModuleList(\n",
        "            [ResidualBlock(out_channels) for _ in range(3)]\n",
        "        )\n",
        "        self.pooling1 = torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.resblocks3 = torch.nn.ModuleList(\n",
        "            [ResidualBlock(out_channels) for _ in range(3)]\n",
        "        )\n",
        "        self.pooling2 = torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        for block in self.resblocks1:\n",
        "            out = block(out)\n",
        "        out = self.conv2(out)\n",
        "        for block in self.resblocks2:\n",
        "            out = block(out)\n",
        "        out = self.pooling1(out)\n",
        "        for block in self.resblocks3:\n",
        "            out = block(out)\n",
        "        out = self.pooling2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class RepresentationNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        stacked_observations,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        downsample,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_downsample = downsample\n",
        "        if self.use_downsample:\n",
        "            self.downsample = DownSample(\n",
        "                observation_shape[0] * (stacked_observations + 1)\n",
        "                + stacked_observations,\n",
        "                num_channels,\n",
        "            )\n",
        "        self.conv = conv3x3(\n",
        "            num_channels\n",
        "            if downsample\n",
        "            else observation_shape[0] * (stacked_observations + 1)\n",
        "            + stacked_observations,\n",
        "            num_channels,\n",
        "        )\n",
        "        self.bn = torch.nn.BatchNorm2d(num_channels)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.resblocks = torch.nn.ModuleList(\n",
        "            [ResidualBlock(num_channels) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_downsample:\n",
        "            out = self.downsample(x)\n",
        "        else:\n",
        "            out = x\n",
        "        out = self.conv(out)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        for block in self.resblocks:\n",
        "            out = block(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DynamicNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        reduced_channels,\n",
        "        fc_reward_layers,\n",
        "        full_support_size,\n",
        "        block_output_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.observation_shape = observation_shape\n",
        "        self.conv = conv3x3(num_channels, num_channels - 1)\n",
        "        self.bn = torch.nn.BatchNorm2d(num_channels - 1)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.resblocks = torch.nn.ModuleList(\n",
        "            [ResidualBlock(num_channels - 1) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = torch.nn.Conv2d(num_channels - 1, reduced_channels, 1)\n",
        "        self.block_output_size = block_output_size\n",
        "        self.fc = FullyConnectedNetwork(\n",
        "            self.block_output_size,\n",
        "            fc_reward_layers,\n",
        "            full_support_size,\n",
        "            activation=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        for block in self.resblocks:\n",
        "            out = block(out)\n",
        "        state = out\n",
        "        out = self.conv1x1(out)\n",
        "        out = out.view(-1, self.block_output_size)\n",
        "        reward = self.fc(out)\n",
        "        return state, reward\n",
        "\n",
        "\n",
        "class PredictionNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        action_space_size,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        reduced_channels,\n",
        "        fc_value_layers,\n",
        "        fc_policy_layers,\n",
        "        full_support_size,\n",
        "        block_output_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.observation_shape = observation_shape\n",
        "        self.resblocks = torch.nn.ModuleList(\n",
        "            [ResidualBlock(num_channels) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = torch.nn.Conv2d(num_channels, reduced_channels, 1)\n",
        "        self.block_output_size = block_output_size\n",
        "        self.fc_value = FullyConnectedNetwork(\n",
        "            self.block_output_size, fc_value_layers, full_support_size, activation=None,\n",
        "        )\n",
        "        self.fc_policy = FullyConnectedNetwork(\n",
        "            self.block_output_size,\n",
        "            fc_policy_layers,\n",
        "            action_space_size,\n",
        "            activation=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for block in self.resblocks:\n",
        "            out = block(out)\n",
        "        out = self.conv1x1(out)\n",
        "        out = out.view(-1, self.block_output_size)\n",
        "        value = self.fc_value(out)\n",
        "        policy = self.fc_policy(out)\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "class MuZeroResidualNetwork(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        stacked_observations,\n",
        "        action_space_size,\n",
        "        num_blocks,\n",
        "        num_channels,\n",
        "        reduced_channels,\n",
        "        fc_reward_layers,\n",
        "        fc_value_layers,\n",
        "        fc_policy_layers,\n",
        "        support_size,\n",
        "        downsample,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.action_space_size = action_space_size\n",
        "        self.full_support_size = 2 * support_size + 1\n",
        "        block_output_size = (\n",
        "            (\n",
        "                reduced_channels\n",
        "                * (observation_shape[1] // 16)\n",
        "                * (observation_shape[2] // 16)\n",
        "            )\n",
        "            if downsample\n",
        "            else (reduced_channels * observation_shape[1] * observation_shape[2])\n",
        "        )\n",
        "\n",
        "        self.representation_network = RepresentationNetwork(\n",
        "            observation_shape,\n",
        "            stacked_observations,\n",
        "            num_blocks,\n",
        "            num_channels,\n",
        "            downsample,\n",
        "        )\n",
        "\n",
        "        self.dynamics_network = DynamicNetwork(\n",
        "            observation_shape,\n",
        "            num_blocks,\n",
        "            num_channels + 1,\n",
        "            reduced_channels,\n",
        "            fc_reward_layers,\n",
        "            self.full_support_size,\n",
        "            block_output_size,\n",
        "        )\n",
        "\n",
        "        self.prediction_network = PredictionNetwork(\n",
        "            observation_shape,\n",
        "            action_space_size,\n",
        "            num_blocks,\n",
        "            num_channels,\n",
        "            reduced_channels,\n",
        "            fc_value_layers,\n",
        "            fc_policy_layers,\n",
        "            self.full_support_size,\n",
        "            block_output_size,\n",
        "        )\n",
        "\n",
        "    def prediction(self, encoded_state):\n",
        "        policy, value = self.prediction_network(encoded_state)\n",
        "        return policy, value\n",
        "\n",
        "    def representation(self, observation):\n",
        "        encoded_state = self.representation_network(observation)\n",
        "\n",
        "        # Scale encoded state between [0, 1] (See appendix paper Training)\n",
        "        min_encoded_state = (\n",
        "            encoded_state.view(\n",
        "                -1,\n",
        "                encoded_state.shape[1],\n",
        "                encoded_state.shape[2] * encoded_state.shape[3],\n",
        "            )\n",
        "            .min(2, keepdim=True)[0]\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        max_encoded_state = (\n",
        "            encoded_state.view(\n",
        "                -1,\n",
        "                encoded_state.shape[1],\n",
        "                encoded_state.shape[2] * encoded_state.shape[3],\n",
        "            )\n",
        "            .max(2, keepdim=True)[0]\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        scale_encoded_state = max_encoded_state - min_encoded_state\n",
        "        scale_encoded_state[scale_encoded_state == 0] = 1\n",
        "        encoded_state_normalized = (\n",
        "            encoded_state - min_encoded_state\n",
        "        ) / scale_encoded_state\n",
        "        return encoded_state_normalized\n",
        "\n",
        "    def dynamics(self, encoded_state, action):\n",
        "        # Stack encoded_state with a game specific one hot encoded action (See paper appendix Network Architecture)\n",
        "        action_one_hot = (\n",
        "            torch.ones(\n",
        "                (\n",
        "                    encoded_state.shape[0],\n",
        "                    1,\n",
        "                    encoded_state.shape[2],\n",
        "                    encoded_state.shape[3],\n",
        "                )\n",
        "            )\n",
        "            .to(action.device)\n",
        "            .float()\n",
        "        )\n",
        "        action_one_hot = (\n",
        "            action[:, :, None, None] * action_one_hot / self.action_space_size\n",
        "        )\n",
        "        x = torch.cat((encoded_state, action_one_hot), dim=1)\n",
        "        next_encoded_state, reward = self.dynamics_network(x)\n",
        "\n",
        "        # Scale encoded state between [0, 1] (See paper appendix Training)\n",
        "        min_next_encoded_state = (\n",
        "            next_encoded_state.view(\n",
        "                -1,\n",
        "                next_encoded_state.shape[1],\n",
        "                next_encoded_state.shape[2] * next_encoded_state.shape[3],\n",
        "            )\n",
        "            .min(2, keepdim=True)[0]\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        max_next_encoded_state = (\n",
        "            next_encoded_state.view(\n",
        "                -1,\n",
        "                next_encoded_state.shape[1],\n",
        "                next_encoded_state.shape[2] * next_encoded_state.shape[3],\n",
        "            )\n",
        "            .max(2, keepdim=True)[0]\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        scale_next_encoded_state = max_next_encoded_state - min_next_encoded_state\n",
        "        scale_next_encoded_state[scale_next_encoded_state == 0] = 1\n",
        "        next_encoded_state_normalized = (\n",
        "            next_encoded_state - min_next_encoded_state\n",
        "        ) / scale_next_encoded_state\n",
        "        return next_encoded_state_normalized, reward\n",
        "\n",
        "    def initial_inference(self, observation):\n",
        "        encoded_state = self.representation(observation)\n",
        "        policy_logits, value = self.prediction(encoded_state)\n",
        "        reward = (\n",
        "            torch.zeros(1, self.full_support_size)\n",
        "            .scatter(1, torch.tensor([[self.full_support_size // 2]]).long(), 1.0)\n",
        "            .repeat(len(observation), 1)\n",
        "            .to(observation.device)\n",
        "        )\n",
        "        return (\n",
        "            value,\n",
        "            reward,\n",
        "            policy_logits,\n",
        "            encoded_state,\n",
        "        )\n",
        "\n",
        "    def recurrent_inference(self, encoded_state, action):\n",
        "        next_encoded_state, reward = self.dynamics(encoded_state, action)\n",
        "        policy_logits, value = self.prediction(next_encoded_state)\n",
        "        return value, reward, policy_logits, next_encoded_state\n",
        "\n",
        "    def get_weights(self):\n",
        "        return {key: value.cpu() for key, value in self.state_dict().items()}\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.load_state_dict(weights)\n",
        "\n",
        "\n",
        "########### End ResNet ###########\n",
        "##################################\n",
        "\n",
        "\n",
        "class FullyConnectedNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_size, layer_sizes, output_size, activation=None):\n",
        "        super().__init__()\n",
        "        size_list = [input_size] + layer_sizes\n",
        "        layers = []\n",
        "        if 1 < len(size_list):\n",
        "            for i in range(len(size_list) - 1):\n",
        "                layers.extend(\n",
        "                    [\n",
        "                        torch.nn.Linear(size_list[i], size_list[i + 1]),\n",
        "                        torch.nn.LeakyReLU(),\n",
        "                    ]\n",
        "                )\n",
        "        layers.append(torch.nn.Linear(size_list[-1], output_size))\n",
        "        if activation:\n",
        "            layers.append(activation)\n",
        "        self.layers = torch.nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def support_to_scalar(logits, support_size):\n",
        "    \"\"\"\n",
        "    Transform a categorical representation to a scalar\n",
        "    See paper appendix Network Architecture\n",
        "    \"\"\"\n",
        "    # Decode to a scalar\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    support = (\n",
        "        torch.tensor([x for x in range(-support_size, support_size + 1)])\n",
        "        .expand(probabilities.shape)\n",
        "        .float()\n",
        "        .to(device=probabilities.device)\n",
        "    )\n",
        "    x = torch.sum(support * probabilities, dim=1, keepdim=True)\n",
        "\n",
        "    # Invert the scaling (defined in https://arxiv.org/abs/1805.11593)\n",
        "    x = torch.sign(x) * (\n",
        "        ((torch.sqrt(1 + 4 * 0.001 * (torch.abs(x) + 1 + 0.001)) - 1) / (2 * 0.001))\n",
        "        ** 2\n",
        "        - 1\n",
        "    )\n",
        "    return x\n",
        "\n",
        "\n",
        "def scalar_to_support(x, support_size):\n",
        "    \"\"\"\n",
        "    Transform a scalar to a categorical representation with (2 * support_size + 1) categories\n",
        "    See paper appendix Network Architecture\n",
        "    \"\"\"\n",
        "    # Reduce the scale (defined in https://arxiv.org/abs/1805.11593)\n",
        "    x = torch.sign(x) * (torch.sqrt(torch.abs(x) + 1) - 1) + 0.001 * x\n",
        "\n",
        "    # Encode on a vector\n",
        "    x = torch.clamp(x, -support_size, support_size)\n",
        "    floor = x.floor()\n",
        "    prob = x - floor\n",
        "    logits = torch.zeros(x.shape[0], x.shape[1], 2 * support_size + 1).to(x.device)\n",
        "    logits.scatter_(\n",
        "        2, (floor + support_size).long().unsqueeze(-1), (1 - prob).unsqueeze(-1)\n",
        "    )\n",
        "    indexes = floor + support_size + 1\n",
        "    prob = prob.masked_fill_(2 * support_size < indexes, 0.0)\n",
        "    indexes = indexes.masked_fill_(2 * support_size < indexes, 0.0)\n",
        "    logits.scatter_(2, indexes.long().unsqueeze(-1), prob.unsqueeze(-1))\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr_8HlWCdfIT",
        "colab_type": "text"
      },
      "source": [
        "#Shared Storage Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYx4UphveR4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ray\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class SharedStorage:\n",
        "    \"\"\"\n",
        "    Class which run in a dedicated thread to store the network weights and some information.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights, game_name, config):\n",
        "        self.config = config\n",
        "        self.game_name = game_name\n",
        "        self.weights = weights\n",
        "        self.infos = {\n",
        "            \"total_reward\": 0,\n",
        "            \"player_0_reward\": 0,\n",
        "            \"player_1_reward\": 0,\n",
        "            \"episode_length\": 0,\n",
        "            \"training_step\": 0,\n",
        "            \"lr\": 0,\n",
        "            \"total_loss\": 0,\n",
        "            \"value_loss\": 0,\n",
        "            \"reward_loss\": 0,\n",
        "            \"policy_loss\": 0,\n",
        "        }\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def set_weights(self, weights, path=None):\n",
        "        self.weights = weights\n",
        "        if not path:\n",
        "            path = os.path.join(self.config.results_path, \"model.weights\")\n",
        "        # path = \"/content/drive/My Drive/deep-learning-final/test_weights/model.weights\"\n",
        "        torch.save(self.weights, path)\n",
        "\n",
        "    def get_infos(self):\n",
        "        return self.infos\n",
        "\n",
        "    def set_infos(self, key, value):\n",
        "        self.infos[key] = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHUz05Hay-fn",
        "colab_type": "text"
      },
      "source": [
        "#Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCr-dWrjbtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "import numpy\n",
        "import ray\n",
        "import torch\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Class which run in a dedicated thread to store played games and generate batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.buffer = []\n",
        "        self.game_priorities = []\n",
        "        self.max_recorded_game_priority = 1.0\n",
        "        self.self_play_count = 0\n",
        "\n",
        "        self.model = MuZeroNetwork(self.config)\n",
        "\n",
        "    def save_game(self, game_history):\n",
        "        if len(self.buffer) > self.config.window_size:\n",
        "            self.buffer.pop(0)\n",
        "            self.game_priorities.pop(0)\n",
        "\n",
        "        if self.config.use_max_priority:\n",
        "            game_history.priorities = (\n",
        "                numpy.ones(len(game_history.root_values))\n",
        "                * self.max_recorded_game_priority\n",
        "            )\n",
        "        self.buffer.append(game_history)\n",
        "        self.game_priorities.append(numpy.mean(game_history.priorities))\n",
        "        self.self_play_count += 1\n",
        "\n",
        "    def get_self_play_count(self):\n",
        "        return self.self_play_count\n",
        "\n",
        "    def get_batch(self, model_weights):\n",
        "        (\n",
        "            index_batch,\n",
        "            observation_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            value_batch,\n",
        "            policy_batch,\n",
        "            weight_batch,\n",
        "            gradient_scale_batch,\n",
        "        ) = ([], [], [], [], [], [], [], [])\n",
        "\n",
        "        total_samples = sum(\n",
        "            (len(game_history.priorities) for game_history in self.buffer)\n",
        "        )\n",
        "\n",
        "        if self.config.use_last_model_value:\n",
        "            self.model.set_weights(model_weights)\n",
        "\n",
        "        for _ in range(self.config.batch_size):\n",
        "            game_index, game_history, game_prob = self.sample_game(self.buffer)\n",
        "            game_pos, pos_prob = self.sample_position(game_history)\n",
        "\n",
        "            values, rewards, policies, actions = self.make_target(\n",
        "                game_history, game_pos\n",
        "            )\n",
        "\n",
        "            index_batch.append([game_index, game_pos])\n",
        "            observation_batch.append(game_history.get_stacked_observations(game_pos, self.config.stacked_observations))\n",
        "            action_batch.append(actions)\n",
        "            value_batch.append(values)\n",
        "            reward_batch.append(rewards)\n",
        "            policy_batch.append(policies)\n",
        "            weight_batch.append(\n",
        "                (total_samples * game_prob * pos_prob) ** (-self.config.PER_beta)\n",
        "            )\n",
        "            gradient_scale_batch.append(\n",
        "                [\n",
        "                    min(\n",
        "                        self.config.num_unroll_steps,\n",
        "                        len(game_history.action_history) - game_pos,\n",
        "                    )\n",
        "                ]\n",
        "                * len(actions)\n",
        "            )\n",
        "\n",
        "        weight_batch = numpy.array(weight_batch) / max(weight_batch)\n",
        "\n",
        "        # observation_batch: batch, channels, height, width\n",
        "        # action_batch: batch, num_unroll_steps+1\n",
        "        # value_batch: batch, num_unroll_steps+1\n",
        "        # reward_batch: batch, num_unroll_steps+1\n",
        "        # policy_batch: batch, num_unroll_steps+1, len(action_space)\n",
        "        # weight_batch: batch\n",
        "        # gradient_scale_batch: batch, num_unroll_steps+1\n",
        "        return (\n",
        "            index_batch,\n",
        "            (\n",
        "                observation_batch,\n",
        "                action_batch,\n",
        "                value_batch,\n",
        "                reward_batch,\n",
        "                policy_batch,\n",
        "                weight_batch,\n",
        "                gradient_scale_batch,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def sample_game(self, buffer):\n",
        "        \"\"\"\n",
        "        Sample game from buffer either uniformly or according to some priority.\n",
        "        See paper appendix Training.\n",
        "        \"\"\"\n",
        "        game_probs = numpy.array(self.game_priorities) / sum(self.game_priorities)\n",
        "        game_index_candidates = numpy.arange(0, len(self.buffer), dtype=int)\n",
        "        game_index = numpy.random.choice(game_index_candidates, p=game_probs)\n",
        "        game_prob = game_probs[game_index]\n",
        "\n",
        "        return game_index, self.buffer[game_index], game_prob\n",
        "\n",
        "    def sample_position(self, game_history):\n",
        "        \"\"\"\n",
        "        Sample position from game either uniformly or according to some priority.\n",
        "        See paper appendix Training.\n",
        "        \"\"\"\n",
        "        position_probs = numpy.array(game_history.priorities) / sum(\n",
        "            game_history.priorities\n",
        "        )\n",
        "        position_index_candidates = numpy.arange(0, len(position_probs), dtype=int)\n",
        "        position_index = numpy.random.choice(\n",
        "            position_index_candidates, p=position_probs\n",
        "        )\n",
        "        position_prob = position_probs[position_index]\n",
        "\n",
        "        return position_index, position_prob\n",
        "\n",
        "    def update_priorities(self, priorities, index_info):\n",
        "        \"\"\"\n",
        "        Update game and position priorities with priorities calculated during the training.\n",
        "        See Distributed Prioritized Experience Replay https://arxiv.org/abs/1803.00933\n",
        "        \"\"\"\n",
        "        for i in range(len(index_info)):\n",
        "            game_index, game_pos = index_info[i]\n",
        "\n",
        "            # update position priorities\n",
        "            priority = priorities[i, :]\n",
        "            start_index = game_pos\n",
        "            end_index = min(\n",
        "                game_pos + len(priority), len(self.buffer[game_index].priorities)\n",
        "            )\n",
        "            self.buffer[game_index].priorities[start_index:end_index] = priority[\n",
        "                : end_index - start_index\n",
        "            ]\n",
        "\n",
        "            # update game priorities\n",
        "            self.game_priorities[game_index] = numpy.max(\n",
        "                self.buffer[game_index].priorities\n",
        "            )  # option: mean, sum, max\n",
        "\n",
        "            self.max_recorded_game_priority = numpy.max(self.game_priorities)\n",
        "\n",
        "    def make_target(self, game_history, state_index):\n",
        "        \"\"\"\n",
        "        Generate targets for every unroll steps.\n",
        "        \"\"\"\n",
        "        dn = DynamicNetwork(self.config.observation_shape, self.config.blocks, \n",
        "                            self.config.channels, self.config.reduced_channels, \n",
        "                            self.config.resnet_fc_reward_layers, self.config.support_size, \n",
        "                            self.config.blocks)\n",
        "        target_values, target_rewards, target_policies, actions = [], [], [], []\n",
        "        for current_index in range(\n",
        "            state_index, state_index + self.config.num_unroll_steps + 1\n",
        "        ):\n",
        "            # The value target is the discounted root value of the search tree td_steps into the\n",
        "            # future, plus the discounted sum of all rewards until then.\n",
        "            bootstrap_index = current_index + self.config.td_steps\n",
        "            if bootstrap_index < len(game_history.root_values):\n",
        "                if self.config.use_last_model_value:\n",
        "                    # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n",
        "                    observation = torch.tensor(\n",
        "                        game_history.get_stacked_observations(bootstrap_index, self.config.stacked_observations)\n",
        "                    ).float()\n",
        "                    last_step_value = support_to_scalar(\n",
        "                        self.model.initial_inference(observation)[0],\n",
        "                        self.config.support_size,\n",
        "                    ).item()\n",
        "                else:\n",
        "                    last_step_value = game_history.root_values[bootstrap_index]\n",
        "\n",
        "                value = last_step_value * self.config.discount ** self.config.td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(\n",
        "                game_history.reward_history[current_index + 1 : bootstrap_index + 1]\n",
        "            ):\n",
        "                value += (\n",
        "                    reward\n",
        "                    if game_history.to_play_history[current_index]\n",
        "                    == game_history.to_play_history[current_index + 1 + i]\n",
        "                    else -reward\n",
        "                ) * self.config.discount ** i\n",
        "\n",
        "            if current_index < len(game_history.root_values):\n",
        "                target_values.append(value)\n",
        "                target_rewards.append(game_history.reward_history[current_index])\n",
        "                target_policies.append(game_history.child_visits[current_index])\n",
        "                actions.append(game_history.action_history[current_index])\n",
        "            elif current_index == len(game_history.root_values):\n",
        "                target_values.append(0)\n",
        "                target_rewards.append(game_history.reward_history[current_index])\n",
        "                # Uniform policy\n",
        "                target_policies.append(\n",
        "                    [\n",
        "                        1 / len(game_history.child_visits[0])\n",
        "                        for _ in range(len(game_history.child_visits[0]))\n",
        "                    ]\n",
        "                )\n",
        "                actions.append(game_history.action_history[current_index])\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states\n",
        "                target_values.append(0)\n",
        "                target_rewards.append(0)\n",
        "                # Uniform policy\n",
        "                target_policies.append(\n",
        "                    [\n",
        "                        1 / len(game_history.child_visits[0])\n",
        "                        for _ in range(len(game_history.child_visits[0]))\n",
        "                    ]\n",
        "                )\n",
        "                actions.append(numpy.random.choice(game_history.action_history))\n",
        "\n",
        "        return target_values, target_rewards, target_policies, actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xX8-gyrzDH3",
        "colab_type": "text"
      },
      "source": [
        "# Self Play\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WtQWh5-fF_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import time\n",
        "\n",
        "import numpy\n",
        "import ray\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "@ray.remote\n",
        "class SelfPlay:\n",
        "    \"\"\"\n",
        "    Class which run in a dedicated thread to play games and save them to the replay-buffer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initial_weights, game, config):\n",
        "        self.config = config\n",
        "        self.game = game\n",
        "\n",
        "        # Initialize the network\n",
        "        self.model = MuZeroNetwork(self.config)\n",
        "        self.model.set_weights(initial_weights)\n",
        "        self.model.to(torch.device(\"cpu\"))\n",
        "        self.model.eval()\n",
        "\n",
        "    def continuous_self_play(self, shared_storage, replay_buffer, test_mode=False):\n",
        "        ratio_list = []\n",
        "        with open ('/content/drive/My Drive/deep-learning-final/csv_folder/test_100k.csv','wt') as csvfile:\n",
        "          writer = csv.writer(csvfile, delimiter =\"\\t\" )\n",
        "          while True:\n",
        "              self.model.set_weights(\n",
        "                  copy.deepcopy(ray.get(shared_storage.get_weights.remote()))\n",
        "              )\n",
        "\n",
        "              # Take the best action (no exploration) in test mode\n",
        "              temperature = (\n",
        "                  0\n",
        "                  if test_mode\n",
        "                  else self.config.visit_softmax_temperature_fn(\n",
        "                      trained_steps=ray.get(shared_storage.get_infos.remote())[\n",
        "                          \"training_step\"\n",
        "                      ]\n",
        "                  )\n",
        "              )\n",
        "              game_history = self.play_game(\n",
        "                  temperature,\n",
        "                  self.config.temperature_threshold,\n",
        "                  False,\n",
        "                  \"self\",\n",
        "                  0,\n",
        "              )\n",
        "              ratio = sum(game_history.reward_history)/self.game.optimal()[0]\n",
        "              if ratio > 1:\n",
        "                writer.writerow([ratio])\n",
        "\n",
        "              # Save to the shared storage\n",
        "              if test_mode:\n",
        "                  shared_storage.set_infos.remote(\n",
        "                      \"total_reward\", sum(game_history.reward_history)\n",
        "                  )\n",
        "                  shared_storage.set_infos.remote(\n",
        "                      \"episode_length\", len(game_history.action_history)\n",
        "                  )\n",
        "                  if 1 < len(self.config.players):\n",
        "                      shared_storage.set_infos.remote(\n",
        "                          \"player_0_reward\",\n",
        "                          sum(\n",
        "                              [\n",
        "                                  reward\n",
        "                                  for i, reward in enumerate(game_history.reward_history)\n",
        "                                  if game_history.to_play_history[i] == 1\n",
        "                              ]\n",
        "                          ),\n",
        "                      )\n",
        "                      shared_storage.set_infos.remote(\n",
        "                          \"player_1_reward\",\n",
        "                          sum(\n",
        "                              [\n",
        "                                  reward\n",
        "                                  for i, reward in enumerate(game_history.reward_history)\n",
        "                                  if game_history.to_play_history[i] == 0\n",
        "                              ]\n",
        "                          ),\n",
        "                      )\n",
        "              if not test_mode:\n",
        "                  replay_buffer.save_game.remote(game_history)\n",
        "\n",
        "              # Managing the self-play / training ratio\n",
        "              if not test_mode and self.config.self_play_delay:\n",
        "                  time.sleep(self.config.self_play_delay)\n",
        "              if not test_mode and self.config.ratio:\n",
        "                  while (\n",
        "                      ray.get(replay_buffer.get_self_play_count.remote())\n",
        "                      / max(\n",
        "                          1, ray.get(shared_storage.get_infos.remote())[\"training_step\"]\n",
        "                      )\n",
        "                      > self.config.ratio\n",
        "                  ):\n",
        "                      time.sleep(0.5)\n",
        "\n",
        "    def play_game(\n",
        "        self, temperature, temperature_threshold, render, opponent, muzero_player\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Play one game with actions based on the Monte Carlo tree search at each moves.\n",
        "        \"\"\"\n",
        "        game_history = GameHistory()\n",
        "        observation = self.game.reset()\n",
        "        game_history.action_history.append(0)\n",
        "        game_history.observation_history.append(observation)\n",
        "        game_history.reward_history.append(0)\n",
        "        game_history.to_play_history.append(self.game.to_play())\n",
        "\n",
        "        done = False\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        if render:\n",
        "            self.game.render(observation)\n",
        "\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            i = 0\n",
        "            while not done:\n",
        "                stacked_observations = game_history.get_stacked_observations(\n",
        "                    -1, self.config.stacked_observations,\n",
        "                )\n",
        "\n",
        "                root, priority, tree_depth = MCTS(self.config).run(\n",
        "                    self.model,\n",
        "                    stacked_observations,\n",
        "                    self.game.legal_actions(observation),\n",
        "                    self.game.to_play(),\n",
        "                    False if temperature == 0 else True,\n",
        "                )\n",
        "\n",
        "                if render:\n",
        "                    print(\"Tree depth: {}\".format(tree_depth))\n",
        "                    print(\n",
        "                        \"Root value for player {0}: {1:.2f}\".format(\n",
        "                            self.game.to_play(), root.value()\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # Choose the action\n",
        "                if opponent == \"self\" or muzero_player == self.game.to_play():\n",
        "                    action = self.select_action(\n",
        "                        root,\n",
        "                        temperature\n",
        "                        if not temperature_threshold\n",
        "                        or len(game_history.action_history) < temperature_threshold\n",
        "                        else 0,\n",
        "                    )\n",
        "                observation, reward, done = self.game.step(action, observation)\n",
        "                count = count + 1\n",
        "\n",
        "                if render:\n",
        "                    print(\n",
        "                        \"Played action: {}\".format(self.game.action_to_string(action))\n",
        "                    )\n",
        "                    self.game.render(observation)\n",
        "\n",
        "                game_history.store_search_statistics(root, self.config.action_space)\n",
        "                if not self.config.use_max_priority:\n",
        "                    game_history.priorities.append(priority)\n",
        "\n",
        "                # Next batch\n",
        "                game_history.action_history.append(action)\n",
        "                game_history.observation_history.append(observation)\n",
        "                game_history.reward_history.append(reward)\n",
        "                game_history.to_play_history.append(self.game.to_play())\n",
        "\n",
        "                # Use for testing -->\n",
        "\n",
        "                # if len(game_history.observation_history) == 9:\n",
        "                #   action = self.game.legal_actions(observation)[0][0]\n",
        "                #   game_history.action_history.append(action)\n",
        "                #   game_history.observation_history.append(np.ones([10, 2]))\n",
        "                #   prev_node = self.game.graph[game_history.action_history[-2]]\n",
        "                #   current_node = self.game.graph[action]\n",
        "                #   reward = 1 - np.linalg.norm(current_node - prev_node)\n",
        "                #   print(\"prev_node\",prev_node)\n",
        "                #   print(\"cur_node\",current_node)\n",
        "                #   print(\"reward\",reward)\n",
        "                #   game_history.reward_history.append(reward)\n",
        "                #   print(\"in play_game actions\", game_history.action_history)                  \n",
        "                  \n",
        "\n",
        "        # print(\"in play_game actions\", game_history.action_history)\n",
        "        self.game.close(game_history.action_history, game_history.reward_history)\n",
        "        return game_history\n",
        "\n",
        "    @staticmethod\n",
        "    def select_action(node, temperature):\n",
        "        \"\"\"\n",
        "        Select action according to the visit count distribution and the temperature.\n",
        "        The temperature is changed dynamically with the visit_softmax_temperature function \n",
        "        in the config.\n",
        "        \"\"\"\n",
        "        visit_counts = numpy.array(\n",
        "            [child.visit_count for child in node.children.values()]\n",
        "        )\n",
        "        actions = [action for action in node.children.keys()]\n",
        "        if temperature == 0:\n",
        "            action = actions[numpy.argmax(visit_counts)]\n",
        "        elif temperature == float(\"inf\"):\n",
        "            action = numpy.random.choice(actions)\n",
        "        else:\n",
        "            # See paper appendix Data Generation\n",
        "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
        "            visit_count_distribution = visit_count_distribution / sum(\n",
        "                visit_count_distribution\n",
        "            )\n",
        "            action = numpy.random.choice(actions, p=visit_count_distribution)\n",
        "        return action\n",
        "\n",
        "\n",
        "# Game independent\n",
        "class MCTS:\n",
        "    \"\"\"\n",
        "    Core Monte Carlo Tree Search algorithm.\n",
        "    To decide on an action, we run N simulations, always starting at the root of\n",
        "    the search tree and traversing the tree according to the UCB formula until we\n",
        "    reach a leaf node.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def run(self, model, observation, legal_actions, to_play, add_exploration_noise):\n",
        "        \"\"\"\n",
        "        At the root of the search tree we use the representation function to obtain a\n",
        "        hidden state given the current observation.\n",
        "        We then run a Monte Carlo Tree Search using only action sequences and the model\n",
        "        learned by the network.\n",
        "        \"\"\"\n",
        "        root = Node(0)\n",
        "        dn = DynamicNetwork(self.config.observation_shape, self.config.blocks, \n",
        "                            self.config.channels, self.config.reduced_channels, \n",
        "                            self.config.resnet_fc_reward_layers, self.config.support_size, \n",
        "                            self.config.blocks)\n",
        "        observation = (\n",
        "            torch.tensor(observation)\n",
        "            .float()\n",
        "            .unsqueeze(0)\n",
        "            .unsqueeze(0)\n",
        "            .to(next(model.parameters()).device)\n",
        "        )\n",
        "        (\n",
        "            root_predicted_value,\n",
        "            reward,\n",
        "            policy_logits,\n",
        "            hidden_state,\n",
        "        ) = model.initial_inference(observation)\n",
        "        root_predicted_value = support_to_scalar(\n",
        "            root_predicted_value, self.config.support_size\n",
        "        ).item()\n",
        "        reward = support_to_scalar(reward, self.config.support_size).item()\n",
        "        \n",
        "        root.expand(\n",
        "            legal_actions[0], to_play, reward, policy_logits, hidden_state,\n",
        "        )\n",
        "        if add_exploration_noise:\n",
        "            root.add_exploration_noise(\n",
        "                dirichlet_alpha=self.config.root_dirichlet_alpha,\n",
        "                exploration_fraction=self.config.root_exploration_fraction,\n",
        "            )\n",
        "\n",
        "        min_max_stats = MinMaxStats()\n",
        "\n",
        "        max_tree_depth = 0\n",
        "        for _ in range(self.config.num_simulations):\n",
        "            virtual_to_play = to_play\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "            current_tree_depth = 0\n",
        "\n",
        "            while node.expanded():\n",
        "                current_tree_depth += 1\n",
        "                action, node = self.select_child(node, min_max_stats)\n",
        "                search_path.append(node)\n",
        "\n",
        "                # Players play turn by turn\n",
        "                if virtual_to_play + 1 < len(self.config.players):\n",
        "                    virtual_to_play = self.config.players[virtual_to_play + 1]\n",
        "                else:\n",
        "                    virtual_to_play = self.config.players[0]\n",
        "\n",
        "            # Inside the search tree we use the dynamics function to obtain the next hidden\n",
        "            # state given an action and the previous hidden state\n",
        "            parent = search_path[-2]\n",
        "            value, reward, policy_logits, hidden_state = model.recurrent_inference(\n",
        "                parent.hidden_state,\n",
        "                torch.tensor([[action]]).to(parent.hidden_state.device),\n",
        "            )\n",
        "            value = support_to_scalar(value, self.config.support_size).item()\n",
        "            reward = support_to_scalar(reward, self.config.support_size).item()\n",
        "            node.expand(\n",
        "                self.config.action_space,\n",
        "                virtual_to_play,\n",
        "                reward,\n",
        "                policy_logits,\n",
        "                hidden_state,\n",
        "            )\n",
        "\n",
        "            self.backpropagate(search_path, value, virtual_to_play, min_max_stats)\n",
        "\n",
        "            max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
        "\n",
        "        priority = (\n",
        "            None\n",
        "            if self.config.use_max_priority\n",
        "            else numpy.abs(root_predicted_value - root.value()) ** self.config.PER_alpha\n",
        "        )\n",
        "\n",
        "        return root, priority, max_tree_depth\n",
        "\n",
        "    def select_child(self, node, min_max_stats):\n",
        "        \"\"\"\n",
        "        Select the child with the highest UCB score.\n",
        "        \"\"\"\n",
        "        _, action, child = max(\n",
        "            (self.ucb_score(node, child, min_max_stats), action, child)\n",
        "            for action, child in node.children.items()\n",
        "        )\n",
        "        return action, child\n",
        "\n",
        "    def ucb_score(self, parent, child, min_max_stats):\n",
        "        \"\"\"\n",
        "        The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
        "        \"\"\"\n",
        "        pb_c = (\n",
        "            math.log(\n",
        "                (parent.visit_count + self.config.pb_c_base + 1) / self.config.pb_c_base\n",
        "            )\n",
        "            + self.config.pb_c_init\n",
        "        )\n",
        "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "        prior_score = pb_c * child.prior\n",
        "\n",
        "        if child.visit_count > 0:\n",
        "            # mean value Q\n",
        "            value_score = min_max_stats.normalize(\n",
        "                child.reward + self.config.discount * child.value()\n",
        "            )\n",
        "        else:\n",
        "            value_score = 0\n",
        "\n",
        "        return prior_score + value_score\n",
        "\n",
        "    def backpropagate(self, search_path, value, to_play, min_max_stats):\n",
        "        \"\"\"\n",
        "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
        "        to the root.\n",
        "        \"\"\"\n",
        "        for node in reversed(search_path):\n",
        "            node.value_sum += value if node.to_play == to_play else -value\n",
        "            node.visit_count += 1\n",
        "            min_max_stats.update(node.reward + self.config.discount * node.value())\n",
        "\n",
        "            value = node.reward + self.config.discount * value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, prior):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "    def expand(self, actions, to_play, reward, policy_logits, hidden_state):\n",
        "        \"\"\"\n",
        "        We expand a node using the value, reward and policy prediction obtained from the\n",
        "        neural network.\n",
        "        \"\"\"\n",
        "        self.to_play = to_play\n",
        "        self.reward = reward\n",
        "        self.hidden_state = hidden_state\n",
        "        policy = {}\n",
        "        for a in numpy.array(actions):\n",
        "          try:\n",
        "            policy[a] = 1 / sum(torch.exp(policy_logits[0] - policy_logits[0][a]))\n",
        "          except OverflowError:\n",
        "            print(\"Warning: prior has been approximated\")\n",
        "            policy[a] = 0.0\n",
        "        for action, p in policy.items():\n",
        "            self.children[action] = Node(p)\n",
        "\n",
        "    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n",
        "        \"\"\"\n",
        "        At the start of each search, we add dirichlet noise to the prior of the root to\n",
        "        encourage the search to explore new actions.\n",
        "        \"\"\"\n",
        "        actions = list(self.children.keys())\n",
        "        noise = numpy.random.dirichlet([dirichlet_alpha] * len(actions))\n",
        "        frac = exploration_fraction\n",
        "        for a, n in zip(actions, noise):\n",
        "            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "class GameHistory:\n",
        "    \"\"\"\n",
        "    Store only usefull information of a self-play game.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.observation_history = []\n",
        "        self.action_history = []\n",
        "        self.reward_history = []\n",
        "        self.to_play_history = []\n",
        "        self.child_visits = []\n",
        "        self.root_values = []\n",
        "        self.priorities = []\n",
        "\n",
        "    def store_search_statistics(self, root, action_space):\n",
        "        # Turn visit count from root into a policy\n",
        "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "        self.child_visits.append(\n",
        "            [\n",
        "                root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "                for a in action_space\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.root_values.append(root.value())\n",
        "\n",
        "    def get_stacked_observations(self, index, num_stacked_observations):\n",
        "        \"\"\"\n",
        "        Generate a new observation with the observation at the index position\n",
        "        and num_stacked_observations past observations and actions stacked.\n",
        "        \"\"\"\n",
        "        # Convert to positive index\n",
        "        index = index % len(self.observation_history)\n",
        "\n",
        "        stacked_observations = self.observation_history[index].copy()\n",
        "        for past_observation_index in reversed(\n",
        "            range(index - num_stacked_observations, index)\n",
        "        ):\n",
        "            if 0 <= past_observation_index:\n",
        "                previous_observation = numpy.concatenate(\n",
        "                    (\n",
        "                        self.observation_history[past_observation_index],\n",
        "                        [\n",
        "                            numpy.ones_like(stacked_observations[0])\n",
        "                            * self.action_history[past_observation_index + 1]\n",
        "                        ],\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                previous_observation = numpy.concatenate(\n",
        "                    (\n",
        "                        numpy.zeros_like(self.observation_history[index]),\n",
        "                        [numpy.zeros_like(stacked_observations[0])],\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            stacked_observations = numpy.concatenate(\n",
        "                (stacked_observations, previous_observation)\n",
        "            )\n",
        "\n",
        "        return stacked_observations\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"\n",
        "    A class that holds the min-max values of the tree.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.maximum = -float(\"inf\")\n",
        "        self.minimum = float(\"inf\")\n",
        "\n",
        "    def update(self, value):\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value):\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGncc5kxzIxM",
        "colab_type": "text"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf_GeH6fk-dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import numpy\n",
        "import ray\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Class which run in a dedicated thread to train a neural network and save it\n",
        "    in the shared storage.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initial_weights, config):\n",
        "        self.config = config\n",
        "        self.training_step = 0\n",
        "\n",
        "        # Initialize the network\n",
        "        self.model = MuZeroNetwork(self.config)\n",
        "        self.model.set_weights(initial_weights)\n",
        "        # self.model.to(torch.device(config.training_device))\n",
        "        self.model.to(torch.device(\"cpu\"))\n",
        "        self.model.train()\n",
        "\n",
        "        if self.config.optimizer == \"SGD\":\n",
        "            self.optimizer = torch.optim.SGD(\n",
        "                self.model.parameters(),\n",
        "                lr=self.config.lr_init,\n",
        "                momentum=self.config.momentum,\n",
        "                weight_decay=self.config.weight_decay,\n",
        "            )\n",
        "        elif self.config.optimizer == \"Adam\":\n",
        "            self.optimizer = torch.optim.Adam(\n",
        "                self.model.parameters(),\n",
        "                lr=self.config.lr_init,\n",
        "                weight_decay=self.config.weight_decay,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"{} is not implemented. You can change the optimizer manually in trainer.py.\"\n",
        "            )\n",
        "\n",
        "    def continuous_update_weights(self, replay_buffer, shared_storage_worker):\n",
        "        # Wait for the replay buffer to be filled\n",
        "        while ray.get(replay_buffer.get_self_play_count.remote()) < 1:\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Training loop\n",
        "        while True:\n",
        "            index_batch, batch = ray.get(replay_buffer.get_batch.remote(self.model.get_weights()))\n",
        "            self.update_lr()\n",
        "            (\n",
        "                priorities,\n",
        "                total_loss,\n",
        "                value_loss,\n",
        "                reward_loss,\n",
        "                policy_loss,\n",
        "            ) = self.update_weights(batch)\n",
        "\n",
        "            if self.config.PER:\n",
        "                # Save new priorities in the replay buffer (See https://arxiv.org/abs/1803.00933)\n",
        "                replay_buffer.update_priorities.remote(priorities, index_batch)\n",
        "\n",
        "            # Save to the shared storage\n",
        "            if self.training_step % self.config.checkpoint_interval == 0:\n",
        "                shared_storage_worker.set_weights.remote(self.model.get_weights())\n",
        "            shared_storage_worker.set_infos.remote(\"training_step\", self.training_step)\n",
        "            shared_storage_worker.set_infos.remote(\n",
        "                \"lr\", self.optimizer.param_groups[0][\"lr\"]\n",
        "            )\n",
        "            shared_storage_worker.set_infos.remote(\"total_loss\", total_loss)\n",
        "            shared_storage_worker.set_infos.remote(\"value_loss\", value_loss)\n",
        "            shared_storage_worker.set_infos.remote(\"reward_loss\", reward_loss)\n",
        "            shared_storage_worker.set_infos.remote(\"policy_loss\", policy_loss)\n",
        "\n",
        "            # Managing the self-play / training ratio\n",
        "            if self.config.training_delay:\n",
        "                time.sleep(self.config.training_delay)\n",
        "            if self.config.ratio:\n",
        "                while (\n",
        "                    ray.get(replay_buffer.get_self_play_count.remote())\n",
        "                    / max(1, self.training_step)\n",
        "                    < self.config.ratio\n",
        "                ):\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "    def update_weights(self, batch):\n",
        "        \"\"\"\n",
        "        Perform one training step.\n",
        "        \"\"\"\n",
        "\n",
        "        (\n",
        "            observation_batch,\n",
        "            action_batch,\n",
        "            target_value,\n",
        "            target_reward,\n",
        "            target_policy,\n",
        "            weight_batch,\n",
        "            gradient_scale_batch,\n",
        "        ) = batch\n",
        "\n",
        "        # Keep values as scalars for calculating the priorities for the prioritized replay\n",
        "        target_value_scalar = numpy.array(target_value)\n",
        "        priorities = numpy.zeros_like(target_value_scalar)\n",
        "        dn = DynamicNetwork(self.config.observation_shape, self.config.blocks, \n",
        "                            self.config.channels, self.config.reduced_channels, \n",
        "                            self.config.resnet_fc_reward_layers, self.config.support_size, \n",
        "                            self.config.blocks)\n",
        "\n",
        "        device = next(self.model.parameters()).device\n",
        "        weight_batch = torch.tensor(weight_batch).float().to(device)\n",
        "        observation_batch = torch.tensor(observation_batch).float().to(device).unsqueeze(1)\n",
        "        action_batch = torch.tensor(action_batch).float().to(device).unsqueeze(-1)\n",
        "        target_value = torch.tensor(target_value).float().to(device)\n",
        "        target_reward = torch.tensor(target_reward).float().to(device)\n",
        "        target_policy = torch.tensor(target_policy).float().to(device)\n",
        "        gradient_scale_batch = torch.tensor(gradient_scale_batch).float().to(device)\n",
        "        # observation_batch: batch, channels, height, width\n",
        "        # action_batch: batch, num_unroll_steps+1, 1 (unsqueeze)\n",
        "        # target_value: batch, num_unroll_steps+1\n",
        "        # target_reward: batch, num_unroll_steps+1\n",
        "        # target_policy: batch, num_unroll_steps+1, len(action_space)\n",
        "        # gradient_scale_batch: batch, num_unroll_steps+1\n",
        "\n",
        "        target_value = scalar_to_support(target_value, self.config.support_size)\n",
        "        target_reward = scalar_to_support(target_reward, self.config.support_size)\n",
        "        # target_value: batch, num_unroll_steps+1, 2*support_size+1\n",
        "        # target_reward: batch, num_unroll_steps+1, 2*support_size+1\n",
        "\n",
        "        ## Generate predictions\n",
        "        value, reward, policy_logits, hidden_state = self.model.initial_inference(\n",
        "            observation_batch\n",
        "        )\n",
        "        predictions = [(value, reward, policy_logits)]\n",
        "        for i in range(1, action_batch.shape[1]):\n",
        "            value, reward, policy_logits, hidden_state = self.model.recurrent_inference(\n",
        "                hidden_state, action_batch[:, i]\n",
        "            )\n",
        "            # Scale the gradient at the start of the dynamics function (See paper appendix Training)\n",
        "            hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "            predictions.append((value, reward, policy_logits))\n",
        "        # predictions: num_unroll_steps+1, 3, batch, 2*support_size+1 | 2*support_size+1 | 9 (according to the 2nd dim)\n",
        "\n",
        "        ## Compute losses\n",
        "        value_loss, reward_loss, policy_loss = (0, 0, 0)\n",
        "        value, reward, policy_logits = predictions[0]\n",
        "        # Ignore reward loss for the first batch step\n",
        "        current_value_loss, _, current_policy_loss = self.loss_function(\n",
        "            value.squeeze(-1),\n",
        "            reward.squeeze(-1),\n",
        "            policy_logits,\n",
        "            target_value[:, 0],\n",
        "            target_reward[:, 0],\n",
        "            target_policy[:, 0],\n",
        "        )\n",
        "        value_loss += current_value_loss\n",
        "        policy_loss += current_policy_loss\n",
        "        # Compute priorities for the prioritized replay (See paper appendix Training)\n",
        "        pred_value_scalar = (\n",
        "            support_to_scalar(value, self.config.support_size)\n",
        "            .detach()\n",
        "            .cpu()\n",
        "            .numpy()\n",
        "            .squeeze()\n",
        "        )\n",
        "        priorities[:, 0] = (\n",
        "            numpy.abs(pred_value_scalar - target_value_scalar[:, 0])\n",
        "            ** self.config.PER_alpha\n",
        "        )\n",
        "\n",
        "        for i in range(1, len(predictions)):\n",
        "            value, reward, policy_logits = predictions[i]\n",
        "            (\n",
        "                current_value_loss,\n",
        "                current_reward_loss,\n",
        "                current_policy_loss,\n",
        "            ) = self.loss_function(\n",
        "                value.squeeze(-1),\n",
        "                reward.squeeze(-1),\n",
        "                policy_logits,\n",
        "                target_value[:, i],\n",
        "                target_reward[:, i],\n",
        "                target_policy[:, i],\n",
        "            )\n",
        "\n",
        "            # Scale gradient by the number of unroll steps (See paper appendix Training)\n",
        "            current_value_loss.register_hook(\n",
        "                lambda grad: grad / gradient_scale_batch[:, i]\n",
        "            )\n",
        "            current_reward_loss.register_hook(\n",
        "                lambda grad: grad / gradient_scale_batch[:, i]\n",
        "            )\n",
        "            current_policy_loss.register_hook(\n",
        "                lambda grad: grad / gradient_scale_batch[:, i]\n",
        "            )\n",
        "\n",
        "            value_loss += current_value_loss\n",
        "            reward_loss += current_reward_loss\n",
        "            policy_loss += current_policy_loss\n",
        "\n",
        "            # Compute priorities for the prioritized replay (See paper appendix Training)\n",
        "            pred_value_scalar = (\n",
        "                support_to_scalar(value, self.config.support_size)\n",
        "                .detach()\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "                .squeeze()\n",
        "            )\n",
        "            priorities[:, i] = (\n",
        "                numpy.abs(pred_value_scalar - target_value_scalar[:, i])\n",
        "                ** self.config.PER_alpha\n",
        "            )\n",
        "\n",
        "        # Scale the value loss, paper recommends by 0.25 (See paper appendix Reanalyze)\n",
        "        loss = value_loss * self.config.value_loss_weight + reward_loss + policy_loss\n",
        "        if self.config.PER:\n",
        "            # Correct PER bias by using importance-sampling (IS) weights\n",
        "            loss *= weight_batch\n",
        "        # Mean over batch dimension (pseudocode do a sum)\n",
        "        loss = loss.mean()\n",
        "\n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.training_step += 1\n",
        "\n",
        "        return (\n",
        "            priorities,\n",
        "            # For log purpose\n",
        "            loss.item(),\n",
        "            value_loss.mean().item(),\n",
        "            reward_loss.mean().item(),\n",
        "            policy_loss.mean().item(),\n",
        "        )\n",
        "\n",
        "    def update_lr(self):\n",
        "        \"\"\"\n",
        "        Update learning rate\n",
        "        \"\"\"\n",
        "        lr = self.config.lr_init * self.config.lr_decay_rate ** (\n",
        "            self.training_step / self.config.lr_decay_steps\n",
        "        )\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_function(\n",
        "        value, reward, policy_logits, target_value, target_reward, target_policy,\n",
        "    ):\n",
        "        # Cross-entropy seems to have a better convergence than MSE\n",
        "        value_loss = (-target_value * torch.nn.LogSoftmax(dim=1)(value)).sum(1)\n",
        "        reward_loss = (-target_reward * torch.nn.LogSoftmax(dim=1)(reward)).sum(1)\n",
        "        policy_loss = (-target_policy * torch.nn.LogSoftmax(dim=1)(policy_logits)).sum(\n",
        "            1\n",
        "        )\n",
        "        return value_loss, reward_loss, policy_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k07xqnp-sP10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBLyOIYuwlRf",
        "colab_type": "text"
      },
      "source": [
        "#MUZERO.PY CLASS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY_-2CqqwgmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import importlib\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import ray\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tensorboard import notebook\n",
        "\n",
        "\n",
        "\n",
        "class MuZero:\n",
        "    \"\"\"\n",
        "    Main class to manage MuZero.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game_name, num_node):\n",
        "        self.game_name = game_name\n",
        "\n",
        "        try:\n",
        "            self.config = MuZeroConfig(num_node)\n",
        "            self.Game = TSP(num_node)\n",
        "        except Exception as err:\n",
        "            print(\n",
        "                '{} is not a supported game name, try \"cartpole\" or refer to the documentation for adding a new game.'.format(\n",
        "                    self.game_name\n",
        "                )\n",
        "            )\n",
        "            raise err\n",
        "\n",
        "        # Fix random generator seed\n",
        "        numpy.random.seed(self.config.seed)\n",
        "        torch.manual_seed(self.config.seed)\n",
        "\n",
        "        # Weights used to initialize components\n",
        "        self.muzero_weights = MuZeroNetwork(self.config).get_weights()\n",
        "\n",
        "    def train(self):\n",
        "        ray.init(webui_host='127.0.0.1', ignore_reinit_error=True)\n",
        "        training_worker = Trainer.remote(copy.deepcopy(self.muzero_weights), self.config)\n",
        "        self_play_workers = [\n",
        "            SelfPlay.remote(\n",
        "                copy.deepcopy(self.muzero_weights),\n",
        "                self.Game,\n",
        "                self.config,\n",
        "            )\n",
        "            for seed in range(self.config.num_actors)\n",
        "        ]\n",
        "        shared_storage_worker = SharedStorage.remote(\n",
        "            copy.deepcopy(self.muzero_weights), self.game_name, self.config,\n",
        "        )\n",
        "        os.makedirs(self.config.results_path, exist_ok=True)\n",
        "        writer = SummaryWriter(\"logs/tensorboard\")\n",
        "\n",
        "        replay_buffer_worker = ReplayBuffer.remote(self.config)\n",
        "        test_worker = SelfPlay.remote(\n",
        "            copy.deepcopy(self.muzero_weights),\n",
        "            self.Game,\n",
        "            self.config,\n",
        "        )\n",
        "        # Launch workers\n",
        "        [\n",
        "            self_play_worker.continuous_self_play.remote(\n",
        "                shared_storage_worker, replay_buffer_worker\n",
        "            )\n",
        "            for self_play_worker in self_play_workers\n",
        "        ]\n",
        "        test_worker.continuous_self_play.remote(shared_storage_worker, None, True)\n",
        "        training_worker.continuous_update_weights.remote(\n",
        "            replay_buffer_worker, shared_storage_worker\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            \"\\nTraining...\\nRun tensorboard --logdir ./results and go to http://localhost:6006/ to see in real time the training performance.\\n\"\n",
        "        )\n",
        "        # Save hyperparameters to TensorBoard\n",
        "        hp_table = [\n",
        "            \"| {} | {} |\".format(key, value)\n",
        "            for key, value in self.config.__dict__.items()\n",
        "        ]\n",
        "        writer.add_text(\n",
        "            \"Hyperparameters\",\n",
        "            \"| Parameter | Value |\\n|-------|-------|\\n\" + \"\\n\".join(hp_table),\n",
        "        )\n",
        "        # Loop for monitoring in real time the workers\n",
        "        counter = 0\n",
        "        infos = ray.get(shared_storage_worker.get_infos.remote())\n",
        "        try:\n",
        "            while infos[\"training_step\"] < self.config.training_steps:\n",
        "                # Get and save real time performance\n",
        "                infos = ray.get(shared_storage_worker.get_infos.remote())\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/1.Total reward\", infos[\"total_reward\"], counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/2.Episode length\", infos[\"episode_length\"], counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/3.Player 0 MuZero reward\",\n",
        "                    infos[\"player_0_reward\"],\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"1.Total reward/4.Player 1 Random reward\",\n",
        "                    infos[\"player_1_reward\"],\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/1.Self played games\",\n",
        "                    ray.get(replay_buffer_worker.get_self_play_count.remote()),\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/2.Training steps\", infos[\"training_step\"], counter\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    \"2.Workers/3.Self played games per training step ratio\",\n",
        "                    ray.get(replay_buffer_worker.get_self_play_count.remote())\n",
        "                    / max(1, infos[\"training_step\"]),\n",
        "                    counter,\n",
        "                )\n",
        "                writer.add_scalar(\"2.Workers/4.Learning rate\", infos[\"lr\"], counter)\n",
        "                writer.add_scalar(\n",
        "                    \"3.Loss/1.Total weighted loss\", infos[\"total_loss\"], counter\n",
        "                )\n",
        "                writer.add_scalar(\"3.Loss/Value loss\", infos[\"value_loss\"], counter)\n",
        "                writer.add_scalar(\"3.Loss/Reward loss\", infos[\"reward_loss\"], counter)\n",
        "                writer.add_scalar(\"3.Loss/Policy loss\", infos[\"policy_loss\"], counter)\n",
        "                count = 0 \n",
        "                if count % 10 == 0:\n",
        "                  print(\n",
        "                      \"Last test reward: {0:.2f}. Training step: {1}/{2}. Played games: {3}. Loss: {4:.2f}\".format(\n",
        "                          infos[\"total_reward\"],\n",
        "                          infos[\"training_step\"],\n",
        "                          self.config.training_steps,\n",
        "                          ray.get(replay_buffer_worker.get_self_play_count.remote()),\n",
        "                          infos[\"total_loss\"],\n",
        "                      ))\n",
        "                count = count + 1\n",
        "                print(\n",
        "                    \"Last test reward: {0:.2f}. Training step: {1}/{2}. Played games: {3}. Loss: {4:.2f}\".format(\n",
        "                        infos[\"total_reward\"],\n",
        "                        infos[\"training_step\"],\n",
        "                        self.config.training_steps,\n",
        "                        ray.get(replay_buffer_worker.get_self_play_count.remote()),\n",
        "                        infos[\"total_loss\"],\n",
        "                    ),\n",
        "                    end=\"\\r\",\n",
        "                )\n",
        "                counter += 1\n",
        "                time.sleep(0.5)\n",
        "        except KeyboardInterrupt as err:\n",
        "            # Comment the line below to be able to stop the training but keep running\n",
        "            # raise err\n",
        "            pass\n",
        "        self.muzero_weights = ray.get(shared_storage_worker.get_weights.remote())\n",
        "        # End running actors\n",
        "        ray.shutdown()\n",
        "\n",
        "    def test(self, render=True, opponent=\"self\", muzero_player=None):\n",
        "        \"\"\"\n",
        "        Test the model in a dedicated thread.\n",
        "\n",
        "        Args:\n",
        "            render: Boolean to display or not the environment.\n",
        "\n",
        "            opponent: \"self\" for self-play, \"human\" for playing against MuZero and \"random\"\n",
        "            for a random agent.\n",
        "\n",
        "            muzero_player: Integer with the player number of MuZero in case of multiplayer\n",
        "            games, None let MuZero play all players turn by turn.\n",
        "        \"\"\"\n",
        "        print(\"\\nTesting...\")\n",
        "        ray.init(webui_host='127.0.0.1', ignore_reinit_error=True)\n",
        "        self_play_workers = SelfPlay.remote(\n",
        "            copy.deepcopy(self.muzero_weights),\n",
        "            self.Game,\n",
        "            self.config,\n",
        "        )\n",
        "        history = ray.get(\n",
        "            self_play_workers.play_game.remote(0, 0, render, opponent, muzero_player)\n",
        "        )\n",
        "        ray.shutdown()\n",
        "        return sum(history.reward_history)\n",
        "\n",
        "    def load_model(self, path=None):\n",
        "        if not path:\n",
        "            path = os.path.join(self.config.results_path, \"model.weights\")\n",
        "        try:\n",
        "            self.muzero_weights = torch.load(path)\n",
        "            print(\"\\nUsing weights from {}\".format(path))\n",
        "        except FileNotFoundError:\n",
        "            print(\"\\nThere is no model saved in {}.\".format(path))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize MuZero\n",
        "    num_node = 5\n",
        "    muzero = MuZero(\"TSP\", num_node)\n",
        "    print(\"Invoking the train function\")\n",
        "\n",
        "    while True:\n",
        "        # Configure running options\n",
        "        options = [\n",
        "            \"Train\",\n",
        "            \"Load pretrained model\",\n",
        "            \"Render some self play games\",\n",
        "            \"Exit\",\n",
        "        ]\n",
        "        print()\n",
        "        for i in range(len(options)):\n",
        "            print(\"{}. {}\".format(i, options[i]))\n",
        "\n",
        "        choice = input(\"Enter a number to choose an action: \")\n",
        "        valid_inputs = [str(i) for i in range(len(options))]\n",
        "        while choice not in valid_inputs:\n",
        "            choice = input(\"Invalid input, enter a number listed above: \")\n",
        "        choice = int(choice)\n",
        "        if choice == 0:\n",
        "            muzero.train()\n",
        "        elif choice == 1:\n",
        "            path = input(\"Enter a path to the model.weights: \") ### set 1 path and pass it as const \n",
        "            while not os.path.isfile(path):\n",
        "                path = input(\"Invalid path. Try again: \")\n",
        "            # path = \"/content/drive/My Drive/deep-learning-final/test_weights/model.weights\"\n",
        "            muzero.load_model(path)\n",
        " \n",
        "        elif choice == 2:\n",
        "            muzero_tour_length = muzero.test(render=True, opponent=\"self\")\n",
        "            optimal_tour_length = muzero.Game.optimal()\n",
        "            print(\"muzero_tour_length\", muzero_tour_length)\n",
        "            print(\"optimal_tour_length\",optimal_tour_length)\n",
        "            print(\"ratio\", muzero_tour_length/optimal_tour_length[0])\n",
        "        else:\n",
        "            break\n",
        "        print(\"\\nDone\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}